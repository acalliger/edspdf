{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>EDS-PDF provides modular framework to extract text information from PDF documents.</p> <p>You can use it out-of-the-box, or extend it to fit your use-case.</p>"},{"location":"#getting-started","title":"Getting started","text":""},{"location":"#installation","title":"Installation","text":"<p>Install the library with pip:</p> <pre><code>$ pip install edspdf\n---&gt; 100%\ncolor:green Installation successful\n</code></pre>"},{"location":"#extracting-text","title":"Extracting text","text":"<p>Let's build a simple PDF extractor that uses a rule-based classifier. There are two ways to do this, either by using the configuration system or by using the pipeline API.</p> Configuration based pipelineAPI based pipeline <p>Create a configuration file:</p> config.cfg<pre><code>[pipeline]\npipeline = [\"extractor\", \"classifier\", \"aggregator\"]\n\n[components.extractor]\n@factory = \"pdfminer-extractor\"\n\n[components.classifier]\n@factory = \"mask-classifier\"\nx0 = 0.2\nx1 = 0.9\ny0 = 0.3\ny1 = 0.6\nthreshold = 0.1\n\n[components.aggregator]\n@factory = \"simple-aggregator\"\n</code></pre> <p>and load it from Python:</p> <pre><code>import edspdf\nfrom pathlib import Path\n\nmodel = edspdf.load(\"config.cfg\")  # (1)\n</code></pre> <p>Or create a pipeline directly from Python:</p> <pre><code>from edspdf import Pipeline\n\nmodel = Pipeline()\nmodel.add_pipe(\"pdfminer-extractor\")\nmodel.add_pipe(\n    \"mask-classifier\",\n    config=dict(\n        x0=0.2,\n        x1=0.9,\n        y0=0.3,\n        y1=0.6,\n        threshold=0.1,\n    ),\n)\nmodel.add_pipe(\"simple-aggregator\")\n</code></pre> <p>This pipeline can then be applied (for instance with this PDF):</p> <pre><code># Get a PDF\npdf = Path(\"/Users/perceval/Development/edspdf/tests/resources/letter.pdf\").read_bytes()\npdf = model(pdf)\n\nbody = pdf.aggregated_texts[\"body\"]\n\ntext, style = body.text, body.properties\n</code></pre> <p>See the rule-based recipe for a step-by-step explanation of what is happening.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use EDS-PDF, please cite us as below.</p> <pre><code>@software{edspdf,\nauthor  = {Dura, Basile and Wajsburt, Perceval and Calliger, Alice and G\u00e9rardin, Christel and Bey, Romain},\ndoi     = {10.5281/zenodo.6902977},\nlicense = {BSD-3-Clause},\ntitle   = {{EDS-PDF: Smart text extraction from PDF documents}},\nurl     = {https://github.com/aphp/edspdf}\n}\n</code></pre>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>We would like to thank Assistance Publique \u2013 H\u00f4pitaux de Paris and AP-HP Foundation for funding this project.</p>"},{"location":"alternatives/","title":"Alternatives &amp; Comparison","text":"<p>EDS-PDF was developed to propose a more modular and extendable approach to PDF extraction than PDFBox, the legacy implementation at APHP's clinical data warehouse.</p> <p>EDS-PDF takes inspiration from Explosion's spaCy pipelining system and closely follows its API. Therefore, the core object within EDS-PDF is the Pipeline, which organises the processing of PDF documents into multiple components. However, unlike spaCy, the library is built around a single deep learning framework, pytorch, which makes model development easier.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v080","title":"v0.8.0","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Add multi-modal transformers (<code>huggingface-embedding</code>) with windowing options</li> <li>Add <code>render_page</code> option to <code>pdfminer</code> extractor, for multi-modal PDF features</li> <li>Add inference utilities (<code>accelerators</code>), with simple mono process support and multi gpu / cpu support</li> <li>Packaging utils (<code>pipeline.package(...)</code>) to make a pip installable package from a pipeline</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Updated API to follow EDS-NLP's refactoring</li> <li>Updated <code>confit</code> to 0.4.2 (better errors) and <code>foldedtensor</code> to 0.3.0 (better multiprocess support)</li> <li>Removed <code>pipeline.score</code>. You should use <code>pipeline.pipe</code>, a custom scorer and <code>pipeline.select_pipes</code> instead.</li> <li>Better test coverage</li> <li>Use <code>hatch</code> instead of <code>setuptools</code> to build the package / docs and run the tests</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed <code>attrs</code> dependency only being installed in dev mode</li> </ul>"},{"location":"changelog/#v070","title":"v0.7.0","text":"<p>Major refactoring of the library:</p>"},{"location":"changelog/#core-features","title":"Core features","text":"<ul> <li>new pipeline system whose API is inspired by spaCy</li> <li>first-class support for pytorch</li> <li>hybrid model inference and training (rules + deep learning)</li> <li>moved from pandas DataFrame to attrs dataclasses (<code>PDFDoc</code>, <code>Page</code>, <code>Box</code>, ...) for representing PDF documents</li> <li>new configuration system based on [config][https://github.com/aphp/config], with support for instantiation of complex deep learning models, off-the-shelf CLI, ...</li> </ul>"},{"location":"changelog/#functional-features","title":"Functional features","text":"<ul> <li>new extractors: pymupdf and poppler (separate packages for licensing reasons)</li> <li>many deep learning layers (box-transformer, 2d attention with relative position information, ...)</li> <li>trainable deep learning classifier</li> <li>training recipes for deep learning models</li> </ul>"},{"location":"changelog/#v063-2023-01-23","title":"v0.6.3 - 2023-01-23","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Allow corrupted PDF to not raise an error by default (they are treated as empty PDFs)</li> <li>Fix classification and aggregation for empty PDFs</li> </ul>"},{"location":"changelog/#v062-2022-12-07","title":"v0.6.2 - 2022-12-07","text":"<p>Cast bytes-like extractor inputs as bytes</p>"},{"location":"changelog/#v061-2022-12-07","title":"v0.6.1 - 2022-12-07","text":"<p>Performance and cuda related fixes.</p>"},{"location":"changelog/#v060-2022-12-05","title":"v0.6.0 - 2022-12-05","text":"<p>Many, many changes: - added torch as the main deep learning framework instead of spaCy and thinc  - added poppler and mupdf as alternatives to pdfminer - new pipeline / config / registry system to facilitate consistency between training and inference - standardization of the exchange format between components with dataclass models (attrs more specifically) instead of pandas dataframes</p>"},{"location":"changelog/#v053-2022-08-31","title":"v0.5.3 - 2022-08-31","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Add label mapping parameter to aggregators (to merge different types of blocks such as <code>title</code> and <code>body</code>)</li> <li>Improved line aggregation formula</li> </ul>"},{"location":"changelog/#v052-2022-08-30","title":"v0.5.2 - 2022-08-30","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Fix aggregation for empty documents</li> </ul>"},{"location":"changelog/#v051-2022-07-26","title":"v0.5.1 - 2022-07-26","text":""},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Drop the <code>pdf2image</code> dependency, replacing it with <code>pypdfium2</code> (easier installation)</li> </ul>"},{"location":"changelog/#v050-2022-07-25","title":"v0.5.0 - 2022-07-25","text":""},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Major refactoring of the library. Moved from concepts (<code>aggregation</code>) to plural names (<code>aggregators</code>).</li> </ul>"},{"location":"changelog/#v043-2022-07-20","title":"v0.4.3 - 2022-07-20","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Multi page boxes alignment</li> </ul>"},{"location":"changelog/#v042-2022-07-06","title":"v0.4.2 - 2022-07-06","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li><code>package-resource.v1</code> in the misc registry</li> </ul>"},{"location":"changelog/#v041-2022-06-14","title":"v0.4.1 - 2022-06-14","text":""},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Remove <code>importlib.metadata</code> dependency, which led to issues with Python 3.7</li> </ul>"},{"location":"changelog/#v040-2022-06-14","title":"v0.4.0 - 2022-06-14","text":""},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Python 3.7 support, by relaxing dependency constraints</li> <li>Support for package-resource pipeline for <code>sklearn-pipeline.v1</code></li> </ul>"},{"location":"changelog/#v032-2022-06-03","title":"v0.3.2 - 2022-06-03","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li><code>compare_results</code> in visualisation</li> </ul>"},{"location":"changelog/#v031-2022-06-02","title":"v0.3.1 - 2022-06-02","text":""},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Rescale transform now keeps origin on top-left corner</li> </ul>"},{"location":"changelog/#v030-2022-06-01","title":"v0.3.0 - 2022-06-01","text":""},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Styles management within the extractor</li> <li><code>styled.v1</code> aggregator, to handle styles</li> <li><code>rescale.v1</code> transform, to go back to the original height and width</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>Styles and text extraction is handled by the extractor directly</li> <li>The PDFMiner <code>line</code> object is not carried around any more</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Outdated <code>params</code> entry in the EDS-PDF registry.</li> </ul>"},{"location":"changelog/#v022-2022-05-12","title":"v0.2.2 - 2022-05-12","text":""},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Fixed <code>merge_lines</code> bug when lines were empty</li> <li>Modified the demo consequently</li> </ul>"},{"location":"changelog/#v021-2022-05-09","title":"v0.2.1 - 2022-05-09","text":""},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>The extractor always returns a pandas DataFrame, be it empty. It enhances robustness and stability.</li> </ul>"},{"location":"changelog/#v020-2022-05-09","title":"v0.2.0 - 2022-05-09","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li><code>aggregation</code> submodule to handle the specifics of aggregating text blocs</li> <li>Base classes for better-defined modules</li> <li>Uniformise the columns to <code>labels</code></li> <li>Add arbitrary contextual information</li> </ul>"},{"location":"changelog/#removed_1","title":"Removed","text":"<ul> <li><code>typer</code> legacy dependency</li> <li><code>models</code> submodule, which handled the configurations for Spark distribution (deferred to another package)</li> <li>specific <code>orbis</code> context, which was APHP-specific</li> </ul>"},{"location":"changelog/#v010-2022-05-06","title":"v0.1.0 - 2022-05-06","text":"<p>Inception ! </p>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>spaCy-like configuration system</li> <li>Available classifiers :</li> <li><code>dummy.v1</code>, that classifies everything to <code>body</code></li> <li><code>mask.v1</code>, for simple rule-based classification</li> <li><code>sklearn.v1</code>, that uses a Scikit-Learn pipeline</li> <li><code>random.v1</code>, to better sow chaos</li> <li>Merge different blocs together for easier visualisation</li> <li>Streamlit demo with visualisation</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>EDS-PDF is built on top of the <code>confit</code> configuration system.</p> <p>The following catalogue registries are included within EDS-PDF:</p> Section Description <code>factory</code> Components factories (most often classes) <code>adapter</code> Raw data preprocessing functions <p>EDS-PDF pipelines are meant to be reproducible and serializable, such that you can always define a pipeline through the configuration system.</p> <p>To wit, compare the API-based approach to the configuration-based approach (the two are strictly equivalent):</p> API-basedConfiguration-based <pre><code>import edspdf\nfrom pathlib import Path\n\nmodel = edspdf.Pipeline()\nmodel.add_pipe(\"pdfminer-extractor\", name=\"extractor\")\nmodel.add_pipe(\"mask-classifier\", name=\"classifier\", config=dict(\nx0=0.2,\nx1=0.9,\ny0=0.3,\ny1=0.6,\nthreshold=0.1,\n)\nmodel.add_pipe(\"simple-aggregator\", name=\"aggregator\")\n# Get a PDF\npdf = Path(\"letter.pdf\").read_bytes()\n\npdf = model(pdf)\n\nstr(pdf.aggregated_texts[\"body\"])\n# Out: Cher Pr ABC, Cher DEF,\\n...\n</code></pre> config.cfg<pre><code>[pipeline]\npipeline = [\"extractor\", \"classifier\", \"aggregator\"]\n\n[components.extractor]\n@factory = \"pdfminer-extractor\"\n\n[components.classifier]\n@factory = \"mask-classifier\"\nx0 = 0.2\nx1 = 0.9\ny0 = 0.3\ny1 = 0.6\nthreshold = 0.1\n\n[components.aggregator]\n@factory = \"simple-aggregator\"\n</code></pre> <pre><code>import edspdf\nfrom pathlib import Path\n\npipeline = edspdf.load(\"config.cfg\")\n# Get a PDF\npdf = Path(\"letter.pdf\").read_bytes()\n\npdf = pipeline(pdf)\n\nstr(pdf.aggregated_texts[\"body\"])\n# Out: Cher Pr ABC, Cher DEF,\\n...\n</code></pre> <p>The configuration-based approach strictly separates the definition of the pipeline to its application and avoids tucking away important configuration details. Changes to the pipeline are transparent as there is a single source of truth: the configuration file.</p>"},{"location":"contributing/","title":"Contributing to EDS-PDF","text":"<p>We welcome contributions ! There are many ways to help. For example, you can:</p> <ol> <li>Help us track bugs by filing issues</li> <li>Suggest and help prioritise new functionalities</li> <li>Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you.</li> </ol>"},{"location":"contributing/#development-installation","title":"Development installation","text":"<p>To be able to run the test suite and develop your own pipeline, you should clone the repo and install it locally. We use the <code>hatch</code> package manager to manage the project.</p> <pre><code>color:gray # Clone the repository and change directory\n$ git clone ssh://git@github.com/aphp/edspdf.git\n---&gt; 100%\n\ncolor:gray # Ensure hatch is installed, preferably via pipx\n$ pipx install hatch\n\n$ cd edspdf\n\ncolor:gray # Enter a shell to develop / test the project. This will install everything required in a virtual environment. You can also `source` the path shown by hatch.\n$ hatch shell\n$ ...\n$ exit  # when you're done\n</code></pre> <p>To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the <code>pre-commit</code> Python library. To use it, simply install it:</p> <pre><code>$ pre-commit install\n</code></pre> <p>The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong.</p> <p>The hooks only run on staged changes. To force-run it on all files, run:</p> <pre><code>$ pre-commit run --all-files\n---&gt; 100%\ncolor:green All good !\n</code></pre>"},{"location":"contributing/#proposing-a-merge-request","title":"Proposing a merge request","text":"<p>At the very least, your changes should :</p> <ul> <li>Be well-documented ;</li> <li>Pass every tests, and preferably implement its own ;</li> <li>Follow the style guide.</li> </ul>"},{"location":"contributing/#testing-your-code","title":"Testing your code","text":"<p>We use the Pytest test suite.</p> <p>The following command will run the test suite. Writing your own tests is encouraged !</p> <pre><code>pytest\n</code></pre> <p>Should your contribution propose a bug fix, we require the bug be thoroughly tested.</p>"},{"location":"contributing/#style-guide","title":"Style Guide","text":"<p>We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short :</p> <p>Black reformats entire files in place. It is not configurable.</p> <p>Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use <code>pre-commit</code> to keep our codebase clean.</p> <p>Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save.</p>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be.</p> <p>We use <code>MkDocs</code> for EDS-PDF's documentation. You can view your changes with</p> <pre><code>color:gray # Run the documentation\n$ hatch run docs:serve\n</code></pre> <p>Go to <code>localhost:8000</code> to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page.</p>"},{"location":"data-structures/","title":"Data Structures","text":"<p>EDS-PDF stores PDFs and their annotation in a custom data structures that are designed to be easy to use and manipulate. We must distinguish between:</p> <ul> <li>the data models used to store the PDFs and exchange them between the   different components of EDS-PDF</li> <li>the tensors structures used to process the PDFs with deep learning models</li> </ul>"},{"location":"data-structures/#itinerary-of-a-pdf","title":"Itinerary of a PDF","text":"<p>A PDF is first converted to a PDFDoc object, which contains the raw PDF content. This task is usually performed a PDF extractor component. Once the PDF is converted, the same object will be used and updated by the different components, and returned at the end of the pipeline.</p> <p>When running a trainable component, the PDFDoc is preprocessed and converted to tensors containing relevant features for the task. This task is performed in the <code>preprocess</code> method of the component. The resulting tensors are then collated together to form a batch, in the <code>collate</code> method of the component. After running the <code>forward</code> method of the component, the tensor predictions are finally assigned as annotations to original PDFDoc objects in the <code>postprocess</code> method.</p>"},{"location":"data-structures/#data-models","title":"Data models","text":"<p>The main data structure is the [PDFDoc][edspdf.structures.PDFDoc], which represents full a PDF document. It contains the raw PDF content, annotations for the full document, regardless of pages. A PDF is split into <code>Page</code> objects that stores their number, dimension and optionally an image of the rendered page.</p> <p>The PDF annotations are stored in <code>Box</code> objects, which represent a rectangular region of the PDF. At the moment, box can only be specialized into <code>TextBox</code> to represent text regions, such as lines extracted by a PDF extractor. Aggregated texts are stored in <code>Text</code> objects, that are not associated with a specific box.</p> <p>A <code>TextBox</code> contains a list of <code>TextProperties</code> objects to store the style properties of a styled spans of the text.</p> Reference"},{"location":"data-structures/#edspdf.structures.PDFDoc","title":"<code>PDFDoc</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This is the main data structure of the library to hold PDFs. It contains the content of the PDF, as well as box annotations and text outputs.</p> ATTRIBUTE DESCRIPTION <code>content</code> <p>The content of the PDF document.</p> <p> TYPE: <code>bytes</code> </p> <code>id</code> <p>The ID of the PDF document.</p> <p> TYPE: <code>(str, optional)</code> </p> <code>pages</code> <p>The pages of the PDF document.</p> <p> TYPE: <code>List[Page]</code> </p> <code>error</code> <p>Whether there was an error when processing this PDF document.</p> <p> TYPE: <code>(bool, optional)</code> </p> <code>content_boxes</code> <p>The content boxes/annotations of the PDF document.</p> <p> TYPE: <code>List[Union[TextBox, ImageBox]]</code> </p> <code>aggregated_texts</code> <p>The aggregated text outputs of the PDF document.</p> <p> TYPE: <code>Dict[str, Text]</code> </p> <code>text_boxes</code> <p>The text boxes of the PDF document.</p> <p> TYPE: <code>List[TextBox]</code> </p>"},{"location":"data-structures/#edspdf.structures.Page","title":"<code>Page</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The <code>Page</code> class represents a page of a PDF document.</p> ATTRIBUTE DESCRIPTION <code>page_num</code> <p>The page number of the page.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>The width of the page.</p> <p> TYPE: <code>float</code> </p> <code>height</code> <p>The height of the page.</p> <p> TYPE: <code>float</code> </p> <code>doc</code> <p>The PDF document that this page belongs to.</p> <p> TYPE: <code>PDFDoc</code> </p> <code>image</code> <p>The rendered image of the page, stored as a NumPy array.</p> <p> TYPE: <code>Optional[ndarray]</code> </p> <code>text_boxes</code> <p>The text boxes of the page.</p> <p> TYPE: <code>List[TextBox]</code> </p>"},{"location":"data-structures/#edspdf.structures.TextProperties","title":"<code>TextProperties</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The <code>TextProperties</code> class represents the style properties of a span of text in a TextBox.</p> ATTRIBUTE DESCRIPTION <code>italic</code> <p>Whether the text is italic.</p> <p> TYPE: <code>bool</code> </p> <code>bold</code> <p>Whether the text is bold.</p> <p> TYPE: <code>bool</code> </p> <code>begin</code> <p>The beginning index of the span of text.</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>The ending index of the span of text.</p> <p> TYPE: <code>int</code> </p> <code>fontname</code> <p>The font name of the span of text.</p> <p> TYPE: <code>Optional[str]</code> </p>"},{"location":"data-structures/#edspdf.structures.Box","title":"<code>Box</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The <code>Box</code> class represents a box annotation in a PDF document. It is the base class of TextBox.</p> ATTRIBUTE DESCRIPTION <code>doc</code> <p>The PDF document that this box belongs to.</p> <p> TYPE: <code>PDFDoc</code> </p> <code>page_num</code> <p>The page number of the box.</p> <p> TYPE: <code>Optional[int]</code> </p> <code>x0</code> <p>The left x-coordinate of the box.</p> <p> TYPE: <code>float</code> </p> <code>x1</code> <p>The right x-coordinate of the box.</p> <p> TYPE: <code>float</code> </p> <code>y0</code> <p>The top y-coordinate of the box.</p> <p> TYPE: <code>float</code> </p> <code>y1</code> <p>The bottom y-coordinate of the box.</p> <p> TYPE: <code>float</code> </p> <code>label</code> <p>The label of the box.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>page</code> <p>The page object that this box belongs to.</p> <p> TYPE: <code>Page</code> </p>"},{"location":"data-structures/#edspdf.structures.Text","title":"<code>Text</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The <code>TextBox</code> class represents text object, not bound to any box.</p> <p>It can be used to store aggregated text from multiple boxes for example.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The text content.</p> <p> TYPE: <code>str</code> </p> <code>properties</code> <p>The style properties of the text.</p> <p> TYPE: <code>List[TextProperties]</code> </p>"},{"location":"data-structures/#edspdf.structures.TextBox","title":"<code>TextBox</code>","text":"<p>             Bases: <code>Box</code></p> <p>The <code>TextBox</code> class represents a text box annotation in a PDF document.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The text content of the text box.</p> <p> TYPE: <code>str</code> </p> <code>props</code> <p>The style properties of the text box.</p> <p> TYPE: <code>List[TextProperties]</code> </p>"},{"location":"data-structures/#edspdf.structures.PDFDoc","title":"Data Structures","text":""},{"location":"data-structures/#edspdf.structures.Page","title":"Data Structures","text":""},{"location":"data-structures/#edspdf.structures.TextProperties","title":"Data Structures","text":""},{"location":"data-structures/#edspdf.structures.Box","title":"Data Structures","text":""},{"location":"data-structures/#edspdf.structures.Text","title":"Data Structures","text":""},{"location":"data-structures/#edspdf.structures.TextBox","title":"Data Structures","text":""},{"location":"data-structures/#tensor-structure","title":"Tensor structure","text":"<p>The tensors used to process PDFs with deep learning models usually contain 4 main dimensions, in addition to the standard embedding dimensions:</p> <ul> <li><code>samples</code>: one entry per PDF in the batch</li> <li><code>pages</code>: one entry per page in a PDF</li> <li><code>boxes</code>: one entry per box in a page</li> <li><code>token</code>: one entry per token in a box (only for text boxes)</li> </ul> <p>These tensors use a special FoldedTensor format to store the data in a compact way and reshape the data depending on the requirements of a layer.</p>"},{"location":"inference/","title":"Inference","text":"<p>Once you have obtained a pipeline, either by composing rule-based components, training a model or loading a model from the disk, you can use it to make predictions on documents. This is referred to as inference.</p>"},{"location":"inference/#inference-on-a-single-document","title":"Inference on a single document","text":"<p>In EDS-PDF, computing the prediction on a single document is done by calling the pipeline on the document. The input can be either:</p> <ul> <li>a sequence of bytes</li> <li>or a PDFDoc object</li> </ul> <pre><code>from pathlib import Path\n\npipeline = ...\ncontent = Path(\"path/to/.pdf\").read_bytes()\ndoc = pipeline(content)\n</code></pre> <p>If you're lucky enough to have a GPU, you can use it to speed up inference by moving the model to the GPU before calling the pipeline. To leverage multiple GPUs, refer to the multiprocessing accelerator description below.</p> <pre><code>pipeline.to(\"cuda\")  # same semantics as pytorch\ndoc = pipeline(content)\n</code></pre>"},{"location":"inference/#inference-on-multiple-documents","title":"Inference on multiple documents","text":"<p>When processing multiple documents, it is usually more efficient to use the <code>pipeline.pipe(...)</code> method, especially when using deep learning components, since this allow matrix multiplications to be batched together. Depending on your computational resources and requirements, EDS-PDF comes with various \"accelerators\" to speed up inference (see the Accelerators section for more details). By default, the <code>.pipe()</code> method uses the <code>simple</code> accelerator but you can switch to a different one by passing the <code>accelerator</code> argument.</p> <pre><code>pipeline = ...\ndocs = pipeline.pipe(\n    [content1, content2, ...],\n    batch_size=16,  # optional, default to the one defined in the pipeline\n    accelerator=my_accelerator,\n)\n</code></pre> <p>The <code>pipe</code> method supports the following arguments :</p> <p>Process a stream of documents by applying each component successively on batches of documents.</p> PARAMETER  DESCRIPTION <code>inputs</code> <p>The inputs to create the PDFDocs from, or the PDFDocs directly.</p> <p> TYPE: <code>Any</code> </p> <code>batch_size</code> <p>The batch size to use. If not provided, the batch size of the pipeline object will be used.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>accelerator</code> <p>The accelerator to use for processing the documents. If not provided, the default accelerator will be used.</p> <p> TYPE: <code>Optional[Union[str, Accelerator]]</code> DEFAULT: <code>None</code> </p> <code>to_doc</code> <p>The function to use to convert the inputs to PDFDoc objects. By default, the <code>content</code> field of the inputs will be used if dict-like objects are provided, otherwise the inputs will be passed directly to the pipeline.</p> <p> TYPE: <code>Optional[ToDoc]</code> DEFAULT: <code>None</code> </p> <code>from_doc</code> <p>The function to use to convert the PDFDoc objects to outputs. By default, the PDFDoc objects will be returned directly.</p> <p> TYPE: <code>FromDoc</code> DEFAULT: <code>lambda : doc</code> </p> RETURNS DESCRIPTION <code>Iterable[PDFDoc]</code> Source code in <code>edspdf/pipeline.py</code> <pre><code>@validate_arguments\ndef pipe(\n    self,\n    inputs: Any,\n    batch_size: Optional[int] = None,\n    *,\n    accelerator: Optional[Union[str, Accelerator]] = None,\n    to_doc: Optional[ToDoc] = None,\n    from_doc: FromDoc = lambda doc: doc,\n) -&gt; Iterable[PDFDoc]:\n\"\"\"\n    Process a stream of documents by applying each component successively on\n    batches of documents.\n\n    Parameters\n    ----------\n    inputs: Iterable[Union[str, PDFDoc]]\n        The inputs to create the PDFDocs from, or the PDFDocs directly.\n    batch_size: Optional[int]\n        The batch size to use. If not provided, the batch size of the pipeline\n        object will be used.\n    accelerator: Optional[Union[str, Accelerator]]\n        The accelerator to use for processing the documents. If not provided,\n        the default accelerator will be used.\n    to_doc: ToDoc\n        The function to use to convert the inputs to PDFDoc objects. By default,\n        the `content` field of the inputs will be used if dict-like objects are\n        provided, otherwise the inputs will be passed directly to the pipeline.\n    from_doc: FromDoc\n        The function to use to convert the PDFDoc objects to outputs. By default,\n        the PDFDoc objects will be returned directly.\n\n    Returns\n    -------\n    Iterable[PDFDoc]\n    \"\"\"\n\n    if batch_size is None:\n        batch_size = self.batch_size\n\n    if accelerator is None:\n        accelerator = {\"@accelerator\": \"simple\", \"batch_size\": batch_size}\n    if isinstance(accelerator, str):\n        accelerator = {\"@accelerator\": accelerator, \"batch_size\": batch_size}\n    if isinstance(accelerator, dict):\n        accelerator = Config(accelerator).resolve(registry=registry)\n\n    kwargs = {\n        \"inputs\": inputs,\n        \"model\": self,\n        \"to_doc\": to_doc,\n        \"from_doc\": from_doc,\n    }\n    for k, v in list(kwargs.items()):\n        if v is None:\n            del kwargs[k]\n\n    with self.train(False):\n        return accelerator(**kwargs)\n</code></pre>"},{"location":"inference/#accelerators","title":"Accelerators","text":""},{"location":"inference/#edspdf.accelerators.simple.SimpleAccelerator","title":"Simple accelerator","text":"<p>             Bases: <code>Accelerator</code></p> <p>This is the simplest accelerator which batches the documents and process each batch on the main process (the one calling <code>.pipe()</code>).</p> <p>Examples:</p> <pre><code>docs = list(pipeline.pipe([content1, content2, ...]))\n</code></pre> <p>or, if you want to override the model defined batch size</p> <pre><code>docs = list(pipeline.pipe([content1, content2, ...], batch_size=8))\n</code></pre> <p>which is equivalent to passing a confit dict</p> <pre><code>docs = list(\n    pipeline.pipe(\n        [content1, content2, ...],\n        accelerator={\n            \"@accelerator\": \"simple\",\n            \"batch_size\": 8,\n        },\n    )\n)\n</code></pre> <p>or the instantiated accelerator directly</p> <pre><code>from edspdf.accelerators.simple import SimpleAccelerator\n\naccelerator = SimpleAccelerator(batch_size=8)\ndocs = list(pipeline.pipe([content1, content2, ...], accelerator=accelerator))\n</code></pre> <p>If you have a GPU, make sure to move the model to the appropriate device before calling <code>.pipe()</code>. If you have multiple GPUs, use the multiprocessing accelerator instead.</p> <pre><code>pipeline.to(\"cuda\")\ndocs = list(pipeline.pipe([content1, content2, ...]))\n</code></pre> PARAMETER  DESCRIPTION <code>batch_size</code> <p>The number of documents to process in each batch.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> Source code in <code>edspdf/accelerators/simple.py</code> <pre><code>def __init__(\n    self,\n    *,\n    batch_size: int = 32,\n):\n    self.batch_size = batch_size\n</code></pre>"},{"location":"inference/#edspdf.accelerators.multiprocessing.MultiprocessingAccelerator","title":"Multiprocessing accelerator","text":"<p>             Bases: <code>Accelerator</code></p> <p>If you have multiple CPU cores, and optionally multiple GPUs, we provide a <code>multiprocessing</code> accelerator that allows to run the inference on multiple processes.</p> <p>This accelerator dispatches the batches between multiple workers (data-parallelism), and distribute the computation of a given batch on one or two workers (model-parallelism). This is done by creating two types of workers:</p> <ul> <li>a <code>CPUWorker</code> which handles the non deep-learning components and the   preprocessing, collating and postprocessing of deep-learning components</li> <li>a <code>GPUWorker</code> which handles the forward call of the deep-learning components</li> </ul> <p>The advantage of dedicating a worker to the deep-learning components is that it allows to prepare multiple batches in parallel in multiple <code>CPUWorker</code>, and ensure that the <code>GPUWorker</code> never wait for a batch to be ready.</p> <p>The overall architecture described in the following figure, for 3 CPU workers and 2 GPU workers.</p> <p>Here is how a small pipeline with rule-based components and deep-learning components is distributed between the workers:</p> <p>Examples:</p> <pre><code>docs = list(\n    pipeline.pipe(\n        [content1, content2, ...],\n        accelerator={\n            \"@accelerator\": \"multiprocessing\",\n            \"num_cpu_workers\": 3,\n            \"num_gpu_workers\": 2,\n            \"batch_size\": 8,\n        },\n    )\n)\n</code></pre> PARAMETER  DESCRIPTION <code>batch_size</code> <p>Number of documents to process at a time in a CPU/GPU worker</p> <p> TYPE: <code>int</code> </p> <code>num_cpu_workers</code> <p>Number of CPU workers. A CPU worker handles the non deep-learning components and the preprocessing, collating and postprocessing of deep-learning components.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>num_gpu_workers</code> <p>Number of GPU workers. A GPU worker handles the forward call of the deep-learning components.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>gpu_pipe_names</code> <p>List of pipe names to accelerate on a GPUWorker, defaults to all pipes that inherit from TrainablePipe</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> Source code in <code>edspdf/accelerators/multiprocessing.py</code> <pre><code>def __init__(\n    self,\n    batch_size: int,\n    num_cpu_workers: Optional[int] = None,\n    num_gpu_workers: Optional[int] = None,\n    gpu_pipe_names: Optional[List[str]] = None,\n    gpu_worker_devices: Optional[List[Union[torch.device, str]]] = None,\n    cpu_worker_devices: Optional[List[Union[torch.device, str]]] = None,\n):\n    self.batch_size = batch_size\n    self.num_gpu_workers: Optional[int] = num_gpu_workers\n    self.num_cpu_workers = num_cpu_workers\n    self.gpu_pipe_names = gpu_pipe_names\n    self.gpu_worker_devices = gpu_worker_devices\n    self.cpu_worker_devices = cpu_worker_devices\n</code></pre>"},{"location":"inference/#edspdf.accelerators.multiprocessing.MultiprocessingAccelerator.__call__","title":"<code>__call__</code>","text":"<p>Stream of documents to process. Each document can be a string or a tuple</p> PARAMETER  DESCRIPTION <code>inputs</code> <p> TYPE: <code>Iterable[Any]</code> </p> <code>model</code> <p> TYPE: <code>Any</code> </p> YIELDS DESCRIPTION <code>Any</code> <p>Processed outputs of the pipeline</p> Source code in <code>edspdf/accelerators/multiprocessing.py</code> <pre><code>def __call__(\n    self,\n    inputs: Iterable[Any],\n    model: Any,\n    to_doc: ToDoc = FromDictFieldsToDoc(\"content\"),\n    from_doc: FromDoc = lambda doc: doc,\n):\n\"\"\"\n    Stream of documents to process. Each document can be a string or a tuple\n\n    Parameters\n    ----------\n    inputs\n    model\n\n    Yields\n    ------\n    Any\n        Processed outputs of the pipeline\n    \"\"\"\n    if torch.multiprocessing.get_start_method() != \"spawn\":\n        torch.multiprocessing.set_start_method(\"spawn\", force=True)\n\n    gpu_pipe_names = (\n        [\n            name\n            for name, component in model.pipeline\n            if isinstance(component, TrainablePipe)\n        ]\n        if self.gpu_pipe_names is None\n        else self.gpu_pipe_names\n    )\n\n    if not all(model.has_pipe(name) for name in gpu_pipe_names):\n        raise ValueError(\n            \"GPU accelerated pipes {} could not be found in the model\".format(\n                sorted(set(model.pipe_names) - set(gpu_pipe_names))\n            )\n        )\n\n    num_devices = torch.cuda.device_count()\n    print(f\"Number of available devices: {num_devices}\", flush=True)\n\n    num_cpu_workers = self.num_cpu_workers\n    num_gpu_workers = self.num_gpu_workers\n\n    if num_gpu_workers is None:\n        num_gpu_workers = num_devices if len(gpu_pipe_names) &gt; 0 else 0\n\n    if num_cpu_workers is None:\n        num_cpu_workers = max(\n            min(mp.cpu_count() - num_gpu_workers, DEFAULT_MAX_CPU_WORKERS), 0\n        )\n\n    if num_gpu_workers == 0:\n        gpu_pipe_names = []\n\n    gpu_worker_devices = (\n        [\n            torch.device(f\"cuda:{gpu_idx * num_devices // num_gpu_workers}\")\n            for gpu_idx in range(num_gpu_workers)\n        ]\n        if self.gpu_worker_devices is None\n        else self.gpu_worker_devices\n    )\n    cpu_worker_devices = (\n        [\"cpu\"] * num_cpu_workers\n        if self.cpu_worker_devices is None\n        else self.cpu_worker_devices\n    )\n    assert len(cpu_worker_devices) == num_cpu_workers\n    assert len(gpu_worker_devices) == num_gpu_workers\n    if num_cpu_workers == 0:\n        (\n            num_cpu_workers,\n            num_gpu_workers,\n            cpu_worker_devices,\n            gpu_worker_devices,\n            gpu_pipe_names,\n        ) = (num_gpu_workers, 0, gpu_worker_devices, [], [])\n\n    debug(f\"Number of CPU workers: {num_cpu_workers}\")\n    debug(f\"Number of GPU workers: {num_gpu_workers}\")\n\n    exchanger = Exchanger(\n        num_stages=len(gpu_pipe_names),\n        num_cpu_workers=num_cpu_workers,\n        num_gpu_workers=num_gpu_workers,\n        gpu_worker_devices=gpu_worker_devices,\n    )\n\n    cpu_workers = []\n    gpu_workers = []\n    model = model.to(\"cpu\")\n\n    for gpu_idx in range(num_gpu_workers):\n        gpu_workers.append(\n            GPUWorker(\n                gpu_idx=gpu_idx,\n                exchanger=exchanger,\n                gpu_pipe_names=gpu_pipe_names,\n                model=model,\n                device=gpu_worker_devices[gpu_idx],\n            )\n        )\n\n    for cpu_idx in range(num_cpu_workers):\n        cpu_workers.append(\n            CPUWorker(\n                cpu_idx=cpu_idx,\n                exchanger=exchanger,\n                gpu_pipe_names=gpu_pipe_names,\n                model=model,\n                device=cpu_worker_devices[cpu_idx],\n            )\n        )\n\n    for worker in (*cpu_workers, *gpu_workers):\n        worker.start()\n\n    try:\n        num_max_enqueued = num_cpu_workers * 2 + 10\n        # Number of input/output batch per process\n        total_inputs = [0] * num_cpu_workers\n        total_outputs = [0] * num_cpu_workers\n        outputs_iterator = exchanger.iter_results()\n\n        cpu_worker_indices = list(range(num_cpu_workers))\n        inputs_iterator = (to_doc(i) for i in inputs)\n        for i, pdfs_batch in enumerate(batchify(inputs_iterator, self.batch_size)):\n            if sum(total_inputs) - sum(total_outputs) &gt;= num_max_enqueued:\n                outputs, cpu_idx, gpu_idx = next(outputs_iterator)\n                if isinstance(outputs, BaseException):\n                    raise outputs  # pragma: no cover\n                yield from (from_doc(o) for o in outputs)\n                total_outputs[cpu_idx] += 1\n\n            # Shuffle to ensure the first process does not receive all the documents\n            # in case of total_inputs - total_outputs equality\n            shuffle(cpu_worker_indices)\n            cpu_idx = min(\n                cpu_worker_indices,\n                key=lambda i: total_inputs[i] - total_outputs[i],\n            )\n            exchanger.put_cpu(pdfs_batch, stage=0, idx=cpu_idx)\n            total_inputs[cpu_idx] += 1\n\n        while sum(total_outputs) &lt; sum(total_inputs):\n            outputs, cpu_idx, gpu_idx = next(outputs_iterator)\n            if isinstance(outputs, BaseException):\n                raise outputs  # pragma: no cover\n            yield from (from_doc(o) for o in outputs)\n            total_outputs[cpu_idx] += 1\n    finally:\n        # Send gpu and cpu process the order to stop processing data\n        # We use the prioritized queue to ensure the stop signal is processed\n        # before the next batch of data\n        for i, worker in enumerate(gpu_workers):\n            exchanger.gpu_inputs_queues[i][-1].put(None)\n            debug(\"Asked gpu worker\", i, \"to stop processing data\")\n        for i, worker in enumerate(cpu_workers):\n            exchanger.cpu_inputs_queues[i][-1].put(None)\n            debug(\"Asked cpu worker\", i, \"to stop processing data\")\n\n        # Enqueue a final non prioritized STOP signal to ensure there remains no\n        # data in the queues (cf drain loop in CPUWorker / GPUWorker)\n        for i, worker in enumerate(gpu_workers):\n            exchanger.gpu_inputs_queues[i][0].put(None)\n            debug(\"Asked gpu\", i, \"to end\")\n        for i, worker in enumerate(gpu_workers):\n            worker.join(timeout=5)\n            debug(\"Joined gpu worker\", i)\n        for i, worker in enumerate(cpu_workers):\n            exchanger.cpu_inputs_queues[i][0].put(None)\n            debug(\"Asked cpu\", i, \"to end\")\n        for i, worker in enumerate(cpu_workers):\n            worker.join(timeout=1)\n            debug(\"Joined cpu worker\", i)\n\n        # If a worker is still alive, kill it\n        # This should not happen, but for a reason I cannot explain, it does in\n        # some CPU workers sometimes when we catch an error, even though each run\n        # method of the workers completes cleanly. Maybe this has something to do\n        # with the cleanup of these processes ?\n        for i, worker in enumerate(gpu_workers):  # pragma: no cover\n            if worker.is_alive():\n                print(\"Killing gpu worker\", i)\n                worker.kill()\n        for i, worker in enumerate(cpu_workers):  # pragma: no cover\n            if worker.is_alive():\n                print(\"Killing cpu worker\", i)\n                worker.kill()\n</code></pre>"},{"location":"pipeline/","title":"Pipeline","text":"<p>The goal of EDS-PDF is to provide a framework for processing PDF documents, along with some utilities and a few components, stitched together by a robust pipeline and configuration system.</p> <p>Processing PDFs usually involves many steps such as extracting lines, running OCR models, detecting and classifying boxes, filtering and aggregating parts of the extracted texts, etc. Organising these steps together, combining static and deep learning components, while remaining modular and efficient is a challenge. This is why EDS-PDF is built on top of a new pipelining system.</p> <p>Deep learning frameworks</p> <p>The EDS-PDF trainable components are built around the PyTorch framework. While you can use any technology in static components, we do not provide tools to train components built with other deep learning frameworks.</p>"},{"location":"pipeline/#creating-a-pipeline","title":"Creating a pipeline","text":"<p>A pipe is a processing block (like a function) that applies a transformation on its input and returns a modified object.</p> <p>At the moment, four types of pipes are implemented in the library:</p> <ol> <li>extraction components extract lines from a raw PDF and return a <code>PDFDoc</code> object filled with these text boxes.</li> <li>classification components classify each box with labels, such as <code>body</code>, <code>header</code>, <code>footer</code>...</li> <li>aggregation components compiles the lines together according to their classes to re-create the original text.</li> <li>embedding components don't directly update the annotations on the document but have specific deep-learning methods (see the TrainablePipe page) that can be composed to form a machine learning model.</li> </ol> <p>To create your first pipeline, execute the following code:</p> <pre><code>from edspdf import Pipeline\n\nmodel = Pipeline()\n# will extract text lines from a document\nmodel.add_pipe(\n    \"pdfminer-extractor\",\n    config=dict(\n        extract_style=False,\n    ),\n)\n# classify everything inside the `body` bounding box as `body`\nmodel.add_pipe(\n    \"mask-classifier\", config=dict(body={\"x0\": 0.1, \"y0\": 0.1, \"x1\": 0.9, \"y1\": 0.9})\n)\n# aggregates the lines together to re-create the original text\nmodel.add_pipe(\"simple-aggregator\")\n</code></pre> <p>This pipeline can then be run on one or more PDF documents. As the pipeline process documents, components will be called in the order they were added to the pipeline.</p> <pre><code>from pathlib import Path\n\npdf_bytes = Path(\"path/to/your/pdf\").read_bytes()\n\n# Processing one document\nmodel(pdf_bytes)\n\n# Processing multiple documents\nmodel.pipe([pdf_bytes, ...])\n</code></pre> <p>For more information on how to use the pipeline, refer to the Inference page.</p>"},{"location":"pipeline/#hybrid-models","title":"Hybrid models","text":"<p>EDS-PDF was designed to facilitate the training and inference of hybrid models that arbitrarily chain static components or trained deep learning components. Static components are callable objects that take a PDFDoc object as input, perform arbitrary transformations over the input, and return the modified object. Trainable pipes, on the other hand, allow for deep learning operations to be performed on the PDFDoc object and must be trained to be used.</p>"},{"location":"pipeline/#saving-and-loading-a-pipeline","title":"Saving and loading a pipeline","text":"<p>Pipelines can be saved and loaded using the <code>save</code> and <code>load</code> methods. The saved pipeline is not a pickled objet but a folder containing the config file, the weights and extra resources for each pipeline. This allows for easy inspection and modification of the pipeline, and avoids the execution of arbitrary code when loading a pipeline.</p> <pre><code>model.save(\"path/to/your/model\")\nmodel = edspdf.load(\"path/to/your/model\")\n</code></pre> <p>To share the pipeline and turn it into a pip installable package, you can use the <code>package</code> method, which will use or create a pyproject.toml file, fill it accordingly, and create a wheel file. At the moment, we only support the poetry package manager.</p> <pre><code>model.package(\n    name=\"your-package-name\",  # leave None to reuse name in pyproject.toml\n    version=\"0.0.1\",\n    root_dir=\"path/to/project/root\",  # optional, to retrieve an existing pyproject.toml file\n    # if you don't have a pyproject.toml, you can provide the metadata here instead\n    metadata=dict(\n        authors=\"Firstname Lastname &lt;your.email@domain.fr&gt;\",\n        description=\"A short description of your package\",\n    ),\n)\n</code></pre> <p>This will create a wheel file in the root_dir/dist folder, which you can share and install with pip</p>"},{"location":"roadmap/","title":"Roadmap","text":"<ul> <li> Style extraction</li> <li> Custom hybrid torch-based pipeline &amp; configuration system</li> <li> Drop pandas DataFrame in favour of a ~~Cython~~ attr wrapper around PDF documents?</li> <li> Add training capabilities with a CLI to automate the annotation/preparation/training loop.       Again, draw inspiration from spaCy, and maybe add the notion of a <code>TrainableClassifier</code>...</li> <li> Add complete serialisation capabilities, to save a full pipeline to disk.       Draw inspiration from spaCy, which took great care to solve these issues:       add <code>save</code> and <code>load</code> methods to every pipeline component</li> <li> Multiple-column extraction</li> <li> Table detector</li> <li> Integrate third-party OCR module</li> </ul>"},{"location":"trainable-pipes/","title":"Trainable pipes","text":"<p>Trainable pipes allow for deep learning operations to be performed on the PDFDoc object and must be trained to be used. Such pipes can be used to train a model to predict the label of the lines extracted from a PDF document.</p>"},{"location":"trainable-pipes/#anatomy-of-a-trainable-pipe","title":"Anatomy of a trainable pipe","text":"<p>Building and running deep learning models usually requires preprocessing the input sample into features, batching or \"collating\" these features together to process multiple samples at once, running deep learning operations over these features (in Pytorch, this step is done in the <code>forward</code> method) and postprocessing the outputs of these operation to complete the original sample.</p> <p>In the trainable pipes of EDS-PDF, preprocessing and postprocessing are decoupled from the deep learning code but collocated with the forward method. This is achieved by splitting the class of a trainable component into four methods, which allows us to keep the development of new deep-learning components simple while ensuring efficient models both during training and inference.</p>"},{"location":"trainable-pipes/#edspdf.trainable_pipe.TrainablePipe.preprocess","title":"<code>preprocess</code>","text":"<p>Preprocess the document to extract features that will be used by the neural network to perform its predictions.</p> PARAMETER  DESCRIPTION <code>doc</code> <p>PDFDocument to preprocess</p> <p> TYPE: <code>PDFDoc</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dictionary (optionally nested) containing the features extracted from the document.</p>"},{"location":"trainable-pipes/#edspdf.trainable_pipe.TrainablePipe.collate","title":"<code>collate</code>","text":"<p>Collate the batch of features into a single batch of tensors that can be used by the forward method of the component.</p> PARAMETER  DESCRIPTION <code>batch</code> <p>Batch of features</p> <p> TYPE: <code>NestedSequences</code> </p> <code>device</code> <p>Device on which the tensors should be moved</p> <p> TYPE: <code>device</code> </p> RETURNS DESCRIPTION <code>InputBatch</code> <p>Dictionary (optionally nested) containing the collated tensors</p>"},{"location":"trainable-pipes/#edspdf.trainable_pipe.TrainablePipe.forward","title":"<code>forward</code>","text":"<p>Perform the forward pass of the neural network, i.e, apply transformations over the collated features to compute new embeddings, probabilities, losses, etc</p> PARAMETER  DESCRIPTION <code>batch</code> <p>Batch of tensors (nested dictionary) computed by the collate method</p> <p> TYPE: <code>InputBatch</code> </p> RETURNS DESCRIPTION <code>OutputBatch</code>"},{"location":"trainable-pipes/#edspdf.trainable_pipe.TrainablePipe.postprocess","title":"<code>postprocess</code>","text":"<p>Update the documents with the predictions of the neural network, for instance converting label probabilities into label attributes on the document lines.</p> <p>By default, this is a no-op.</p> PARAMETER  DESCRIPTION <code>docs</code> <p>Batch of documents</p> <p> TYPE: <code>Sequence[PDFDoc]</code> </p> <code>batch</code> <p>Batch of predictions, as returned by the forward method</p> <p> TYPE: <code>OutputBatch</code> </p> RETURNS DESCRIPTION <code>Sequence[PDFDoc]</code> <p>Additionally, there is a fifth method:</p>"},{"location":"trainable-pipes/#edspdf.trainable_pipe.TrainablePipe.post_init","title":"<code>post_init</code>","text":"<p>This method completes the attributes of the component, by looking at some documents. It is especially useful to build vocabularies or detect the labels of a classification task.</p> PARAMETER  DESCRIPTION <code>gold_data</code> <p>The documents to use for initialization.</p> <p> TYPE: <code>Iterable[PDFDoc]</code> </p> <code>exclude</code> <p>The names of components to exclude from initialization. This argument will be gradually updated  with the names of initialized components</p> <p> TYPE: <code>set</code> </p>"},{"location":"trainable-pipes/#implementing-a-trainable-component","title":"Implementing a trainable component","text":"<p>Here is an example of a trainable component:</p> <pre><code>from typing import Any, Dict, Iterable, Sequence\n\nimport torch\nfrom tqdm import tqdm\n\nfrom edspdf import Pipeline, TrainablePipe, registry\nfrom edspdf.structures import PDFDoc\n\n\n@registry.factory.register(\"my-component\")\nclass MyComponent(TrainablePipe):\n    def __init__(\n        self,\n        # A subcomponent\n        pipeline: Pipeline,\n        name: str,\n        embedding: TrainablePipe,\n    ):\n        super().__init__(pipeline=pipeline, name=name)\n        self.embedding = embedding\n\n    def post_init(self, gold_data: Iterable[PDFDoc], exclude: set):\n        # Initialize the component with the gold documents\n        with self.label_vocabulary.initialization():\n            for doc in tqdm(gold_data, desc=\"Initializing the component\"):\n                # Do something like learning a vocabulary over the initialization\n                # documents\n                ...\n\n        # And post_init the subcomponent\n        exclude.add(self.name)\n        self.embedding.post_init(gold_data, exclude)\n\n        # Initialize any layer that might be missing from the module\n        self.classifier = torch.nn.Linear(...)\n\n    def preprocess(self, doc: PDFDoc, supervision: bool = False) -&gt; Dict[str, Any]:\n        # Preprocess the doc to extract features required to run the embedding\n        # subcomponent, and this component\n        return {\n            \"embedding\": self.embedding.preprocess_supervised(doc),\n            \"my-feature\": ...(doc),\n        }\n\n    def collate(self, batch, device: torch.device) -&gt; Dict:\n        # Collate the features of the \"embedding\" subcomponent\n        # and the features of this component as well\n        return {\n            \"embedding\": self.embedding.collate(batch[\"embedding\"], device),\n            \"my-feature\": torch.as_tensor(batch[\"my-feature\"], device=device),\n        }\n\n    def forward(self, batch: Dict, supervision=False) -&gt; Dict:\n        # Call the embedding subcomponent\n        embeds = self.embedding(batch[\"embedding\"])\n\n        # Do something with the embedding tensors\n        output = ...(embeds)\n\n        return output\n\n    def postprocess(self, docs: Sequence[PDFDoc], output: Dict) -&gt; Sequence[PDFDoc]:\n        # Annotate the docs with the outputs of the forward method\n        ...\n        return docs\n</code></pre>"},{"location":"trainable-pipes/#nesting-trainable-pipes","title":"Nesting trainable pipes","text":"<p>Like pytorch modules, you can compose trainable pipes together to build complex architectures. For instance, a trainable classifier component may delegate some of its logic to an embedding component, which will only be responsible for converting PDF lines into multidimensional arrays of numbers.</p> <p>Nesting pipes allows switching parts of the neural networks to test various architectures and keeping the modelling logic modular.</p>"},{"location":"trainable-pipes/#sharing-subcomponents","title":"Sharing subcomponents","text":"<p>Sharing parts of a neural network while training on different tasks can be an effective way to improve the network efficiency. For instance, it is common to share an embedding layer between multiple tasks that require embedding the same inputs.</p> <p>In EDS-PDF, sharing a subcomponent is simply done by sharing the object between the multiple pipes. You can either refer to an existing subcomponent when configuring a new component in Python, or use the interpolation mechanism of our configuration system.</p> API-basedConfiguration-based <pre><code>pipeline.add_pipe(\n    \"my-component-1\",\n    name=\"first\",\n    config={\n        \"embedding\": {\n            \"@factory\": \"box-embedding\",\n            # ...\n        }\n    },\n)\npipeline.add_pipe(\n    \"my-component-2\",\n    name=\"second\",\n    config={\n        \"embedding\": pipeline.components.first.embedding,\n    },\n)\n</code></pre> <pre><code>[components.first]\n@factory = \"my-component-1\"\n\n[components.first.embedding]\n@factory = \"box-embedding\"\n...\n\n[components.second]\n@factory = \"my-component-2\"\nembedding = ${components.first.embedding}\n</code></pre> <p>To avoid recomputing the <code>preprocess</code> / <code>forward</code> and <code>collate</code> in the multiple components that use it, we rely on a light cache system.</p> <p>During the training loop, when computing the loss for each component, the forward calls must be wrapped by the <code>pipeline.cache()</code> context to enable this caching mechanism between components.</p>"},{"location":"layers/","title":"Deep learning layers","text":"<p>EDS-PDF provides a set of specialized deep learning layers that can be used to build trainable components. These layers are built on top of the PyTorch framework and can be used in any PyTorch model.</p> Layer Description <code>BoxTransformerModule</code> Contextualize box embeddings with a 2d Transformer with relative position representations <code>BoxTransformerLayer</code> A single layer of the above <code>BoxTransformerModule</code> layer <code>RelativeAttention</code> A 2d attention layer that optionally uses relative position to compute its attention scores <code>SinusoidalEmbedding</code> A position embedding that uses trigonometric functions to encode positions <code>Vocabulary</code> A non deep learning layer to encodes / decode vocabularies"},{"location":"layers/box-transformer-layer/","title":"BoxTransformerLayer","text":"<p>BoxTransformerLayer combining a self attention layer and a linear-&gt;activation-&gt;linear transformation. This layer is used in the BoxTransformerModule module.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>Input embedding size</p> <p> TYPE: <code>int</code> </p> <code>num_heads</code> <p>Number of attention heads in the attention layer</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>dropout_p</code> <p>Dropout probability both for the attention layer and embedding projections</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>head_size</code> <p>Head sizes of the attention layer</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>activation</code> <p>Activation function used in the linear-&gt;activation-&gt;linear transformation</p> <p> TYPE: <code>ActivationFunction</code> DEFAULT: <code>'gelu'</code> </p> <code>init_resweight</code> <p>Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attention_mode</code> <p>Mode of relative position infused attention layer. See the relative attention documentation for more information.</p> <p> TYPE: <code>Sequence[Literal['c2c', 'c2p', 'p2c']]</code> DEFAULT: <code>('c2c', 'c2p', 'p2c')</code> </p> <code>position_embedding</code> <p>Position embedding to use as key/query position embedding in the attention computation.</p> <p> TYPE: <code>Optional[Union[FloatTensor, Parameter]]</code> DEFAULT: <code>None</code> </p>"},{"location":"layers/box-transformer-layer/#edspdf.layers.box_transformer.BoxTransformerLayer.forward","title":"<code>forward</code>","text":"<p>Forward pass of the BoxTransformerLayer</p> PARAMETER  DESCRIPTION <code>embeds</code> <p>Embeddings to contextualize Shape: <code>n_samples * n_keys * input_size</code></p> <p> TYPE: <code>FloatTensor</code> </p> <code>mask</code> <p>Mask of the embeddings. 0 means padding element. Shape: <code>n_samples * n_keys</code></p> <p> TYPE: <code>BoolTensor</code> </p> <code>relative_positions</code> <p>Position of the keys relatively to the query elements Shape: <code>n_samples * n_queries * n_keys * n_coordinates (2 for x/y)</code></p> <p> TYPE: <code>LongTensor</code> </p> <code>no_position_mask</code> <p>Key / query pairs for which the position attention terms should be disabled. Shape: <code>n_samples * n_queries * n_keys</code></p> <p> TYPE: <code>Optional[BoolTensor]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tuple[FloatTensor, FloatTensor]</code> <ul> <li>Contextualized embeddings   Shape: <code>n_samples * n_queries * n_keys</code></li> <li>Attention logits   Shape: <code>n_samples * n_queries * n_keys * n_heads</code></li> </ul>"},{"location":"layers/box-transformer/","title":"BoxTransformerModule","text":"<p>Box Transformer architecture combining a multiple BoxTransformerLayer modules. It is mainly used in BoxTransformer.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>Input embedding size</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>num_heads</code> <p>Number of attention heads in the attention layers</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>n_relative_positions</code> <p>Maximum range of embeddable relative positions between boxes (further distances are capped to \u00b1n_relative_positions // 2)</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>dropout_p</code> <p>Dropout probability both for the attention layers and embedding projections</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>head_size</code> <p>Head sizes of the attention layers</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>activation</code> <p>Activation function used in the linear-&gt;activation-&gt;linear transformations</p> <p> TYPE: <code>ActivationFunction</code> DEFAULT: <code>'gelu'</code> </p> <code>init_resweight</code> <p>Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attention_mode</code> <p>Mode of relative position infused attention layer. See the relative attention documentation for more information.</p> <p> TYPE: <code>Sequence[Literal['c2c', 'c2p', 'p2c']]</code> DEFAULT: <code>('c2c', 'c2p', 'p2c')</code> </p> <code>n_layers</code> <p>Number of layers in the Transformer</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p>"},{"location":"layers/box-transformer/#edspdf.layers.box_transformer.BoxTransformerModule.forward","title":"<code>forward</code>","text":"<p>Forward pass of the BoxTransformer</p> PARAMETER  DESCRIPTION <code>embeds</code> <p>Embeddings to contextualize Shape: <code>n_samples * n_keys * input_size</code></p> <p> TYPE: <code>FoldedTensor</code> </p> <code>boxes</code> <p>Layout features of the input elements</p> <p> TYPE: <code>Dict</code> </p> RETURNS DESCRIPTION <code>Tuple[FloatTensor, List[FloatTensor]]</code> <ul> <li>Output of the last BoxTransformerLayer   Shape: <code>n_samples * n_queries * n_keys</code></li> <li>Attention logits of all layers   Shape: <code>n_samples * n_queries * n_keys * n_heads</code></li> </ul>"},{"location":"layers/relative-attention/","title":"RelativeAttention","text":"<p>A self/cross-attention layer that takes relative position of elements into account to compute the attention weights. When running a relative attention layer, key and queries are represented using content and position embeddings, where position embeddings are retrieved using the relative position of keys relative to queries</p> PARAMETER  DESCRIPTION <code>size</code> <p>The size of the output embeddings Also serves as default if query_size, pos_size, or key_size is None</p> <p> TYPE: <code>int</code> </p> <code>n_heads</code> <p>The number of attention heads</p> <p> TYPE: <code>int</code> </p> <code>query_size</code> <p>The size of the query embeddings.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>key_size</code> <p>The size of the key embeddings.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>value_size</code> <p>The size of the value embeddings</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>head_size</code> <p>The size of each query / key / value chunk used in the attention dot product Default: <code>key_size / n_heads</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>position_embedding</code> <p>The position embedding used as key and query embeddings</p> <p> TYPE: <code>Optional[Union[FloatTensor, Parameter]]</code> DEFAULT: <code>None</code> </p> <code>dropout_p</code> <p>Dropout probability applied on the attention weights Default: 0.1</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>same_key_query_proj</code> <p>Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>same_positional_key_query_proj</code> <p>Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>n_coordinates</code> <p>The number of positional coordinates For instance, text is 1D so 1 coordinate, images are 2D so 2 coordinates ... Default: 1</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>head_bias</code> <p>Whether to learn a bias term to add to the attention logits This is only useful if you plan to use the attention logits for subsequent operations, since attention weights are unaffected by bias terms.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>do_pooling</code> <p>Whether to compute the output embedding. If you only plan to use attention logits, you should disable this parameter. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>mode</code> <p>Whether to compute content to content (c2c), content to position (c2p) or position to content (p2c) attention terms. Setting <code>mode=('c2c\")</code> disable relative position attention terms: this is the standard attention layer. To get a better intuition about these different types of attention, here is a formulation as fictitious search samples from a word in a (1D) text:</p> <ul> <li>content-content : \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other   words whose content contains information about temporality\"</li> <li>content-position: \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other   words that are 3 positions after of me\"</li> <li>position-content : \"regardless of my content, I will attend to the word   one position after from me if it contains information about temporality,   two words after me if it contains information about location, etc.\"</li> </ul> <p> TYPE: <code>Sequence[Literal['c2c', 'c2p', 'p2c']]</code> DEFAULT: <code>('c2c', 'p2c', 'c2p')</code> </p> <code>n_additional_heads</code> <p>The number of additional head logits to compute. Those are not used to compute output embeddings, but may be useful in subsequent operation. Default: 0</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p>"},{"location":"layers/relative-attention/#edspdf.layers.relative_attention.RelativeAttention.forward","title":"<code>forward</code>","text":"<p>Forward pass of the RelativeAttention layer.</p> PARAMETER  DESCRIPTION <code>content_queries</code> <p>The content query embedding to use in the attention computation Shape: <code>n_samples * n_queries * query_size</code></p> <p> TYPE: <code>FloatTensor</code> </p> <code>content_keys</code> <p>The content key embedding to use in the attention computation. If None, defaults to the <code>content_queries</code> Shape: <code>n_samples * n_keys * query_size</code></p> <p> TYPE: <code>Optional[FloatTensor]</code> DEFAULT: <code>None</code> </p> <code>content_values</code> <p>The content values embedding to use in the final pooling computation. If None, pooling won't be performed. Shape: <code>n_samples * n_keys * query_size</code></p> <p> TYPE: <code>Optional[FloatTensor]</code> DEFAULT: <code>None</code> </p> <code>mask</code> <p>The content key embedding to use in the attention computation. If None, defaults to the <code>content_queries</code> Shape: either - <code>n_samples * n_keys</code> - <code>n_samples * n_queries * n_keys</code> - <code>n_samples * n_queries * n_keys * n_heads</code></p> <p> TYPE: <code>Optional[BoolTensor]</code> DEFAULT: <code>None</code> </p> <code>relative_positions</code> <p>The relative position of keys relative to queries If None, positional attention terms won't be computed. Shape: <code>n_samples * n_queries * n_keys * n_coordinates</code></p> <p> TYPE: <code>Optional[LongTensor]</code> DEFAULT: <code>None</code> </p> <code>no_position_mask</code> <p>Key / query pairs for which the position attention terms should be disabled. Shape: <code>n_samples * n_queries * n_keys</code></p> <p> TYPE: <code>Optional[BoolTensor]</code> DEFAULT: <code>None</code> </p> <code>base_attn</code> <p>Attention logits to add to the computed attention logits Shape: <code>n_samples * n_queries * n_keys * n_heads</code></p> <p> TYPE: <code>Optional[FloatTensor]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[FloatTensor, FloatTensor], FloatTensor]</code> <ul> <li>the output contextualized embeddings (only if content_values is not None   and the <code>do_pooling</code> attribute is set to True)   Shape: n_sample * n_keys * <code>size</code></li> <li>the attention logits   Shape: n_sample * n_keys * n_queries * (n_heads + n_additional_heads)</li> </ul>"},{"location":"layers/sinusoidal-embedding/","title":"SinusoidalEmbedding","text":"<p>A position embedding lookup table that stores embeddings for a fixed number of positions. The value of each of the <code>embedding_dim</code> channels of the generated embedding is generated according to a trigonometric function (sin for even channels, cos for odd channels). The frequency of the signal in each pair of channels varies according to the temperature parameter.</p> <p>Any input position above the maximum value <code>num_embeddings</code> will be capped to <code>num_embeddings - 1</code></p> PARAMETER  DESCRIPTION <code>num_embeddings</code> <p>The maximum number of position embeddings store in this table</p> <p> TYPE: <code>int</code> </p> <code>embedding_dim</code> <p>The embedding size</p> <p> TYPE: <code>int</code> </p> <code>temperature</code> <p>The temperature controls the range of frequencies used by each channel of the embedding</p> <p> TYPE: <code>float</code> DEFAULT: <code>10000.0</code> </p>"},{"location":"layers/sinusoidal-embedding/#edspdf.layers.sinusoidal_embedding.SinusoidalEmbedding.forward","title":"<code>forward</code>","text":"<p>Forward pass of the SinusoidalEmbedding module</p> PARAMETER  DESCRIPTION <code>indices</code> <p>Shape: any</p> <p> TYPE: <code>LongTensor</code> </p> RETURNS DESCRIPTION <code>FloatTensor</code> <p>Shape: <code>(*input_shape, embedding_dim)</code></p>"},{"location":"layers/vocabulary/","title":"Vocabulary","text":"<p>Vocabulary layer. This is not meant to be used as a <code>torch.nn.Module</code> but subclassing <code>torch.nn.Module</code> makes the instances appear when printing a model, which is nice.</p> PARAMETER  DESCRIPTION <code>items</code> <p>Initial vocabulary elements if any. Specific elements such as padding and unk can be set here to enforce their index in the vocabulary.</p> <p> TYPE: <code>Sequence[T]</code> DEFAULT: <code>None</code> </p> <code>default</code> <p>Default index to use for out of vocabulary elements Defaults to -100</p> <p> TYPE: <code>int</code> DEFAULT: <code>-100</code> </p>"},{"location":"layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary-functions","title":"Functions","text":""},{"location":"layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary.initialization","title":"<code>initialization</code>","text":"<p>Enters the initialization mode. Out of vocabulary elements will be assigned an index.</p>"},{"location":"layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary.encode","title":"<code>encode</code>","text":"<p>Converts an element into its vocabulary index If the layer is in its initialization mode (<code>with vocab.initialization(): ...</code>), and the element is out of vocabulary, a new index will be created and returned. Otherwise, any oov element will be encoded with the <code>default</code> index.</p> PARAMETER  DESCRIPTION <code>item</code> <p> </p> RETURNS DESCRIPTION <code>int</code>"},{"location":"layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary.decode","title":"<code>decode</code>","text":"<p>Converts an index into its original value</p> PARAMETER  DESCRIPTION <code>idx</code> <p> </p> RETURNS DESCRIPTION <code>InputT</code>"},{"location":"pipes/","title":"Components overview","text":"<p>EDS-PDF provides easy-to-use components for defining PDF processing pipelines.</p> Box extractorsBox classifiersAggregatorsEmbeddings Factory name Description <code>pdfminer-extractor</code> Extracts text lines with the <code>pdfminer</code> library <code>mupdf-extractor</code> Extracts text lines with the <code>pymupdf</code> library <code>poppler-extractor</code> Extracts text lines with the <code>poppler</code> library Factory name Description <code>mask-classifier</code> Simple rule-based classification <code>multi-mask-classifier</code> Simple rule-based classification <code>dummy-classifier</code> Dummy classifier, for testing purposes. <code>random-classifier</code> To sow chaos <code>trainable-classifier</code> Trainable box classification model Factory name Description <code>simple-aggregator</code> Returns a dictionary with one key for each detected class <p></p> Factory name Description <code>simple-text-embedding</code> A module that embeds the textual features of the blocks. <code>embedding-combiner</code> Encodes boxes using a combination of multiple encoders <code>sub-box-cnn-pooler</code> Pools the output of a CNN over the elements of a box (like words) <code>box-layout-embedding</code> Encodes the layout of the boxes <code>box-transformer</code> Contextualizes box representations using a transformer <code>huggingface-embedding</code> Box representations using a Huggingface multi-modal model. <p>You can add them to your EDS-PDF pipeline by simply calling <code>add_pipe</code>, for instance:</p> <pre><code># \u2191 Omitted code that defines the pipeline object \u2191\npipeline.add_pipe(\"pdfminer-extractor\", name=\"component-name\", config=...)\n</code></pre>"},{"location":"pipes/aggregators/","title":"Aggregation","text":"<p>The aggregation step compiles extracted text blocs together according to their detected class.</p> Factory name Description <code>simple-aggregator</code> Returns a dictionary with one key for each detected class"},{"location":"pipes/aggregators/simple-aggregator/","title":"Simple aggregator","text":""},{"location":"pipes/aggregators/simple-aggregator/#edspdf.pipes.aggregators.simple.SimpleAggregator","title":"<code>SimpleAggregator</code>","text":"<p>Aggregator that returns texts and styles. It groups all text boxes with the same label under the <code>aggregated_text</code>, and additionally aggregates the styles of the text boxes.</p> <p>Examples:</p> <p>Create a pipeline</p> API-basedConfiguration-based <pre><code>pipeline = ...\npipeline.add_pipe(\n    \"simple-aggregator\",\n    name=\"aggregator\",\n    config={\n        \"new_line_threshold\": 0.2,\n        \"new_paragraph_threshold\": 1.5,\n        \"label_map\": {\n            \"body\": \"text\",\n            \"table\": \"text\",\n        },\n    },\n)\n</code></pre> <pre><code>...\n\n[components.aggregator]\n@factory = \"simple-aggregator\"\nnew_line_threshold = 0.2\nnew_paragraph_threshold = 1.5\nlabel_map = { body = \"text\", table = \"text\" }\n\n...\n</code></pre> <p>and run it on a document:</p> <pre><code>doc = pipeline(doc)\nprint(doc.aggregated_texts)\n# {\n#     \"text\": \"This is the body of the document, followed by a table | A | B |\"\n# }\n</code></pre> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'simple-aggregator'</code> </p> <code>sort</code> <p>Whether to sort text boxes inside each label group by (page, y, x) position before merging them.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>new_line_threshold</code> <p>Minimum ratio of the distance between two lines to the median height of lines to consider them as being on separate lines</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>new_paragraph_threshold</code> <p>Minimum ratio of the distance between two lines to the median height of lines to consider them as being on separate paragraphs and thus add a newline character between them.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.5</code> </p> <code>label_map</code> <p>A dictionary mapping labels to new labels. This is useful to group labels together, for instance, to output both \"body\" and \"table\" as \"text\".</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p> Source code in <code>edspdf/pipes/aggregators/simple.py</code> <pre><code>def __init__(\n    self,\n    pipeline: Pipeline = None,\n    name: str = \"simple-aggregator\",\n    sort: bool = False,\n    new_line_threshold: float = 0.2,\n    new_paragraph_threshold: float = 1.5,\n    label_map: Dict = {},\n) -&gt; None:\n    self.name = name\n    self.sort = sort\n    self.label_map = dict(label_map)\n    self.new_line_threshold = new_line_threshold\n    self.new_paragraph_threshold = new_paragraph_threshold\n</code></pre>"},{"location":"pipes/box-classifiers/","title":"Box classifiers","text":"<p>We developed EDS-PDF with modularity in mind. To that end, you can choose between multiple classification methods.</p> Factory name Description <code>mask-classifier</code> Simple rule-based classification <code>multi-mask-classifier</code> Simple rule-based classification <code>dummy-classifier</code> Dummy classifier, for testing purposes. <code>random-classifier</code> To sow chaos <code>trainable-classifier</code> Trainable box classification model"},{"location":"pipes/box-classifiers/dummy/","title":"Dummy classifier","text":"<p>Dummy classifier, for chaos purposes. Classifies each line to a random element.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object.</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'dummy-classifier'</code> </p> <code>label</code> <p>The label to assign to each line.</p> <p> TYPE: <code>str</code> </p>"},{"location":"pipes/box-classifiers/mask/","title":"Mask Classification","text":"<p>We developed a simple classifier that roughly uses the same strategy as PDFBox, namely:</p> <ul> <li>define a \"mask\" on the PDF documents ;</li> <li>keep every text bloc within that mask, tag everything else as pollution.</li> </ul>"},{"location":"pipes/box-classifiers/mask/#factories","title":"Factories","text":"<p>Two factories are available in the <code>classifiers</code> registry: <code>mask-classifier</code> and <code>multi-mask-classifier</code>.</p>"},{"location":"pipes/box-classifiers/mask/#edspdf.pipes.classifiers.mask.simple_mask_classifier_factory","title":"<code>mask-classifier</code>","text":"<p>The simplest form of mask classification. You define the mask, everything else is tagged as pollution.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'mask-classifier'</code> </p> <code>x0</code> <p>The x0 coordinate of the mask</p> <p> TYPE: <code>float</code> </p> <code>y0</code> <p>The y0 coordinate of the mask</p> <p> TYPE: <code>float</code> </p> <code>x1</code> <p>The x1 coordinate of the mask</p> <p> TYPE: <code>float</code> </p> <code>y1</code> <p>The y1 coordinate of the mask</p> <p> TYPE: <code>float</code> </p> <code>threshold</code> <p>The threshold for the alignment</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <p>Examples:</p> API-basedConfiguration-based <pre><code>pipeline.add_pipe(\n    \"mask-classifier\",\n    name=\"classifier\",\n    config={\n        \"threshold\": 0.9,\n        \"x0\": 0.1,\n        \"y0\": 0.1,\n        \"x1\": 0.9,\n        \"y1\": 0.9,\n    },\n)\n</code></pre> <pre><code>[components.classifier]\n@classifiers = \"mask-classifier\"\nx0 = 0.1\ny0 = 0.1\nx1 = 0.9\ny1 = 0.9\nthreshold = 0.9\n</code></pre>"},{"location":"pipes/box-classifiers/mask/#edspdf.pipes.classifiers.mask.mask_classifier_factory","title":"<code>multi-mask-classifier</code>","text":"<p>A generalisation, wherein the user defines a number of regions.</p> <p>The following configuration produces exactly the same classifier as <code>mask.v1</code> example above.</p> <p>Any bloc that is not part of a mask is tagged as <code>pollution</code>.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p> TYPE: <code>str</code> DEFAULT: <code>'multi-mask-classifier'</code> </p> <code>threshold</code> <p>The threshold for the alignment</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>masks</code> <p>The masks</p> <p> TYPE: <code>Box</code> DEFAULT: <code>{}</code> </p> <p>Examples:</p> API-basedConfiguration-based <pre><code>pipeline.add_pipe(\n    \"multi-mask-classifier\",\n    name=\"classifier\",\n    config={\n        \"threshold\": 0.9,\n        \"mymask\": {\"x0\": 0.1, \"y0\": 0.1, \"x1\": 0.9, \"y1\": 0.3, \"label\": \"body\"},\n    },\n)\n</code></pre> <pre><code>[components.classifier]\n@factory = \"multi-mask-classifier\"\nthreshold = 0.9\n\n[components.classifier.mymask]\nlabel = \"body\"\nx0 = 0.1\ny0 = 0.1\nx1 = 0.9\ny1 = 0.9\n</code></pre> <p>The following configuration defines a <code>header</code> region.</p> API-basedConfiguration-based <pre><code>pipeline.add_pipe(\n    \"multi-mask-classifier\",\n    name=\"classifier\",\n    config={\n        \"threshold\": 0.9,\n        \"body\": {\"x0\": 0.1, \"y0\": 0.1, \"x1\": 0.9, \"y1\": 0.3, \"label\": \"header\"},\n        \"header\": {\"x0\": 0.1, \"y0\": 0.3, \"x1\": 0.9, \"y1\": 0.9, \"label\": \"body\"},\n    },\n)\n</code></pre> <pre><code>[components.classifier]\n@factory = \"multi-mask-classifier\"\nthreshold = 0.9\n\n[components.classifier.header]\nlabel = \"header\"\nx0 = 0.1\ny0 = 0.1\nx1 = 0.9\ny1 = 0.3\n\n[components.classifier.body]\nlabel = \"body\"\nx0 = 0.1\ny0 = 0.3\nx1 = 0.9\ny1 = 0.9\n</code></pre>"},{"location":"pipes/box-classifiers/random/","title":"Random classifier","text":"<p>Random classifier, for chaos purposes. Classifies each box to a random element.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object.</p> <p> TYPE: <code>Pipeline</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'random-classifier'</code> </p> <code>labels</code> <p>The labels to assign to each line. If a list is passed, each label is assigned with equal probability. If a dict is passed, the keys are the labels and the values are the probabilities.</p> <p> TYPE: <code>Union[List[str], Dict[str, float]]</code> </p>"},{"location":"pipes/box-classifiers/trainable/","title":"Trainable classifier","text":"<p>This component predicts a label for each box over the whole document using machine learning.</p> <p>Note</p> <p>You must train the model your model to use this classifier. See Model training for more information</p> <p>Examples:</p> <p>The classifier is composed of the following blocks:</p> <ul> <li>a configurable box embedding layer</li> <li>a linear classification layer</li> </ul> <p>In this example, we use a <code>box-embedding</code> layer to generate the embeddings of the boxes. It is composed of a text encoder that embeds the text features of the boxes and a layout encoder that embeds the layout features of the boxes. These two embeddings are summed and passed through an optional <code>contextualizer</code>, here a <code>box-transformer</code>.</p> API-basedConfiguration-based <pre><code>pipeline.add_pipe(\n    \"trainable-classifier\",\n    name=\"classifier\",\n    config={\n        # simple embedding computed by pooling embeddings of words in each box\n        \"embedding\": {\n            \"@factory\": \"sub-box-cnn-pooler\",\n            \"out_channels\": 64,\n            \"kernel_sizes\": (3, 4, 5),\n            \"embedding\": {\n                \"@factory\": \"simple-text-embedding\",\n                \"size\": 72,\n            },\n        },\n        \"labels\": [\"body\", \"pollution\"],\n    },\n)\n</code></pre> <pre><code>[components.classifier]\n@factory = \"trainable-classifier\"\nlabels = [\"body\", \"pollution\"]\n\n[components.classifier.embedding]\n@factory = \"sub-box-cnn-pooler\"\nout_channels = 64\nkernel_sizes = (3, 4, 5)\n\n[components.classifier.embedding.embedding]\n@factory = \"simple-text-embedding\"\nsize = 72\n</code></pre> PARAMETER  DESCRIPTION <code>labels</code> <p>Initial labels of the classifier (will be completed during initialization)</p> <p> TYPE: <code>Sequence[str]</code> DEFAULT: <code>('pollution')</code> </p> <code>embedding</code> <p>Embedding module to encode the PDF boxes</p> <p> TYPE: <code>TrainablePipe[EmbeddingOutput]</code> </p>"},{"location":"pipes/embeddings/","title":"Embeddings","text":"<p>We offer multiple embedding methods to encode the text and layout information of the PDFs. The following components can be added to a pipeline or composed together, and contain preprocessing and postprocessing logic to convert and batch documents.</p> Factory name Description <code>simple-text-embedding</code> A module that embeds the textual features of the blocks. <code>embedding-combiner</code> Encodes boxes using a combination of multiple encoders <code>sub-box-cnn-pooler</code> Pools the output of a CNN over the elements of a box (like words) <code>box-layout-embedding</code> Encodes the layout of the boxes <code>box-transformer</code> Contextualizes box representations using a transformer <code>huggingface-embedding</code> Box representations using a Huggingface multi-modal model. <p>Layers</p> <p>These components are not to be confused with <code>layers</code>, which are standard PyTorch modules that can be used to build trainable components, such as the ones described here.</p>"},{"location":"pipes/embeddings/box-layout-embedding/","title":"BoxLayoutEmbedding","text":"<p>This component encodes the geometrical features of a box, as extracted by the BoxLayoutPreprocessor module, into an embedding. For position modes, use:</p> <ul> <li><code>\"sin\"</code> to embed positions with a fixed   SinusoidalEmbedding</li> <li><code>\"learned\"</code> to embed positions using a learned standard pytorch embedding layer</li> </ul> <p>Each produces embedding is the concatenation of the box width, height and the top, left, bottom and right coordinates, each embedded depending on the <code>*_mode</code> param.</p> PARAMETER  DESCRIPTION <code>size</code> <p>Size of the output box embedding</p> <p> TYPE: <code>int</code> </p> <code>n_positions</code> <p>Number of position embeddings stored in the PositionEmbedding module</p> <p> TYPE: <code>int</code> </p> <code>x_mode</code> <p>Position embedding mode of the x coordinates</p> <p> TYPE: <code>Literal['sin', 'learned']</code> DEFAULT: <code>'sin'</code> </p> <code>y_mode</code> <p>Position embedding mode of the x coordinates</p> <p> TYPE: <code>Literal['sin', 'learned']</code> DEFAULT: <code>'sin'</code> </p> <code>w_mode</code> <p>Position embedding mode of the width features</p> <p> TYPE: <code>Literal['sin', 'learned']</code> DEFAULT: <code>'sin'</code> </p> <code>h_mode</code> <p>Position embedding mode of the height features</p> <p> TYPE: <code>Literal['sin', 'learned']</code> DEFAULT: <code>'sin'</code> </p>"},{"location":"pipes/embeddings/box-transformer/","title":"BoxTransformer","text":"<p>BoxTransformer using BoxTransformerModule under the hood.</p> <p>Note</p> <p>This module is a TrainablePipe and can be used in a Pipeline, while BoxTransformerModule is a standard PyTorch module, which does not take care of the preprocessing, collating, etc. of the input documents.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>Pipeline instance</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'box-transformer'</code> </p> <code>num_heads</code> <p>Number of attention heads in the attention layers</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>n_relative_positions</code> <p>Maximum range of embeddable relative positions between boxes (further distances are capped to \u00b1n_relative_positions // 2)</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>dropout_p</code> <p>Dropout probability both for the attention layers and embedding projections</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>head_size</code> <p>Head sizes of the attention layers</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>activation</code> <p>Activation function used in the linear-&gt;activation-&gt;linear transformations</p> <p> TYPE: <code>ActivationFunction</code> DEFAULT: <code>'gelu'</code> </p> <code>init_resweight</code> <p>Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attention_mode</code> <p>Mode of relative position infused attention layer. See the relative attention documentation for more information.</p> <p> TYPE: <code>Sequence[Literal['c2c', 'c2p', 'p2c']]</code> DEFAULT: <code>('c2c', 'c2p', 'p2c')</code> </p> <code>n_layers</code> <p>Number of layers in the Transformer</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p>"},{"location":"pipes/embeddings/embedding-combiner/","title":"EmbeddingCombiner","text":"<p>Encodes boxes using a combination of multiple encoders</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'embedding-combiner'</code> </p> <code>mode</code> <p>The mode to use to combine the encoders:</p> <ul> <li><code>sum</code>: Sum the outputs of the encoders</li> <li><code>cat</code>: Concatenate the outputs of the encoders</li> </ul> <p> TYPE: <code>Literal['sum', 'cat']</code> DEFAULT: <code>'sum'</code> </p> <code>dropout_p</code> <p>Dropout probability used on the output of the box and textual encoders</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>encoders</code> <p>The encoders to use. The keys are the names of the encoders and the values are the encoders themselves.</p> <p> TYPE: <code>TrainablePipe[EmbeddingOutput]</code> DEFAULT: <code>{}</code> </p>"},{"location":"pipes/embeddings/huggingface-embedding/","title":"HuggingfaceEmbedding","text":"<p>The HuggingfaceEmbeddings component is a wrapper around the Huggingface multi-modal models. Such pre-trained models should offer better results than a model trained from scratch. Compared to using the raw Huggingface model, we offer a simple mechanism to split long documents into strided windows before feeding them to the model.</p>"},{"location":"pipes/embeddings/huggingface-embedding/#edspdf.pipes.embeddings.huggingface_embedding.HuggingfaceEmbedding--windowing","title":"Windowing","text":"<p>The HuggingfaceEmbedding component splits long documents into smaller windows before feeding them to the model. This is done to avoid hitting the maximum number of tokens that can be processed by the model on a single device. The window size and stride can be configured using the <code>window</code> and <code>stride</code> parameters. The default values are 510 and 255 respectively, which means that the model will process windows of 510 tokens, each separated by 255 tokens. Whenever a token appears in multiple windows, the embedding of the \"most contextualized\" occurrence is used, i.e. the occurrence that is the closest to the center of its window.</p> <p>Here is an overview how this works in a classifier model : </p> <p>Examples:</p> <p>Here is an example of how to define a pipeline with the HuggingfaceEmbedding component:</p> <pre><code>from edspdf import Pipeline\n\nmodel = Pipeline()\nmodel.add_pipe(\n    \"pdfminer-extractor\",\n    name=\"extractor\",\n    config={\n        \"render_pages\": True,\n    },\n)\nmodel.add_pipe(\n    \"huggingface-embedding\",\n    name=\"embedding\",\n    config={\n        \"model\": \"microsoft/layoutlmv3-base\",\n        \"use_image\": False,\n        \"window\": 128,\n        \"stride\": 64,\n        \"line_pooling\": \"mean\",\n    },\n)\nmodel.add_pipe(\n    \"trainable-classifier\",\n    name=\"classifier\",\n    config={\n        \"embedding\": model.get_pipe(\"embedding\"),\n        \"labels\": [],\n    },\n)\n</code></pre> <p>This model can then be trained following the training recipe.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline instance</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name</p> <p> TYPE: <code>str</code> DEFAULT: <code>'huggingface-embedding'</code> </p> <code>model</code> <p>The Huggingface model name or path</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>use_image</code> <p>Whether to use the image or not in the model</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>window</code> <p>The window size to use when splitting long documents into smaller windows before feeding them to the Transformer model (default: 510 = 512 - 2)</p> <p> TYPE: <code>int</code> DEFAULT: <code>510</code> </p> <code>stride</code> <p>The stride (distance between windows) to use when splitting long documents into smaller windows: (default: 510 / 2 = 255)</p> <p> TYPE: <code>int</code> DEFAULT: <code>255</code> </p> <code>line_pooling</code> <p>The pooling strategy to use when combining the embeddings of the tokens in a line into a single line embedding</p> <p> TYPE: <code>Literal['mean', 'max', 'sum']</code> DEFAULT: <code>'mean'</code> </p> <code>max_tokens_per_device</code> <p>The maximum number of tokens that can be processed by the model on a single device. This does not affect the results but can be used to reduce the memory usage of the model, at the cost of a longer processing time.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128 * 128</code> </p>"},{"location":"pipes/embeddings/simple-text-embedding/","title":"SimpleTextEmbedding","text":"<p>A module that embeds the textual features of the blocks</p> PARAMETER  DESCRIPTION <code>size</code> <p>Size of the output box embedding</p> <p> TYPE: <code>int</code> </p> <code>pipeline</code> <p>The pipeline object</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'simple-text-embedding'</code> </p>"},{"location":"pipes/embeddings/sub-box-cnn-pooler/","title":"SubBoxCNNPooler","text":"<p>One dimension CNN encoding multi-kernel layer. Input embeddings are convoluted using linear kernels each parametrized with a (window) size of <code>kernel_size[kernel_i]</code> The output of the kernels are concatenated together, max-pooled and finally projected to a size of <code>output_size</code>.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>Pipeline instance</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sub-box-cnn-pooler'</code> </p> <code>output_size</code> <p>Size of the output embeddings Defaults to the <code>input_size</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>out_channels</code> <p>Number of channels</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>kernel_sizes</code> <p>Window size of each kernel</p> <p> TYPE: <code>Sequence[int]</code> DEFAULT: <code>(3, 4, 5)</code> </p> <code>activation</code> <p>Activation function to use</p> <p> TYPE: <code>ActivationFunction</code> DEFAULT: <code>'relu'</code> </p>"},{"location":"pipes/extractors/","title":"Extraction","text":"<p>The extraction phase consists of reading the PDF document and gather text blocs, along with their dimensions and position within the document. Said blocs will go on to the classification phase to separate the body from the rest.</p>"},{"location":"pipes/extractors/#text-based-pdf","title":"Text-based PDF","text":"<p>We provide a multiple extractor architectures for text-based PDFs :</p> Factory name Description <code>pdfminer-extractor</code> Extracts text lines with the <code>pdfminer</code> library <code>mupdf-extractor</code> Extracts text lines with the <code>pymupdf</code> library <code>poppler-extractor</code> Extracts text lines with the <code>poppler</code> library"},{"location":"pipes/extractors/#image-based-pdf","title":"Image-based PDF","text":"<p>Image-based PDF documents require an OCR1 step, which is not natively supported by EDS-PDF. However, you can easily extend EDS-PDF by adding such a method to the registry.</p> <p>We plan on adding such an OCR extractor component in the future.</p> <ol> <li> <p>Optical Character Recognition, or OCR, is the process of extracting characters and words from an image.\u00a0\u21a9</p> </li> </ol> <ol></ol>"},{"location":"pipes/extractors/pdfminer/","title":"PdfMiner Extractor","text":"<p>We provide a PDF line extractor built on top of PdfMiner.</p> <p>This is the most portable extractor, since it is pure-python and can therefore be run on any platform. Be sure to have a look at their documentation, especially the part providing a bird's eye view of the PDF extraction process.</p> <p>Examples:</p> API-basedConfiguration-based <pre><code>pipeline.add_pipe(\n    \"pdfminer-extractor\",\n    config=dict(\n        extract_style=False,\n    ),\n)\n</code></pre> <pre><code>[components.extractor]\n@factory = \"pdfminer-extractor\"\nextract_style = false\n</code></pre> <p>And use the pipeline on a PDF document:</p> <pre><code>from pathlib import Path\n\n# Apply on a new document\npipeline(Path(\"path/to/your/pdf/document\").read_bytes())\n</code></pre> PARAMETER  DESCRIPTION <code>line_overlap</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>char_margin</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.05</code> </p> <code>line_margin</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>word_margin</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>boxes_flow</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>0.5</code> </p> <code>detect_vertical</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>all_texts</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>extract_style</code> <p>Whether to extract style (font, size, ...) information for each line of the document. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>render_pages</code> <p>Whether to extract the rendered page as a numpy array in the <code>page.image</code> attribute (defaults to False)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>render_dpi</code> <p>DPI to use when rendering the page (defaults to 200)</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>raise_on_error</code> <p>Whether to raise an error if the PDF cannot be parsed. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"recipes/","title":"EDS-PDF Recipes","text":"<p>This section goes over a few use-cases for PDF extraction. It is meant as a more hands-on tutorial to get a grip on the library.</p>"},{"location":"recipes/annotation/","title":"PDF Annotation","text":"<p>In this section, we will cover one methodology to annotate PDF documents.</p> <p>Data annotation at AP-HP's CDW</p> <p>At AP-HP's CDW1, we recently moved away from a rule- and Java-based PDF extraction pipeline (using PDFBox) to one using EDS-PDF. Hence, EDS-PDF is used in production, helping extract text from around 100k PDF documents every day.</p> <p>To train our pipeline presently in production, we annotated around 270 documents, and reached a f1-score of 0.98 on the body classification.</p>"},{"location":"recipes/annotation/#preparing-the-data-for-annotation","title":"Preparing the data for annotation","text":"<p>We will frame the annotation phase as an image segmentation task, where annotators are asked to draw bounding boxes around the different sections. Hence, the very first step is to convert PDF documents to images. We suggest using the library <code>pdf2image</code> for that step.</p> <p>The following script will convert the PDF documents located in a <code>data/pdfs</code> directory to PNG images inside the <code>data/images</code> folder.</p> <pre><code>import pdf2image\nfrom pathlib import Path\n\nDATA_DIR = Path(\"data\")\nPDF_DIR = DATA_DIR / \"pdfs\"\nIMAGE_DIR = DATA_DIR / \"images\"\n\nfor pdf in PDF_DIR.glob(\"*.pdf\"):\n    imgs = pdf2image.convert_from_bytes(pdf)\n\n    for page, img in enumerate(imgs):\n        path = IMAGE_DIR / f\"{pdf.stem}_{page}.png\"\n        img.save(path)\n</code></pre> <p>You can use any annotation tool to annotate the images. If you're looking for a simple way to annotate from within a Jupyter Notebook, ipyannotations might be a good fit.</p> <p>You will need to post-process the output to convert the annotations to the following format:</p> Key Description <code>page</code> Page within the PDF (0-indexed) <code>x0</code> Horizontal position of the top-left corner of the bounding box <code>x1</code> Horizontal position of the bottom-right corner of the bounding box <code>y0</code> Vertical position of the top-left corner of the bounding box <code>y1</code> Vertical position of the bottom-right corner of the bounding box <code>label</code> Class of the bounding box (eg <code>body</code>, <code>header</code>...) <p>All dimensions should be normalised by the height and width of the page.</p>"},{"location":"recipes/annotation/#saving-the-dataset","title":"Saving the dataset","text":"<p>Once the annotation phase is complete, make sure the train/test split is performed once and for all when you create the dataset.</p> <p>We suggest the following structure:</p> Directory structure<pre><code>dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 &lt;note_id_1&gt;.pdf\n\u2502   \u251c\u2500\u2500 &lt;note_id_1&gt;.json\n\u2502   \u251c\u2500\u2500 &lt;note_id_2&gt;.pdf\n\u2502   \u251c\u2500\u2500 &lt;note_id_2&gt;.json\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 test/\n    \u251c\u2500\u2500 &lt;note_id_n&gt;.pdf\n    \u251c\u2500\u2500 &lt;note_id_n&gt;.json\n    \u2514\u2500\u2500 ...\n</code></pre> <p>Where the normalised annotation resides in a JSON file living next to the related PDF, and uses the following schema:</p> Key Description <code>note_id</code> Reference to the document <code>&lt;properties&gt;</code> Optional property of the document itself <code>annotations</code> List of annotations, following the schema above <p>This structure presents the advantage of being machine- and human-friendly. The JSON file contains annotated regions as well as any document property that could be useful to adapt the pipeline (typically for the classification step).</p>"},{"location":"recipes/annotation/#extracting-annotations","title":"Extracting annotations","text":"<p>The following snippet extracts the annotations into a workable format:</p> <pre><code>from pathlib import Path\nimport pandas as pd\n\n\ndef get_annotations(\n    directory: Path,\n) -&gt; pd.DataFrame:\n\"\"\"\n    Read annotations from the dataset directory.\n\n    Parameters\n    ----------\n    directory : Path\n        Dataset directory\n\n    Returns\n    -------\n    pd.DataFrame\n        Pandas DataFrame containing the annotations.\n    \"\"\"\n    dfs = []\n\n    iterator = tqdm(list(directory.glob(\"*.json\")))\n\n    for path in iterator:\n        meta = json.loads(path.read_text())\n        df = pd.DataFrame.from_records(meta.pop(\"annotations\"))\n\n        for k, v in meta.items():  # (1)\n            df[k] = v\n\n        dfs.append(df)\n\n    return pd.concat(dfs)\n\n\ntrain_path = Path(\"dataset/train\")\n\nannotations = get_annotations(train_path)\n</code></pre> <ol> <li>Add a column for each additional property saved in the dataset.</li> </ol> <p>The annotations compiled this way can be used to train a pipeline. See the trained pipeline recipe for more detail.</p> <ol> <li> <p>Greater Paris University Hospital's Clinical Data Warehouse\u00a0\u21a9</p> </li> </ol> <ol></ol>"},{"location":"recipes/extension/","title":"Extending EDS-PDF","text":"<p>EDS-PDF is organised around a function registry powered by catalogue and a custom configuration system. The result is a powerful framework that is easy to extend - and we'll see how in this section.</p> <p>For this recipe, let's imagine we're not entirely satisfied with the aggregation proposed by EDS-PDF. For instance, we might want an aggregator that outputs the text in Markdown format.</p> <p>Note</p> <p>Properly converting to markdown is no easy task. For this example, we will limit ourselves to detecting bold and italics sections.</p>"},{"location":"recipes/extension/#developing-the-new-aggregator","title":"Developing the new aggregator","text":"<p>Our aggregator will inherit from the <code>SimpleAggregator</code>, and use the style to detect italics and bold sections.</p> markdown_aggregator.py<pre><code>from edspdf import registry\nfrom edspdf.pipes.aggregators.simple import SimpleAggregator\nfrom edspdf.structures import PDFDoc, Text\n\n\n@registry.factory.register(\"markdown-aggregator\")  # (1)\nclass MarkdownAggregator(SimpleAggregator):\n    def __call__(self, doc: PDFDoc) -&gt; PDFDoc:\n        doc = super().__call__(doc)\n\n        for label in doc.aggregated_texts.keys():\n            text = doc.aggregated_texts[label].text\n\n            fragments = []\n\n            offset = 0\n            for s in doc.aggregated_texts[label].properties:\n                if s.begin &gt;= s.end:\n                    continue\n                if offset &lt; s.begin:\n                    fragments.append(text[offset : s.begin])\n\n                offset = s.end\n                snippet = text[s.begin : s.end]\n                if s.bold:\n                    snippet = f\"**{snippet}**\"\n                if s.italic:\n                    snippet = f\"_{snippet}_\"\n                fragments.append(snippet)\n\n            if offset &lt; len(text):\n                fragments.append(text[offset:])\n\n            doc.aggregated_texts[label] = Text(text=\"\".join(fragments))\n\n        return doc\n</code></pre> <ol> <li>The new aggregator is registered via this line</li> <li>The new aggregator redefines the <code>__call__</code> method.    It will output a single string, corresponding to the markdown-formatted output.</li> </ol> <p>That's it! You can use this new aggregator with the API:</p> <pre><code>from edspdf import Pipeline\nfrom markdown_aggregator import MarkdownAggregator  # (1)\n\nmodel = Pipeline()\n# will extract text lines from a document\nmodel.add_pipe(\n    \"pdfminer-extractor\",\n    config=dict(\n        extract_style=False,\n    ),\n)\n# classify everything inside the `body` bounding box as `body`\nmodel.add_pipe(\"mask-classifier\", config={\"x0\": 0.1, \"y0\": 0.1, \"x1\": 0.9, \"y1\": 0.9})\n# aggregates the lines together to generate the markdown formatted text\nmodel.add_pipe(\"markdown-aggregator\")\n</code></pre> <ol> <li>We're importing the aggregator that we just defined.</li> </ol> <p>It all works relatively smoothly!</p>"},{"location":"recipes/extension/#making-the-aggregator-discoverable","title":"Making the aggregator discoverable","text":"<p>Now, how can we instantiate the pipeline using the configuration system? The registry needs to be aware of the new function, but we shouldn't have to import <code>mardown_aggregator.py</code> just so that the module is registered as a side-effect...</p> <p>Catalogue solves this problem by using Python entry points.</p> pyproject.tomlsetup.py <pre><code>[project.entry-points.\"edspdf_factories\"]\n\"markdown-aggregator\" = \"markdown_aggregator:MarkdownAggregator\"\n</code></pre> <pre><code>from setuptools import setup\n\nsetup(\n    name=\"edspdf-markdown-aggregator\",\n    entry_points={\n        \"edspdf_factories\": [\n            \"markdown-aggregator = markdown_aggregator:MarkdownAggregator\"\n        ]\n    },\n)\n</code></pre> <p>By declaring the new aggregator as an entrypoint, it will become discoverable by EDS-PDF as long as it is installed in your environment!</p>"},{"location":"recipes/rule-based/","title":"Rule-based extraction","text":"<p>Let's create a rule-based extractor for PDF documents.</p> <p>Note</p> <p>This pipeline will likely perform poorly as soon as your PDF documents come in varied forms. In that case, even a very simple trained pipeline may give you a substantial performance boost (see next section).</p> <p>First, download this example PDF.</p> <p>We will use the following configuration:</p> config.cfg<pre><code>[pipeline]\ncomponents = [\"extractor\", \"classifier\", \"aggregator\"]\ncomponents_config = ${components}\n\n[components.extractor]\n@factory = \"pdfminer-extractor\"  # (2)\nextract_style = true\n\n[components.classifier]\n@factory = \"mask-classifier\"  # (3)\nx0 = 0.2\nx1 = 0.9\ny0 = 0.3\ny1 = 0.6\nthreshold = 0.1\n\n[components.aggregator]\n@factory = \"styled-aggregator\"  # (4)\n</code></pre> <ol> <li>This is the top-level object, which organises the entire extraction process.</li> <li>Here we use the provided text-based extractor, based on the PDFMiner library</li> <li>This is where we define the rule-based classifier. Here, we use a \"mask\",    meaning that every text bloc that falls within the boundaries will be assigned    the <code>body</code> label, everything else will be tagged as pollution.</li> <li>This aggregator returns a tuple of dictionaries. The first contains compiled text for each    label, the second exports their style.</li> </ol> <p>Save the configuration as <code>config.cfg</code> and run the following snippet:</p> <pre><code>import edspdf\nimport pandas as pd\nfrom pathlib import Path\n\nmodel = edspdf.load(\"config.cfg\")  # (1)\n\n# Get a PDF\npdf = Path(\"/Users/perceval/Development/edspdf/tests/resources/letter.pdf\").read_bytes()\npdf = model(pdf)\n\nbody = pdf.aggregated_texts[\"body\"]\n\ntext, style = body.text, body.properties\nprint(text)\nprint(pd.DataFrame(style))\n</code></pre> <p>This code will output the following results:</p> VisualisationExtracted TextExtracted Style <p></p> <pre><code>Cher Pr ABC, Cher DEF,\n\nNous souhaitons remercier le CSE pour son avis favorable quant \u00e0 l\u2019acc\u00e8s aux donn\u00e9es de\nl\u2019Entrep\u00f4t de Donn\u00e9es de Sant\u00e9 du projet n\u00b0 XXXX.\n\nNous avons bien pris connaissance des conditions requises pour cet avis favorable, c\u2019est\npourquoi nous nous engageons par la pr\u00e9sente \u00e0 :\n\n\u2022 Informer individuellement les patients concern\u00e9s par la recherche, admis \u00e0 l'AP-HP\navant juillet 2017, sortis vivants, et non r\u00e9admis depuis.\n\n\u2022 Effectuer une demande d'autorisation \u00e0 la CNIL en cas d'appariement avec d\u2019autres\ncohortes.\n\nBien cordialement,\n</code></pre> <p>The <code>start</code> and <code>end</code> columns refer to the character indices within the extracted text.</p> italic bold fontname start end False False BCDFEE+Calibri 0 22 False False BCDFEE+Calibri 24 90 False False BCDHEE+Calibri 90 91 False False BCDFEE+Calibri 91 111 False False BCDFEE+Calibri 112 113 False False BCDHEE+Calibri 113 114 False False BCDFEE+Calibri 114 161 False False BCDFEE+Calibri 163 247 False False BCDHEE+Calibri 247 248 False False BCDFEE+Calibri 248 251 False False BCDFEE+Calibri 252 300 False False SymbolMT 302 303 False False BCDFEE+Calibri 304 386 False False BCDFEE+Calibri 387 445 False False SymbolMT 447 448 False False BCDFEE+Calibri 449 523 False False BCDHEE+Calibri 523 524 False False BCDFEE+Calibri 524 530 False False BCDFEE+Calibri 531 540 False False BCDFEE+Calibri 542 560 <ol></ol>"},{"location":"recipes/training/","title":"Training a Pipeline","text":"<p>In this chapter, we'll see how we can train a deep-learning based classifier to better classify the lines of the document and extract texts from the document.</p>"},{"location":"recipes/training/#step-by-step-walkthrough","title":"Step-by-step walkthrough","text":"<p>Training supervised models consists in feeding batches of samples taken from a training corpus to a model instantiated from a given architecture and optimizing the learnable weights of the model to decrease a given loss. The process of training a pipeline with EDS-PDF is as follows:</p> <ol> <li> <p>We first start by seeding the random states and instantiating a new trainable pipeline. Here we show two examples of pipeline, the first one based on a custom embedding architecture and the second one based on a pre-trained HuggingFace transformer model.</p> Custom architecturePre-trained HuggingFace transformer <p>The architecture of the trainable classifier of this recipe is described in the following figure: </p> <pre><code>from edspdf import Pipeline\nfrom edspdf.utils.random import set_seed\n\nset_seed(42)\n\nmodel = Pipeline()\nmodel.add_pipe(\"pdfminer-extractor\", name=\"extractor\") # (1)\nmodel.add_pipe(\n    \"box-transformer\",\n    name=\"embedding\",\n    config={\n        \"num_heads\": 4,\n        \"dropout_p\": 0.1,\n        \"activation\": \"gelu\",\n        \"init_resweight\": 0.01,\n        \"head_size\": 16,\n        \"attention_mode\": [\"c2c\", \"c2p\", \"p2c\"],\n        \"n_layers\": 1,\n        \"n_relative_positions\": 64,\n        \"embedding\": {\n            \"@factory\": \"embedding-combiner\",\n            \"dropout_p\": 0.1,\n            \"text_encoder\": {\n                \"@factory\": \"sub-box-cnn-pooler\",\n                \"out_channels\": 64,\n                \"kernel_sizes\": (3, 4, 5),\n                \"embedding\": {\n                    \"@factory\": \"simple-text-embedding\",\n                    \"size\": 72,\n                },\n            },\n            \"layout_encoder\": {\n                \"@factory\": \"box-layout-embedding\",\n                \"n_positions\": 64,\n                \"x_mode\": \"learned\",\n                \"y_mode\": \"learned\",\n                \"w_mode\": \"learned\",\n                \"h_mode\": \"learned\",\n                \"size\": 72,\n            },\n        },\n    },\n)\nmodel.add_pipe(\n    \"trainable-classifier\",\n    name=\"classifier\",\n    config={\n        \"embedding\": model.get_pipe(\"embedding\"),\n        \"labels\": [],\n    },\n)\n</code></pre> <ol> <li>You can choose between multiple extractors, such as \"pdfminer-extractor\", \"mupdf-extractor\" or \"poppler-extractor\" (the latter does not support rendering images). See the extractors list here extractors for more details.</li> </ol> <pre><code>model = Pipeline()\nmodel.add_pipe(\n    \"mupdf-extractor\",\n    name=\"extractor\",\n    config={\n        \"render_pages\": True,\n    },\n) # (1)\nmodel.add_pipe(\n    \"huggingface-embedding\",\n    name=\"embedding\",\n    config={\n        \"model\": \"microsoft/layoutlmv3-base\",\n        \"use_image\": False,\n        \"window\": 128,\n        \"stride\": 64,\n        \"line_pooling\": \"mean\",\n    },\n)\nmodel.add_pipe(\n    \"trainable-classifier\",\n    name=\"classifier\",\n    config={\n        \"embedding\": model.get_pipe(\"embedding\"),\n        \"labels\": [],\n    },\n)\n</code></pre> <ol> <li>You can choose between multiple extractors, such as \"pdfminer-extractor\", \"mupdf-extractor\" or \"poppler-extractor\" (the latter does not support rendering images). See the extractors list here extractors for more details.</li> </ol> </li> <li> <p>We then load and adapt (i.e., convert into PDFDoc) the training and validation dataset, which is often a combination of JSON and PDF files. The recommended way of doing this is to make a Python generator of PDFDoc objects.     <pre><code>train_docs = list(segmentation_adapter(train_path)(model))\nval_docs = list(segmentation_adapter(val_path)(model))\n</code></pre></p> </li> <li> <p>We initialize the missing or incomplete components attributes (such as vocabularies) with the training dataset     <pre><code>model.post_init(train_docs)\n</code></pre></p> </li> <li> <p>The training dataset is then preprocessed into features. The resulting preprocessed dataset is then wrapped into a pytorch DataLoader to be fed to the model during the training loop with the model's own collate method.     <pre><code>preprocessed = list(model.preprocess_many(train_docs, supervision=True))\ndataloader = DataLoader(\n    preprocessed,\n    batch_size=batch_size,\n    collate_fn=model.collate,\n    shuffle=True,\n)\n</code></pre></p> </li> <li> <p>We instantiate an optimizer and start the training loop     <pre><code>from itertools import chain, repeat\n\noptimizer = torch.optim.AdamW(\n    params=model.parameters(),\n    lr=lr,\n)\n\n# We will loop over the dataloader\niterator = chain.from_iterable(repeat(dataloader))\n\nfor step in tqdm(range(max_steps), \"Training model\", leave=True):\n    batch = next(iterator)\n    optimizer.zero_grad()\n</code></pre></p> </li> <li> <p>The trainable components are fed the collated batches from the dataloader with the <code>TrainablePipe.module_forward</code> methods to compute the losses. Since outputs of shared subcomponents are reused between components, we enable caching by wrapping this step in a cache context. The training loop is otherwise carried in a similar fashion to a standard pytorch training loop     <pre><code>with model.cache():\n    loss = torch.zeros((), device=\"cpu\")\n    for name, component in model.trainable_pipes():\n        output = component.module_forward(batch[component.name])\n        if \"loss\" in output:\n            loss += output[\"loss\"]\n\n    loss.backward()\n\n    optimizer.step()\n</code></pre></p> </li> <li> <p>Finally, the model is evaluated on the validation dataset at regular intervals and saved at the end of the training. To score the model, we only want to run \"classifier\" component and not the extractor, otherwise we would overwrite annotated text boxes on documents in the <code>val_docs</code> dataset, and have mismatching text boxes between the gold and predicted documents. To save the model, although you can use <code>torch.save</code> to save your model, we provide a safer method to avoid the security pitfalls of pickle models        <pre><code>from edspdf import Pipeline\nfrom sklearn.metrics import classification_report\nfrom copy import deepcopy\n\n\ndef score(golds, preds):\n    return classification_report(\n        [b.label for gold in golds for b in gold.text_boxes if b.text != \"\"],\n        [b.label for pred in preds for b in pred.text_boxes if b.text != \"\"],\n        output_dict=True,\n        zero_division=0,\n    )\n\n\n...\n\nif (step % 100) == 0:\n    # we only want to run \"classifier\" component, not overwrite the text boxes\n    with model.select_pipes(enable=[\"classifier\"]):\n        print(score(val_docs, model.pipe(deepcopy(val_docs))))\n\n# torch.save(model, \"model.pt\")\nmodel.save(\"model\")\n</code></pre></p> </li> </ol>"},{"location":"recipes/training/#adapting-a-dataset","title":"Adapting a dataset","text":"<p>The first step of training a pipeline is to adapt the dataset to the pipeline. This is done by converting the dataset into a list of PDFDoc objects, using an extractor. The following function loads a dataset of <code>.pdf</code> and <code>.json</code> files, where each <code>.json</code> file contain box annotations represented with <code>page</code>, <code>x0</code>, <code>x1</code>, <code>y0</code>, <code>y1</code> and <code>label</code>.</p> <pre><code>from edspdf.utils.alignment import align_box_labels\nfrom pathlib import Path\nfrom pydantic import DirectoryPath\nfrom edspdf.registry import registry\nfrom edspdf.structures import Box\nimport json\n\n\n@registry.adapter.register(\"my-segmentation-adapter\")\ndef segmentation_adapter(\n    path: DirectoryPath,\n):\n    def adapt_to(model):\n        for anns_filepath in sorted(Path(path).glob(\"*.json\")):\n            pdf_filepath = str(anns_filepath).replace(\".json\", \".pdf\")\n            with open(anns_filepath) as f:\n                sample = json.load(f)\n            pdf = Path(pdf_filepath).read_bytes()\n\n            if len(sample[\"annotations\"]) == 0:\n                continue\n\n            doc = model.components.extractor(pdf)\n            doc.id = pdf_filepath.split(\".\")[0].split(\"/\")[-1]\n            doc.lines = [\n                line\n                for page in sorted(set(b.page for b in doc.lines))\n                for line in align_box_labels(\n                    src_boxes=[\n                        Box(\n                            page_num=b[\"page\"],\n                            x0=b[\"x0\"],\n                            x1=b[\"x1\"],\n                            y0=b[\"y0\"],\n                            y1=b[\"y1\"],\n                            label=b[\"label\"],\n                        )\n                        for b in sample[\"annotations\"]\n                        if b[\"page\"] == page\n                    ],\n                    dst_boxes=doc.lines,\n                    pollution_label=None,\n                )\n                if line.text == \"\" or line.label is not None\n            ]\n            yield doc\n\n    return adapt_to\n</code></pre>"},{"location":"recipes/training/#full-example","title":"Full example","text":"<p>Let's wrap the training code in a function, and make it callable from the command line using confit !</p> train.py <pre><code>import itertools\nimport json\nfrom copy import deepcopy\nfrom pathlib import Path\n\nimport torch\nfrom confit import Cli\nfrom pydantic import DirectoryPath\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom edspdf import Pipeline, registry\nfrom edspdf.structures import Box\nfrom edspdf.utils.alignment import align_box_labels\nfrom edspdf.utils.random import set_seed\n\napp = Cli(pretty_exceptions_show_locals=False)\n\n\ndef score(golds, preds):\n    return classification_report(\n        [b.label for gold in golds for b in gold.text_boxes if b.text != \"\"],\n        [b.label for pred in preds for b in pred.text_boxes if b.text != \"\"],\n        output_dict=True,\n        zero_division=0,\n    )\n\n\n@registry.adapter.register(\"my-segmentation-adapter\")\ndef segmentation_adapter(\n    path: str,\n):\n    def adapt_to(model):\n        for anns_filepath in sorted(Path(path).glob(\"*.json\")):\n            pdf_filepath = str(anns_filepath).replace(\".json\", \".pdf\")\n            with open(anns_filepath) as f:\n                sample = json.load(f)\n            pdf = Path(pdf_filepath).read_bytes()\n\n            if len(sample[\"annotations\"]) == 0:\n                continue\n\n            doc = model.get_pipe(\"extractor\")(pdf)\n            doc.id = pdf_filepath.split(\".\")[0].split(\"/\")[-1]\n            doc.content_boxes = [\n                line\n                for page_num in sorted(set(b.page_num for b in doc.lines))\n                for line in align_box_labels(\n                    src_boxes=[\n                        Box(\n                            page_num=b[\"page\"],\n                            x0=b[\"x0\"],\n                            x1=b[\"x1\"],\n                            y0=b[\"y0\"],\n                            y1=b[\"y1\"],\n                            label=b[\"label\"],\n                        )\n                        for b in sample[\"annotations\"]\n                        if b[\"page\"] == page_num\n                    ],\n                    dst_boxes=doc.lines,\n                    pollution_label=None,\n                )\n                if line.text == \"\" or line.label is not None\n            ]\n            yield doc\n\n    return adapt_to\n\n\n@app.command(name=\"train\")\ndef train_my_model(\n    train_path: DirectoryPath = \"dataset/train\",\n    val_path: DirectoryPath = \"dataset/dev\",\n    max_steps: int = 1000,\n    batch_size: int = 4,\n    lr: float = 3e-4,\n):\n    set_seed(42)\n\n    # We define the model\n    model = Pipeline()\n    model.add_pipe(\"mupdf-extractor\", name=\"extractor\")\n    model.add_pipe(\n        \"box-transformer\",\n        name=\"embedding\",\n        config={\n            \"num_heads\": 4,\n            \"dropout_p\": 0.1,\n            \"activation\": \"gelu\",\n            \"init_resweight\": 0.01,\n            \"head_size\": 16,\n            \"attention_mode\": [\"c2c\", \"c2p\", \"p2c\"],\n            \"n_layers\": 1,\n            \"n_relative_positions\": 64,\n            \"embedding\": {\n                \"@factory\": \"embedding-combiner\",\n                \"dropout_p\": 0.1,\n                \"text_encoder\": {\n                    \"@factory\": \"sub-box-cnn-pooler\",\n                    \"out_channels\": 64,\n                    \"kernel_sizes\": (3, 4, 5),\n                    \"embedding\": {\n                        \"@factory\": \"simple-text-embedding\",\n                        \"size\": 72,\n                    },\n                },\n                \"layout_encoder\": {\n                    \"@factory\": \"box-layout-embedding\",\n                    \"n_positions\": 64,\n                    \"x_mode\": \"learned\",\n                    \"y_mode\": \"learned\",\n                    \"w_mode\": \"learned\",\n                    \"h_mode\": \"learned\",\n                    \"size\": 72,\n                },\n            },\n        },\n    )\n    model.add_pipe(\n        \"trainable-classifier\",\n        name=\"classifier\",\n        config={\n            \"embedding\": model.get_pipe(\"embedding\"),\n            \"labels\": [],\n        },\n    )\n\n    # Loading and adapting the training and validation data\n    train_docs = list(segmentation_adapter(train_path)(model))\n    val_docs = list(segmentation_adapter(val_path)(model))\n\n    # Taking the first `initialization_subset` samples to initialize the model\n    model.post_init(train_docs)\n\n    # Preprocessing the training dataset into a dataloader\n    preprocessed = list(model.preprocess_many(train_docs, supervision=True))\n    dataloader = DataLoader(\n        preprocessed,\n        batch_size=batch_size,\n        collate_fn=model.collate,\n        shuffle=True,\n    )\n\n    optimizer = torch.optim.AdamW(\n        params=model.parameters(),\n        lr=lr,\n    )\n\n    # We will loop over the dataloader\n    iterator = itertools.chain.from_iterable(itertools.repeat(dataloader))\n\n    for step in tqdm(range(max_steps), \"Training model\", leave=True):\n        batch = next(iterator)\n        optimizer.zero_grad()\n\n        with model.cache():\n            loss = torch.zeros((), device=\"cpu\")\n            for name, component in model.trainable_pipes():\n                output = component.module_forward(batch[component.name])\n                if \"loss\" in output:\n                    loss += output[\"loss\"]\n\n            loss.backward()\n\n            optimizer.step()\n\n        if (step % 100) == 0:\n            with model.select_pipes(enable=[\"classifier\"]):\n                print(score(val_docs, model.pipe(deepcopy(val_docs))))\n            model.save(\"model\")\n\n    return model\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <pre><code>python train.py --seed 42\n</code></pre> <p>At the end of the training, the pipeline is ready to use (with the <code>.pipe</code> method) since every trained component of the pipeline is self-sufficient, ie contains the preprocessing, inference and postprocessing code required to run it.</p>"},{"location":"recipes/training/#configuration","title":"Configuration","text":"<p>To decouple the configuration and the code of our training script, let's define a configuration file where we will describe both our training parameters and the pipeline. You can either write the config of the pipeline by hand, or generate it from an instantiated pipeline by running:</p> <pre><code>print(pipeline.config.to_str())\n</code></pre> Custom architecturePretrained Huggingface Transformer config.cfg<pre><code># This is this equivalent of the API-based declaration at the beginning of the tutorial\n[pipeline]\npipeline = [\"extractor\", \"embedding\", \"classifier\"]\ndisabled = []\ncomponents = ${components}\n\n[components]\n\n[components.extractor]\n@factory = \"pdfminer-extractor\"\n\n[components.embedding]\n@factory = \"box-transformer\"\nnum_heads = 4\ndropout_p = 0.1\nactivation = \"gelu\"\ninit_resweight = 0.01\nhead_size = 16\nattention_mode = [\"c2c\", \"c2p\", \"p2c\"]\nn_layers = 1\nn_relative_positions = 64\n\n[components.embedding.embedding]\n@factory = \"embedding-combiner\"\ndropout_p = 0.1\n\n[components.embedding.embedding.text_encoder]\n@factory = \"sub-box-cnn-pooler\"\nout_channels = 64\nkernel_sizes = (3, 4, 5)\n\n[components.embedding.embedding.text_encoder.embedding]\n@factory = \"simple-text-embedding\"\nsize = 72\n\n[components.embedding.embedding.layout_encoder]\n@factory = \"box-layout-embedding\"\nn_positions = 64\nx_mode = \"learned\"\ny_mode = \"learned\"\nw_mode = \"learned\"\nh_mode = \"learned\"\nsize = 72\n\n[components.classifier]\n@factory = \"trainable-classifier\"\nembedding = ${components.embedding}\nlabels = []\n\n# This is were we define the training script parameters\n# the \"train\" section refers to the name of the command in the training script\n[train]\nmodel = ${pipeline}\ntrain_data = {\"@adapter\": \"my-segmentation-adapter\", \"path\": \"data/train\"}\nval_data = {\"@adapter\": \"my-segmentation-adapter\", \"path\": \"data/val\"}\nmax_steps = 1000\nseed = 42\nlr = 3e-4\nbatch_size = 4\n</code></pre> config.cfg<pre><code>[pipeline]\npipeline = [\"extractor\", \"embedding\", \"classifier\"]\ndisabled = []\ncomponents = ${components}\n\n[components]\n\n[components.extractor]\n@factory = \"mupdf-extractor\"\nrender_pages = true\n\n[components.embedding]\n@factory = \"huggingface-embedding\"\nmodel = \"microsoft/layoutlmv3-base\"\nuse_image = false\nwindow = 128\nstride = 64\nline_pooling = \"mean\"\n\n[components.classifier]\n@factory = \"trainable-classifier\"\nembedding = ${components.embedding}\nlabels = []\n\n[train]\nmodel = ${pipeline}\nmax_steps = 1000\nlr = 5e-5\nseed = 42\ntrain_data = {\"@adapter\": \"my-segmentation-adapter\", \"path\": \"data/train\"}\nval_data = {\"@adapter\": \"my-segmentation-adapter\", \"path\": \"data/val\"}\nbatch_size = 8\n</code></pre> <p>and update our training script to use the pipeline and the data adapters defined in the configuration file instead of the Python declaration :</p> <pre><code>@app.command(name=\"train\")\ndef train_my_model(\n+   model: Pipeline,\n+   train_path: DirectoryPath = \"data/train\",\n-   train_data: Callable = segmentation_adapter(\"data/train\"),\n+   val_path: DirectoryPath = \"data/val\",\n-   val_data: Callable = segmentation_adapter(\"data/val\"),\n   seed: int = 42,\n    max_steps: int = 1000,\n    batch_size: int = 4,\n    lr: float = 3e-4,\n):\n    # Seed will be set by the CLI util, before `model` is instanciated\n-   set_seed(seed)\n\n   # Model will be defined from the config file using registries\n-   model = Pipeline()\n-   model.add_pipe(\"mupdf-extractor\", name=\"extractor\")\n-   model.add_pipe(\n-       \"box-transformer\",\n-       name=\"embedding\",\n-       config={\n-           \"num_heads\": 4,\n-           \"dropout_p\": 0.1,\n-           \"activation\": \"gelu\",\n-           \"init_resweight\": 0.01,\n-           \"head_size\": 16,\n-           \"attention_mode\": [\"c2c\", \"c2p\", \"p2c\"],\n-           \"n_layers\": 1,\n-           \"n_relative_positions\": 64,\n-           \"embedding\": {\n-               \"@factory\": \"embedding-combiner\",\n-               \"dropout_p\": 0.1,\n-               \"text_encoder\": {\n-                   \"@factory\": \"sub-box-cnn-pooler\",\n-                   \"out_channels\": 64,\n-                   \"kernel_sizes\": (3, 4, 5),\n-                   \"embedding\": {\n-                       \"@factory\": \"simple-text-embedding\",\n-                       \"size\": 72,\n-                   },\n-               },\n-               \"layout_encoder\": {\n-                   \"@factory\": \"box-layout-embedding\",\n-                   \"n_positions\": 64,\n-                   \"x_mode\": \"learned\",\n-                   \"y_mode\": \"learned\",\n-                   \"w_mode\": \"learned\",\n-                   \"h_mode\": \"learned\",\n-                   \"size\": 72,\n-               },\n-           },\n-       },\n-   )\n-   model.add_pipe(\n-       \"trainable-classifier\",\n-       name=\"classifier\",\n-       config={\n-           \"embedding\": model.get_pipe(\"embedding\"),\n-           \"labels\": [],\n-       },\n-   )\n\n   # Loading and adapting the training and validation data\n-    train_docs = list(segmentation_adapter(train_path)(model))\n+    train_docs = list(train_data(model))\n-    val_docs = list(segmentation_adapter(val_path)(model))\n+    val_docs = list(val_data(model))\n\n   # Taking the first `initialization_subset` samples to initialize the model\n    ...\n</code></pre> <p>That's it ! We can now call the training script with the configuration file as a parameter, and override some of its defaults values:</p> <pre><code>python train.py --config config.cfg --components.extractor.extract_styles=true --seed 43\n</code></pre>"},{"location":"reference/edspdf/","title":"<code>edspdf</code>","text":""},{"location":"reference/edspdf/pipeline/","title":"<code>edspdf.pipeline</code>","text":""},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"<p>Pipeline to build hybrid and multitask PDF processing pipeline. It uses PyTorch as the deep-learning backend and allows components to share subcomponents.</p> <p>See the documentation for more details.</p> PARAMETER  DESCRIPTION <code>batch_size</code> <p>Batch size to use in the <code>.pipe()</code> method</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>4</code> </p> <code>meta</code> <p>Meta information about the pipeline</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.disabled","title":"<code>disabled</code>  <code>property</code>","text":"<p>The names of the disabled components</p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.cfg","title":"<code>cfg: Config</code>  <code>property</code>","text":"<p>Returns the config of the pipeline, including the config of all components. Updated from spacy to allow references between components.</p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.get_pipe","title":"<code>get_pipe</code>","text":"<p>Get a component by its name.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the component to get.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Pipe</code>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.has_pipe","title":"<code>has_pipe</code>","text":"<p>Check if a component exists in the pipeline.</p> PARAMETER  DESCRIPTION <code>name</code> <p>The name of the component to check.</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.create_pipe","title":"<code>create_pipe</code>","text":"<p>Create a component from a factory name.</p> PARAMETER  DESCRIPTION <code>factory</code> <p>The name of the factory to use</p> <p> TYPE: <code>str</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> </p> <code>config</code> <p>The config to pass to the factory</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Pipe</code>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.add_pipe","title":"<code>add_pipe</code>","text":"<p>Add a component to the pipeline.</p> PARAMETER  DESCRIPTION <code>factory</code> <p>The name of the component to add or the component itself</p> <p> TYPE: <code>Union[str, Pipe]</code> </p> <code>name</code> <p>The name of the component. If not provided, the name of the component will be used if it has one (.name), otherwise the factory name will be used.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>first</code> <p>Whether to add the component to the beginning of the pipeline. This argument is mutually exclusive with <code>before</code> and <code>after</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>before</code> <p>The name of the component to add the new component before. This argument is mutually exclusive with <code>after</code> and <code>first</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>after</code> <p>The name of the component to add the new component after. This argument is mutually exclusive with <code>before</code> and <code>first</code>.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>config</code> <p>The arguments to pass to the component factory.</p> <p>Note that instead of replacing arguments with the same keys, the config will be merged with the default config of the component. This means that you can override specific nested arguments without having to specify the entire config.</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Pipe</code> <p>The component that was added to the pipeline.</p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.__call__","title":"<code>__call__</code>","text":"<p>Apply each component successively on a document.</p> PARAMETER  DESCRIPTION <code>doc</code> <p>The doc to create the PDFDoc from, or a PDFDoc.</p> <p> TYPE: <code>Any</code> </p> RETURNS DESCRIPTION <code>PDFDoc</code>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.pipe","title":"<code>pipe</code>","text":"<p>Process a stream of documents by applying each component successively on batches of documents.</p> PARAMETER  DESCRIPTION <code>inputs</code> <p>The inputs to create the PDFDocs from, or the PDFDocs directly.</p> <p> TYPE: <code>Any</code> </p> <code>batch_size</code> <p>The batch size to use. If not provided, the batch size of the pipeline object will be used.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>accelerator</code> <p>The accelerator to use for processing the documents. If not provided, the default accelerator will be used.</p> <p> TYPE: <code>Optional[Union[str, Accelerator]]</code> DEFAULT: <code>None</code> </p> <code>to_doc</code> <p>The function to use to convert the inputs to PDFDoc objects. By default, the <code>content</code> field of the inputs will be used if dict-like objects are provided, otherwise the inputs will be passed directly to the pipeline.</p> <p> TYPE: <code>Optional[ToDoc]</code> DEFAULT: <code>None</code> </p> <code>from_doc</code> <p>The function to use to convert the PDFDoc objects to outputs. By default, the PDFDoc objects will be returned directly.</p> <p> TYPE: <code>FromDoc</code> DEFAULT: <code>lambda : doc</code> </p> RETURNS DESCRIPTION <code>Iterable[PDFDoc]</code>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.cache","title":"<code>cache</code>","text":"<p>Enable caching for all (trainable) components in the pipeline</p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.trainable_pipes","title":"<code>trainable_pipes</code>","text":"<p>Yields components that are PyTorch modules.</p> PARAMETER  DESCRIPTION <code>disable</code> <p>The names of disabled components, which will be skipped.</p> <p> TYPE: <code>Sequence[str]</code> DEFAULT: <code>()</code> </p> RETURNS DESCRIPTION <code>Iterable[Tuple[str, TrainablePipe]]</code>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.post_init","title":"<code>post_init</code>","text":"<p>Completes the initialization of the pipeline by calling the post_init method of all components that have one. This is useful for components that need to see some data to build their vocabulary, for instance.</p> PARAMETER  DESCRIPTION <code>gold_data</code> <p>The documents to use for initialization. Each component will not necessarily see all the data.</p> <p> TYPE: <code>Iterable[PDFDoc]</code> </p> <code>exclude</code> <p>The names of components to exclude from initialization. This argument will be gradually updated  with the names of initialized components</p> <p> TYPE: <code>Optional[set]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.from_config","title":"<code>from_config</code>  <code>classmethod</code>","text":"<p>Create a pipeline from a config object</p> PARAMETER  DESCRIPTION <code>config</code> <p>The config to use</p> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>{}</code> </p> <code>disable</code> <p>Components to disable</p> <p> TYPE: <code>Optional[Set[str]]</code> DEFAULT: <code>None</code> </p> <code>enable</code> <p>Components to enable</p> <p> TYPE: <code>Optional[Set[str]]</code> DEFAULT: <code>None</code> </p> <code>exclude</code> <p>Components to exclude</p> <p> TYPE: <code>Optional[Set[str]]</code> DEFAULT: <code>None</code> </p> <code>meta</code> <p>Metadata to add to the pipeline</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Pipeline</code>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.__get_validators__","title":"<code>__get_validators__</code>  <code>classmethod</code>","text":"<p>Pydantic validators generator</p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.validate","title":"<code>validate</code>  <code>classmethod</code>","text":"<p>Pydantic validator, used in the <code>validate_arguments</code> decorated functions</p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.preprocess","title":"<code>preprocess</code>","text":"<p>Run the preprocessing methods of each component in the pipeline on a document and returns a dictionary containing the results, with the component names as keys.</p> PARAMETER  DESCRIPTION <code>doc</code> <p>The document to preprocess</p> <p> TYPE: <code>PDFDoc</code> </p> <code>supervision</code> <p>Whether to include supervision information in the preprocessing</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.preprocess_many","title":"<code>preprocess_many</code>","text":"<p>Runs the preprocessing methods of each component in the pipeline on a collection of documents and returns an iterable of dictionaries containing the results, with the component names as keys.</p> PARAMETER  DESCRIPTION <code>docs</code> <p> TYPE: <code>Iterable[PDFDoc]</code> </p> <code>compress</code> <p>Whether to deduplicate identical preprocessing outputs of the results if multiple documents share identical subcomponents. This step is required to enable the cache mechanism when training or running the pipeline over a tabular datasets such as pyarrow tables that do not store referential equality information.</p> <p> DEFAULT: <code>True</code> </p> <code>supervision</code> <p>Whether to include supervision information in the preprocessing</p> <p> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Iterable[OutputT]</code>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.collate","title":"<code>collate</code>","text":"<p>Collates a batch of preprocessed samples into a single (maybe nested) dictionary of tensors by calling the collate method of each component.</p> PARAMETER  DESCRIPTION <code>batch</code> <p>The batch of preprocessed samples</p> <p> TYPE: <code>List[Dict[str, Any]]</code> </p> <code>device</code> <p>The device to move the tensors to before returning them</p> <p> TYPE: <code>Optional[device]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>The collated batch</p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.parameters","title":"<code>parameters</code>","text":"<p>Returns an iterator over the Pytorch parameters of the components in the pipeline</p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.named_parameters","title":"<code>named_parameters</code>","text":"<p>Returns an iterator over the Pytorch parameters of the components in the pipeline</p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.to","title":"<code>to</code>","text":"<p>Moves the pipeline to a given device</p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.train","title":"<code>train</code>","text":"<p>Enables training mode on pytorch modules</p> PARAMETER  DESCRIPTION <code>mode</code> <p>Whether to enable training or not</p> <p> DEFAULT: <code>True</code> </p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.save","title":"<code>save</code>","text":"<p>Save the pipeline to a directory.</p> PARAMETER  DESCRIPTION <code>path</code> <p>The path to the directory to save the pipeline to. Every component will be saved to separated subdirectories of this directory, except for tensors that will be saved to a shared files depending on the references between the components.</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>exclude</code> <p>The names of the components, or attributes to exclude from the saving process. This list will be gradually filled in place as components are saved</p> <p> TYPE: <code>Optional[Set[str]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.load_state_from_disk","title":"<code>load_state_from_disk</code>","text":"<p>Load the pipeline from a directory. Components will be updated in-place.</p> PARAMETER  DESCRIPTION <code>path</code> <p>The path to the directory to load the pipeline from</p> <p> TYPE: <code>Union[str, Path]</code> </p> <code>exclude</code> <p>The names of the components, or attributes to exclude from the loading process. This list will be gradually filled in place as components are loaded</p> <p> TYPE: <code>Set[str]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edspdf/pipeline/#edspdf.pipeline.Pipeline.select_pipes","title":"<code>select_pipes</code>","text":"<p>Temporarily disable and enable components in the pipeline.</p> PARAMETER  DESCRIPTION <code>disable</code> <p>The name of the component to disable, or a list of names.</p> <p> TYPE: <code>Optional[Union[str, Iterable[str]]]</code> DEFAULT: <code>None</code> </p> <code>enable</code> <p>The name of the component to enable, or a list of names.</p> <p> TYPE: <code>Optional[Union[str, Iterable[str]]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edspdf/registry/","title":"<code>edspdf.registry</code>","text":""},{"location":"reference/edspdf/registry/#edspdf.registry.CurriedFactory","title":"<code>CurriedFactory</code>","text":""},{"location":"reference/edspdf/registry/#edspdf.registry.CurriedFactory.instantiate","title":"<code>instantiate</code>","text":"<p>We need to support passing in the pipeline object and name to factories from a config file. Since components can be nested, we need to add them to every factory in the config.</p>"},{"location":"reference/edspdf/registry/#edspdf.registry.FactoryRegistry","title":"<code>FactoryRegistry</code>","text":"<p>             Bases: <code>Registry</code></p> <p>A registry that validates the input arguments of the registered functions.</p>"},{"location":"reference/edspdf/registry/#edspdf.registry.FactoryRegistry.get","title":"<code>get</code>","text":"<p>Get the registered function for a given name.</p> <p>name (str): The name. RETURNS (Any): The registered function.</p>"},{"location":"reference/edspdf/registry/#edspdf.registry.FactoryRegistry.register","title":"<code>register</code>","text":"<p>This is a convenience wrapper around <code>confit.Registry.register</code>, that curries the function to be registered, allowing to instantiate the class later once <code>pipeline</code> and <code>name</code> are known.</p> PARAMETER  DESCRIPTION <code>name</code> <p> TYPE: <code>str</code> </p> <code>func</code> <p> TYPE: <code>Optional[InFunc]</code> DEFAULT: <code>None</code> </p> <code>default_config</code> <p> TYPE: <code>Dict[str, Any]</code> DEFAULT: <code>FrozenDict()</code> </p> <code>assigns</code> <p> TYPE: <code>Iterable[str]</code> DEFAULT: <code>FrozenList()</code> </p> <code>requires</code> <p> TYPE: <code>Iterable[str]</code> DEFAULT: <code>FrozenList()</code> </p> <code>retokenizes</code> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>default_score_weights</code> <p> TYPE: <code>Dict[str, Optional[float]]</code> DEFAULT: <code>FrozenDict()</code> </p> RETURNS DESCRIPTION <code>Callable[[InFunc], InFunc]</code>"},{"location":"reference/edspdf/registry/#edspdf.registry.accepted_arguments","title":"<code>accepted_arguments</code>","text":"<p>Checks that a function accepts a list of keyword arguments</p> PARAMETER  DESCRIPTION <code>func</code> <p>Function to check</p> <p> TYPE: <code>Callable</code> </p> <code>args</code> <p>Argument or list of arguments to check</p> <p> TYPE: <code>Sequence[str]</code> </p> RETURNS DESCRIPTION <code>List[str]</code>"},{"location":"reference/edspdf/structures/","title":"<code>edspdf.structures</code>","text":""},{"location":"reference/edspdf/structures/#edspdf.structures.PDFDoc","title":"<code>PDFDoc</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>This is the main data structure of the library to hold PDFs. It contains the content of the PDF, as well as box annotations and text outputs.</p> ATTRIBUTE DESCRIPTION <code>content</code> <p>The content of the PDF document.</p> <p> TYPE: <code>bytes</code> </p> <code>id</code> <p>The ID of the PDF document.</p> <p> TYPE: <code>(str, optional)</code> </p> <code>pages</code> <p>The pages of the PDF document.</p> <p> TYPE: <code>List[Page]</code> </p> <code>error</code> <p>Whether there was an error when processing this PDF document.</p> <p> TYPE: <code>(bool, optional)</code> </p> <code>content_boxes</code> <p>The content boxes/annotations of the PDF document.</p> <p> TYPE: <code>List[Union[TextBox, ImageBox]]</code> </p> <code>aggregated_texts</code> <p>The aggregated text outputs of the PDF document.</p> <p> TYPE: <code>Dict[str, Text]</code> </p> <code>text_boxes</code> <p>The text boxes of the PDF document.</p> <p> TYPE: <code>List[TextBox]</code> </p>"},{"location":"reference/edspdf/structures/#edspdf.structures.Page","title":"<code>Page</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The <code>Page</code> class represents a page of a PDF document.</p> ATTRIBUTE DESCRIPTION <code>page_num</code> <p>The page number of the page.</p> <p> TYPE: <code>int</code> </p> <code>width</code> <p>The width of the page.</p> <p> TYPE: <code>float</code> </p> <code>height</code> <p>The height of the page.</p> <p> TYPE: <code>float</code> </p> <code>doc</code> <p>The PDF document that this page belongs to.</p> <p> TYPE: <code>PDFDoc</code> </p> <code>image</code> <p>The rendered image of the page, stored as a NumPy array.</p> <p> TYPE: <code>Optional[ndarray]</code> </p> <code>text_boxes</code> <p>The text boxes of the page.</p> <p> TYPE: <code>List[TextBox]</code> </p>"},{"location":"reference/edspdf/structures/#edspdf.structures.TextProperties","title":"<code>TextProperties</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The <code>TextProperties</code> class represents the style properties of a span of text in a TextBox.</p> ATTRIBUTE DESCRIPTION <code>italic</code> <p>Whether the text is italic.</p> <p> TYPE: <code>bool</code> </p> <code>bold</code> <p>Whether the text is bold.</p> <p> TYPE: <code>bool</code> </p> <code>begin</code> <p>The beginning index of the span of text.</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>The ending index of the span of text.</p> <p> TYPE: <code>int</code> </p> <code>fontname</code> <p>The font name of the span of text.</p> <p> TYPE: <code>Optional[str]</code> </p>"},{"location":"reference/edspdf/structures/#edspdf.structures.Box","title":"<code>Box</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The <code>Box</code> class represents a box annotation in a PDF document. It is the base class of TextBox.</p> ATTRIBUTE DESCRIPTION <code>doc</code> <p>The PDF document that this box belongs to.</p> <p> TYPE: <code>PDFDoc</code> </p> <code>page_num</code> <p>The page number of the box.</p> <p> TYPE: <code>Optional[int]</code> </p> <code>x0</code> <p>The left x-coordinate of the box.</p> <p> TYPE: <code>float</code> </p> <code>x1</code> <p>The right x-coordinate of the box.</p> <p> TYPE: <code>float</code> </p> <code>y0</code> <p>The top y-coordinate of the box.</p> <p> TYPE: <code>float</code> </p> <code>y1</code> <p>The bottom y-coordinate of the box.</p> <p> TYPE: <code>float</code> </p> <code>label</code> <p>The label of the box.</p> <p> TYPE: <code>Optional[str]</code> </p> <code>page</code> <p>The page object that this box belongs to.</p> <p> TYPE: <code>Page</code> </p>"},{"location":"reference/edspdf/structures/#edspdf.structures.Text","title":"<code>Text</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The <code>TextBox</code> class represents text object, not bound to any box.</p> <p>It can be used to store aggregated text from multiple boxes for example.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The text content.</p> <p> TYPE: <code>str</code> </p> <code>properties</code> <p>The style properties of the text.</p> <p> TYPE: <code>List[TextProperties]</code> </p>"},{"location":"reference/edspdf/structures/#edspdf.structures.TextBox","title":"<code>TextBox</code>","text":"<p>             Bases: <code>Box</code></p> <p>The <code>TextBox</code> class represents a text box annotation in a PDF document.</p> ATTRIBUTE DESCRIPTION <code>text</code> <p>The text content of the text box.</p> <p> TYPE: <code>str</code> </p> <code>props</code> <p>The style properties of the text box.</p> <p> TYPE: <code>List[TextProperties]</code> </p>"},{"location":"reference/edspdf/trainable_pipe/","title":"<code>edspdf.trainable_pipe</code>","text":""},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe","title":"<code>TrainablePipe</code>","text":"<p>             Bases: <code>Module</code>, <code>Generic[OutputBatch]</code></p> <p>A TrainablePipe is a Component that can be trained and inherits <code>torch.nn.Module</code>. You can use it either as a torch module inside a more complex neural network, or as a standalone component in a Pipeline.</p> <p>In addition to the methods of a torch module, a TrainablePipe adds a few methods to handle preprocessing and collating features, as well as caching intermediate results for components that share a common subcomponent.</p>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.save_extra_data","title":"<code>save_extra_data</code>","text":"<p>Dumps vocabularies indices to json files</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the directory where the files will be saved</p> <p> TYPE: <code>Path</code> </p> <code>exclude</code> <p>The set of component names to exclude from saving This is useful when components are repeated in the pipeline.</p> <p> TYPE: <code>set</code> </p>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.load_extra_data","title":"<code>load_extra_data</code>","text":"<p>Loads vocabularies indices from json files</p> PARAMETER  DESCRIPTION <code>path</code> <p>Path to the directory where the files will be loaded</p> <p> TYPE: <code>Path</code> </p> <code>exclude</code> <p>The set of component names to exclude from loading This is useful when components are repeated in the pipeline.</p> <p> TYPE: <code>set</code> </p>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.post_init","title":"<code>post_init</code>","text":"<p>This method completes the attributes of the component, by looking at some documents. It is especially useful to build vocabularies or detect the labels of a classification task.</p> PARAMETER  DESCRIPTION <code>gold_data</code> <p>The documents to use for initialization.</p> <p> TYPE: <code>Iterable[PDFDoc]</code> </p> <code>exclude</code> <p>The names of components to exclude from initialization. This argument will be gradually updated  with the names of initialized components</p> <p> TYPE: <code>set</code> </p>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.preprocess","title":"<code>preprocess</code>","text":"<p>Preprocess the document to extract features that will be used by the neural network to perform its predictions.</p> PARAMETER  DESCRIPTION <code>doc</code> <p>PDFDocument to preprocess</p> <p> TYPE: <code>PDFDoc</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dictionary (optionally nested) containing the features extracted from the document.</p>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.collate","title":"<code>collate</code>","text":"<p>Collate the batch of features into a single batch of tensors that can be used by the forward method of the component.</p> PARAMETER  DESCRIPTION <code>batch</code> <p>Batch of features</p> <p> TYPE: <code>NestedSequences</code> </p> <code>device</code> <p>Device on which the tensors should be moved</p> <p> TYPE: <code>device</code> </p> RETURNS DESCRIPTION <code>InputBatch</code> <p>Dictionary (optionally nested) containing the collated tensors</p>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.forward","title":"<code>forward</code>","text":"<p>Perform the forward pass of the neural network, i.e, apply transformations over the collated features to compute new embeddings, probabilities, losses, etc</p> PARAMETER  DESCRIPTION <code>batch</code> <p>Batch of tensors (nested dictionary) computed by the collate method</p> <p> TYPE: <code>InputBatch</code> </p> RETURNS DESCRIPTION <code>OutputBatch</code>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.module_forward","title":"<code>module_forward</code>","text":"<p>This is a wrapper around <code>torch.nn.Module.__call__</code> to avoid conflict with the <code>TrainablePipe.__call__</code> method.</p>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.make_batch","title":"<code>make_batch</code>","text":"<p>Convenience method to preprocess a batch of documents and collate them Features corresponding to the same path are grouped together in a list, under the same key.</p> PARAMETER  DESCRIPTION <code>docs</code> <p>Batch of documents</p> <p> TYPE: <code>Sequence[PDFDoc]</code> </p> <code>supervision</code> <p>Whether to extract supervision features or not</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>Dict[str, Sequence[Any]]</code>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.batch_process","title":"<code>batch_process</code>","text":"<p>Process a batch of documents using the neural network. This differs from the <code>pipe</code> method in that it does not return an iterator, but executes the component on the whole batch at once.</p> PARAMETER  DESCRIPTION <code>docs</code> <p>Batch of documents</p> <p> TYPE: <code>Sequence[PDFDoc]</code> </p> RETURNS DESCRIPTION <code>Sequence[PDFDoc]</code> <p>Batch of updated documents</p>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.postprocess","title":"<code>postprocess</code>","text":"<p>Update the documents with the predictions of the neural network, for instance converting label probabilities into label attributes on the document lines.</p> <p>By default, this is a no-op.</p> PARAMETER  DESCRIPTION <code>docs</code> <p>Batch of documents</p> <p> TYPE: <code>Sequence[PDFDoc]</code> </p> <code>batch</code> <p>Batch of predictions, as returned by the forward method</p> <p> TYPE: <code>OutputBatch</code> </p> RETURNS DESCRIPTION <code>Sequence[PDFDoc]</code>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.preprocess_supervised","title":"<code>preprocess_supervised</code>","text":"<p>Preprocess the document to extract features that will be used by the neural network to perform its training. By default, this returns the same features as the <code>preprocess</code> method.</p> PARAMETER  DESCRIPTION <code>doc</code> <p>PDFDocument to preprocess</p> <p> TYPE: <code>PDFDoc</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>Dictionary (optionally nested) containing the features extracted from the document.</p>"},{"location":"reference/edspdf/trainable_pipe/#edspdf.trainable_pipe.TrainablePipe.__call__","title":"<code>__call__</code>","text":"<p>Applies the component on a single doc. For multiple documents, prefer batch processing via the batch_process method. In general, prefer the Pipeline methods</p> PARAMETER  DESCRIPTION <code>doc</code> <p> TYPE: <code>PDFDoc</code> </p> RETURNS DESCRIPTION <code>PDFDoc</code>"},{"location":"reference/edspdf/accelerators/","title":"<code>edspdf.accelerators</code>","text":""},{"location":"reference/edspdf/accelerators/base/","title":"<code>edspdf.accelerators.base</code>","text":""},{"location":"reference/edspdf/accelerators/base/#edspdf.accelerators.base.FromDoc","title":"<code>FromDoc</code>","text":"<p>A FromDoc converter (from a PDFDoc to an arbitrary type) can be either:</p> <ul> <li>a dict mapping field names to doc attributes</li> <li>a callable that takes a PDFDoc and returns an arbitrary type</li> </ul>"},{"location":"reference/edspdf/accelerators/multiprocessing/","title":"<code>edspdf.accelerators.multiprocessing</code>","text":""},{"location":"reference/edspdf/accelerators/multiprocessing/#edspdf.accelerators.multiprocessing.MultiprocessingAccelerator","title":"<code>MultiprocessingAccelerator</code>","text":"<p>             Bases: <code>Accelerator</code></p> <p>If you have multiple CPU cores, and optionally multiple GPUs, we provide a <code>multiprocessing</code> accelerator that allows to run the inference on multiple processes.</p> <p>This accelerator dispatches the batches between multiple workers (data-parallelism), and distribute the computation of a given batch on one or two workers (model-parallelism). This is done by creating two types of workers:</p> <ul> <li>a <code>CPUWorker</code> which handles the non deep-learning components and the   preprocessing, collating and postprocessing of deep-learning components</li> <li>a <code>GPUWorker</code> which handles the forward call of the deep-learning components</li> </ul> <p>The advantage of dedicating a worker to the deep-learning components is that it allows to prepare multiple batches in parallel in multiple <code>CPUWorker</code>, and ensure that the <code>GPUWorker</code> never wait for a batch to be ready.</p> <p>The overall architecture described in the following figure, for 3 CPU workers and 2 GPU workers.</p> <p>Here is how a small pipeline with rule-based components and deep-learning components is distributed between the workers:</p> <p>Examples:</p> <pre><code>docs = list(\n    pipeline.pipe(\n        [content1, content2, ...],\n        accelerator={\n            \"@accelerator\": \"multiprocessing\",\n            \"num_cpu_workers\": 3,\n            \"num_gpu_workers\": 2,\n            \"batch_size\": 8,\n        },\n    )\n)\n</code></pre> PARAMETER  DESCRIPTION <code>batch_size</code> <p>Number of documents to process at a time in a CPU/GPU worker</p> <p> TYPE: <code>int</code> </p> <code>num_cpu_workers</code> <p>Number of CPU workers. A CPU worker handles the non deep-learning components and the preprocessing, collating and postprocessing of deep-learning components.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>num_gpu_workers</code> <p>Number of GPU workers. A GPU worker handles the forward call of the deep-learning components.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>gpu_pipe_names</code> <p>List of pipe names to accelerate on a GPUWorker, defaults to all pipes that inherit from TrainablePipe</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edspdf/accelerators/multiprocessing/#edspdf.accelerators.multiprocessing.MultiprocessingAccelerator.__call__","title":"<code>__call__</code>","text":"<p>Stream of documents to process. Each document can be a string or a tuple</p> PARAMETER  DESCRIPTION <code>inputs</code> <p> TYPE: <code>Iterable[Any]</code> </p> <code>model</code> <p> TYPE: <code>Any</code> </p> YIELDS DESCRIPTION <code>Any</code> <p>Processed outputs of the pipeline</p>"},{"location":"reference/edspdf/accelerators/simple/","title":"<code>edspdf.accelerators.simple</code>","text":""},{"location":"reference/edspdf/accelerators/simple/#edspdf.accelerators.simple.SimpleAccelerator","title":"<code>SimpleAccelerator</code>","text":"<p>             Bases: <code>Accelerator</code></p> <p>This is the simplest accelerator which batches the documents and process each batch on the main process (the one calling <code>.pipe()</code>).</p> <p>Examples:</p> <pre><code>docs = list(pipeline.pipe([content1, content2, ...]))\n</code></pre> <p>or, if you want to override the model defined batch size</p> <pre><code>docs = list(pipeline.pipe([content1, content2, ...], batch_size=8))\n</code></pre> <p>which is equivalent to passing a confit dict</p> <pre><code>docs = list(\n    pipeline.pipe(\n        [content1, content2, ...],\n        accelerator={\n            \"@accelerator\": \"simple\",\n            \"batch_size\": 8,\n        },\n    )\n)\n</code></pre> <p>or the instantiated accelerator directly</p> <pre><code>from edspdf.accelerators.simple import SimpleAccelerator\n\naccelerator = SimpleAccelerator(batch_size=8)\ndocs = list(pipeline.pipe([content1, content2, ...], accelerator=accelerator))\n</code></pre> <p>If you have a GPU, make sure to move the model to the appropriate device before calling <code>.pipe()</code>. If you have multiple GPUs, use the multiprocessing accelerator instead.</p> <pre><code>pipeline.to(\"cuda\")\ndocs = list(pipeline.pipe([content1, content2, ...]))\n</code></pre> PARAMETER  DESCRIPTION <code>batch_size</code> <p>The number of documents to process in each batch.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p>"},{"location":"reference/edspdf/layers/","title":"<code>edspdf.layers</code>","text":""},{"location":"reference/edspdf/layers/box_transformer/","title":"<code>edspdf.layers.box_transformer</code>","text":""},{"location":"reference/edspdf/layers/box_transformer/#edspdf.layers.box_transformer.BoxTransformerLayer","title":"<code>BoxTransformerLayer</code>","text":"<p>             Bases: <code>Module</code></p> <p>BoxTransformerLayer combining a self attention layer and a linear-&gt;activation-&gt;linear transformation. This layer is used in the BoxTransformerModule module.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>Input embedding size</p> <p> TYPE: <code>int</code> </p> <code>num_heads</code> <p>Number of attention heads in the attention layer</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>dropout_p</code> <p>Dropout probability both for the attention layer and embedding projections</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>head_size</code> <p>Head sizes of the attention layer</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>activation</code> <p>Activation function used in the linear-&gt;activation-&gt;linear transformation</p> <p> TYPE: <code>ActivationFunction</code> DEFAULT: <code>'gelu'</code> </p> <code>init_resweight</code> <p>Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attention_mode</code> <p>Mode of relative position infused attention layer. See the relative attention documentation for more information.</p> <p> TYPE: <code>Sequence[Literal['c2c', 'c2p', 'p2c']]</code> DEFAULT: <code>('c2c', 'c2p', 'p2c')</code> </p> <code>position_embedding</code> <p>Position embedding to use as key/query position embedding in the attention computation.</p> <p> TYPE: <code>Optional[Union[FloatTensor, Parameter]]</code> DEFAULT: <code>None</code> </p>"},{"location":"reference/edspdf/layers/box_transformer/#edspdf.layers.box_transformer.BoxTransformerLayer.forward","title":"<code>forward</code>","text":"<p>Forward pass of the BoxTransformerLayer</p> PARAMETER  DESCRIPTION <code>embeds</code> <p>Embeddings to contextualize Shape: <code>n_samples * n_keys * input_size</code></p> <p> TYPE: <code>FloatTensor</code> </p> <code>mask</code> <p>Mask of the embeddings. 0 means padding element. Shape: <code>n_samples * n_keys</code></p> <p> TYPE: <code>BoolTensor</code> </p> <code>relative_positions</code> <p>Position of the keys relatively to the query elements Shape: <code>n_samples * n_queries * n_keys * n_coordinates (2 for x/y)</code></p> <p> TYPE: <code>LongTensor</code> </p> <code>no_position_mask</code> <p>Key / query pairs for which the position attention terms should be disabled. Shape: <code>n_samples * n_queries * n_keys</code></p> <p> TYPE: <code>Optional[BoolTensor]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Tuple[FloatTensor, FloatTensor]</code> <ul> <li>Contextualized embeddings   Shape: <code>n_samples * n_queries * n_keys</code></li> <li>Attention logits   Shape: <code>n_samples * n_queries * n_keys * n_heads</code></li> </ul>"},{"location":"reference/edspdf/layers/box_transformer/#edspdf.layers.box_transformer.BoxTransformerModule","title":"<code>BoxTransformerModule</code>","text":"<p>             Bases: <code>Module</code></p> <p>Box Transformer architecture combining a multiple BoxTransformerLayer modules. It is mainly used in BoxTransformer.</p> PARAMETER  DESCRIPTION <code>input_size</code> <p>Input embedding size</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>num_heads</code> <p>Number of attention heads in the attention layers</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>n_relative_positions</code> <p>Maximum range of embeddable relative positions between boxes (further distances are capped to \u00b1n_relative_positions // 2)</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>dropout_p</code> <p>Dropout probability both for the attention layers and embedding projections</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>head_size</code> <p>Head sizes of the attention layers</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>activation</code> <p>Activation function used in the linear-&gt;activation-&gt;linear transformations</p> <p> TYPE: <code>ActivationFunction</code> DEFAULT: <code>'gelu'</code> </p> <code>init_resweight</code> <p>Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attention_mode</code> <p>Mode of relative position infused attention layer. See the relative attention documentation for more information.</p> <p> TYPE: <code>Sequence[Literal['c2c', 'c2p', 'p2c']]</code> DEFAULT: <code>('c2c', 'c2p', 'p2c')</code> </p> <code>n_layers</code> <p>Number of layers in the Transformer</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p>"},{"location":"reference/edspdf/layers/box_transformer/#edspdf.layers.box_transformer.BoxTransformerModule.forward","title":"<code>forward</code>","text":"<p>Forward pass of the BoxTransformer</p> PARAMETER  DESCRIPTION <code>embeds</code> <p>Embeddings to contextualize Shape: <code>n_samples * n_keys * input_size</code></p> <p> TYPE: <code>FoldedTensor</code> </p> <code>boxes</code> <p>Layout features of the input elements</p> <p> TYPE: <code>Dict</code> </p> RETURNS DESCRIPTION <code>Tuple[FloatTensor, List[FloatTensor]]</code> <ul> <li>Output of the last BoxTransformerLayer   Shape: <code>n_samples * n_queries * n_keys</code></li> <li>Attention logits of all layers   Shape: <code>n_samples * n_queries * n_keys * n_heads</code></li> </ul>"},{"location":"reference/edspdf/layers/relative_attention/","title":"<code>edspdf.layers.relative_attention</code>","text":""},{"location":"reference/edspdf/layers/relative_attention/#edspdf.layers.relative_attention.RelativeAttention","title":"<code>RelativeAttention</code>","text":"<p>             Bases: <code>Module</code></p> <p>A self/cross-attention layer that takes relative position of elements into account to compute the attention weights. When running a relative attention layer, key and queries are represented using content and position embeddings, where position embeddings are retrieved using the relative position of keys relative to queries</p> PARAMETER  DESCRIPTION <code>size</code> <p>The size of the output embeddings Also serves as default if query_size, pos_size, or key_size is None</p> <p> TYPE: <code>int</code> </p> <code>n_heads</code> <p>The number of attention heads</p> <p> TYPE: <code>int</code> </p> <code>query_size</code> <p>The size of the query embeddings.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>key_size</code> <p>The size of the key embeddings.</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>value_size</code> <p>The size of the value embeddings</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>head_size</code> <p>The size of each query / key / value chunk used in the attention dot product Default: <code>key_size / n_heads</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>position_embedding</code> <p>The position embedding used as key and query embeddings</p> <p> TYPE: <code>Optional[Union[FloatTensor, Parameter]]</code> DEFAULT: <code>None</code> </p> <code>dropout_p</code> <p>Dropout probability applied on the attention weights Default: 0.1</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>same_key_query_proj</code> <p>Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>same_positional_key_query_proj</code> <p>Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>n_coordinates</code> <p>The number of positional coordinates For instance, text is 1D so 1 coordinate, images are 2D so 2 coordinates ... Default: 1</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>head_bias</code> <p>Whether to learn a bias term to add to the attention logits This is only useful if you plan to use the attention logits for subsequent operations, since attention weights are unaffected by bias terms.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>do_pooling</code> <p>Whether to compute the output embedding. If you only plan to use attention logits, you should disable this parameter. Default: True</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>mode</code> <p>Whether to compute content to content (c2c), content to position (c2p) or position to content (p2c) attention terms. Setting <code>mode=('c2c\")</code> disable relative position attention terms: this is the standard attention layer. To get a better intuition about these different types of attention, here is a formulation as fictitious search samples from a word in a (1D) text:</p> <ul> <li>content-content : \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other   words whose content contains information about temporality\"</li> <li>content-position: \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other   words that are 3 positions after of me\"</li> <li>position-content : \"regardless of my content, I will attend to the word   one position after from me if it contains information about temporality,   two words after me if it contains information about location, etc.\"</li> </ul> <p> TYPE: <code>Sequence[Literal['c2c', 'c2p', 'p2c']]</code> DEFAULT: <code>('c2c', 'p2c', 'c2p')</code> </p> <code>n_additional_heads</code> <p>The number of additional head logits to compute. Those are not used to compute output embeddings, but may be useful in subsequent operation. Default: 0</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p>"},{"location":"reference/edspdf/layers/relative_attention/#edspdf.layers.relative_attention.RelativeAttention.forward","title":"<code>forward</code>","text":"<p>Forward pass of the RelativeAttention layer.</p> PARAMETER  DESCRIPTION <code>content_queries</code> <p>The content query embedding to use in the attention computation Shape: <code>n_samples * n_queries * query_size</code></p> <p> TYPE: <code>FloatTensor</code> </p> <code>content_keys</code> <p>The content key embedding to use in the attention computation. If None, defaults to the <code>content_queries</code> Shape: <code>n_samples * n_keys * query_size</code></p> <p> TYPE: <code>Optional[FloatTensor]</code> DEFAULT: <code>None</code> </p> <code>content_values</code> <p>The content values embedding to use in the final pooling computation. If None, pooling won't be performed. Shape: <code>n_samples * n_keys * query_size</code></p> <p> TYPE: <code>Optional[FloatTensor]</code> DEFAULT: <code>None</code> </p> <code>mask</code> <p>The content key embedding to use in the attention computation. If None, defaults to the <code>content_queries</code> Shape: either - <code>n_samples * n_keys</code> - <code>n_samples * n_queries * n_keys</code> - <code>n_samples * n_queries * n_keys * n_heads</code></p> <p> TYPE: <code>Optional[BoolTensor]</code> DEFAULT: <code>None</code> </p> <code>relative_positions</code> <p>The relative position of keys relative to queries If None, positional attention terms won't be computed. Shape: <code>n_samples * n_queries * n_keys * n_coordinates</code></p> <p> TYPE: <code>Optional[LongTensor]</code> DEFAULT: <code>None</code> </p> <code>no_position_mask</code> <p>Key / query pairs for which the position attention terms should be disabled. Shape: <code>n_samples * n_queries * n_keys</code></p> <p> TYPE: <code>Optional[BoolTensor]</code> DEFAULT: <code>None</code> </p> <code>base_attn</code> <p>Attention logits to add to the computed attention logits Shape: <code>n_samples * n_queries * n_keys * n_heads</code></p> <p> TYPE: <code>Optional[FloatTensor]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Union[Tuple[FloatTensor, FloatTensor], FloatTensor]</code> <ul> <li>the output contextualized embeddings (only if content_values is not None   and the <code>do_pooling</code> attribute is set to True)   Shape: n_sample * n_keys * <code>size</code></li> <li>the attention logits   Shape: n_sample * n_keys * n_queries * (n_heads + n_additional_heads)</li> </ul>"},{"location":"reference/edspdf/layers/sinusoidal_embedding/","title":"<code>edspdf.layers.sinusoidal_embedding</code>","text":""},{"location":"reference/edspdf/layers/sinusoidal_embedding/#edspdf.layers.sinusoidal_embedding.SinusoidalEmbedding","title":"<code>SinusoidalEmbedding</code>","text":"<p>             Bases: <code>Module</code></p> <p>A position embedding lookup table that stores embeddings for a fixed number of positions. The value of each of the <code>embedding_dim</code> channels of the generated embedding is generated according to a trigonometric function (sin for even channels, cos for odd channels). The frequency of the signal in each pair of channels varies according to the temperature parameter.</p> <p>Any input position above the maximum value <code>num_embeddings</code> will be capped to <code>num_embeddings - 1</code></p> PARAMETER  DESCRIPTION <code>num_embeddings</code> <p>The maximum number of position embeddings store in this table</p> <p> TYPE: <code>int</code> </p> <code>embedding_dim</code> <p>The embedding size</p> <p> TYPE: <code>int</code> </p> <code>temperature</code> <p>The temperature controls the range of frequencies used by each channel of the embedding</p> <p> TYPE: <code>float</code> DEFAULT: <code>10000.0</code> </p>"},{"location":"reference/edspdf/layers/sinusoidal_embedding/#edspdf.layers.sinusoidal_embedding.SinusoidalEmbedding.forward","title":"<code>forward</code>","text":"<p>Forward pass of the SinusoidalEmbedding module</p> PARAMETER  DESCRIPTION <code>indices</code> <p>Shape: any</p> <p> TYPE: <code>LongTensor</code> </p> RETURNS DESCRIPTION <code>FloatTensor</code> <p>Shape: <code>(*input_shape, embedding_dim)</code></p>"},{"location":"reference/edspdf/layers/vocabulary/","title":"<code>edspdf.layers.vocabulary</code>","text":""},{"location":"reference/edspdf/layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary","title":"<code>Vocabulary</code>","text":"<p>             Bases: <code>Module</code>, <code>Generic[T]</code></p> <p>Vocabulary layer. This is not meant to be used as a <code>torch.nn.Module</code> but subclassing <code>torch.nn.Module</code> makes the instances appear when printing a model, which is nice.</p> PARAMETER  DESCRIPTION <code>items</code> <p>Initial vocabulary elements if any. Specific elements such as padding and unk can be set here to enforce their index in the vocabulary.</p> <p> TYPE: <code>Sequence[T]</code> DEFAULT: <code>None</code> </p> <code>default</code> <p>Default index to use for out of vocabulary elements Defaults to -100</p> <p> TYPE: <code>int</code> DEFAULT: <code>-100</code> </p>"},{"location":"reference/edspdf/layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary.initialization","title":"<code>initialization</code>","text":"<p>Enters the initialization mode. Out of vocabulary elements will be assigned an index.</p>"},{"location":"reference/edspdf/layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary.encode","title":"<code>encode</code>","text":"<p>Converts an element into its vocabulary index If the layer is in its initialization mode (<code>with vocab.initialization(): ...</code>), and the element is out of vocabulary, a new index will be created and returned. Otherwise, any oov element will be encoded with the <code>default</code> index.</p> PARAMETER  DESCRIPTION <code>item</code> <p> </p> RETURNS DESCRIPTION <code>int</code>"},{"location":"reference/edspdf/layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary.decode","title":"<code>decode</code>","text":"<p>Converts an index into its original value</p> PARAMETER  DESCRIPTION <code>idx</code> <p> </p> RETURNS DESCRIPTION <code>InputT</code>"},{"location":"reference/edspdf/pipes/","title":"<code>edspdf.pipes</code>","text":""},{"location":"reference/edspdf/pipes/aggregators/","title":"<code>edspdf.pipes.aggregators</code>","text":""},{"location":"reference/edspdf/pipes/aggregators/simple/","title":"<code>edspdf.pipes.aggregators.simple</code>","text":""},{"location":"reference/edspdf/pipes/aggregators/simple/#edspdf.pipes.aggregators.simple.SimpleAggregator","title":"<code>SimpleAggregator</code>","text":"<p>Aggregator that returns texts and styles. It groups all text boxes with the same label under the <code>aggregated_text</code>, and additionally aggregates the styles of the text boxes.</p> <p>Examples:</p> <p>Create a pipeline</p> API-basedConfiguration-based <pre><code>pipeline = ...\npipeline.add_pipe(\n    \"simple-aggregator\",\n    name=\"aggregator\",\n    config={\n        \"new_line_threshold\": 0.2,\n        \"new_paragraph_threshold\": 1.5,\n        \"label_map\": {\n            \"body\": \"text\",\n            \"table\": \"text\",\n        },\n    },\n)\n</code></pre> <pre><code>...\n\n[components.aggregator]\n@factory = \"simple-aggregator\"\nnew_line_threshold = 0.2\nnew_paragraph_threshold = 1.5\nlabel_map = { body = \"text\", table = \"text\" }\n\n...\n</code></pre> <p>and run it on a document:</p> <pre><code>doc = pipeline(doc)\nprint(doc.aggregated_texts)\n# {\n#     \"text\": \"This is the body of the document, followed by a table | A | B |\"\n# }\n</code></pre> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'simple-aggregator'</code> </p> <code>sort</code> <p>Whether to sort text boxes inside each label group by (page, y, x) position before merging them.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>new_line_threshold</code> <p>Minimum ratio of the distance between two lines to the median height of lines to consider them as being on separate lines</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.2</code> </p> <code>new_paragraph_threshold</code> <p>Minimum ratio of the distance between two lines to the median height of lines to consider them as being on separate paragraphs and thus add a newline character between them.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.5</code> </p> <code>label_map</code> <p>A dictionary mapping labels to new labels. This is useful to group labels together, for instance, to output both \"body\" and \"table\" as \"text\".</p> <p> TYPE: <code>Dict</code> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edspdf/pipes/classifiers/","title":"<code>edspdf.pipes.classifiers</code>","text":""},{"location":"reference/edspdf/pipes/classifiers/dummy/","title":"<code>edspdf.pipes.classifiers.dummy</code>","text":""},{"location":"reference/edspdf/pipes/classifiers/dummy/#edspdf.pipes.classifiers.dummy.DummyClassifier","title":"<code>DummyClassifier</code>","text":"<p>Dummy classifier, for chaos purposes. Classifies each line to a random element.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object.</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'dummy-classifier'</code> </p> <code>label</code> <p>The label to assign to each line.</p> <p> TYPE: <code>str</code> </p>"},{"location":"reference/edspdf/pipes/classifiers/mask/","title":"<code>edspdf.pipes.classifiers.mask</code>","text":""},{"location":"reference/edspdf/pipes/classifiers/mask/#edspdf.pipes.classifiers.mask.MaskClassifier","title":"<code>MaskClassifier</code>","text":"<p>Simple mask classifier, that labels every box inside one of the masks with its label.</p>"},{"location":"reference/edspdf/pipes/classifiers/mask/#edspdf.pipes.classifiers.mask.simple_mask_classifier_factory","title":"<code>simple_mask_classifier_factory</code>","text":"<p>The simplest form of mask classification. You define the mask, everything else is tagged as pollution.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'mask-classifier'</code> </p> <code>x0</code> <p>The x0 coordinate of the mask</p> <p> TYPE: <code>float</code> </p> <code>y0</code> <p>The y0 coordinate of the mask</p> <p> TYPE: <code>float</code> </p> <code>x1</code> <p>The x1 coordinate of the mask</p> <p> TYPE: <code>float</code> </p> <code>y1</code> <p>The y1 coordinate of the mask</p> <p> TYPE: <code>float</code> </p> <code>threshold</code> <p>The threshold for the alignment</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <p>Examples:</p> API-basedConfiguration-based <pre><code>pipeline.add_pipe(\n    \"mask-classifier\",\n    name=\"classifier\",\n    config={\n        \"threshold\": 0.9,\n        \"x0\": 0.1,\n        \"y0\": 0.1,\n        \"x1\": 0.9,\n        \"y1\": 0.9,\n    },\n)\n</code></pre> <pre><code>[components.classifier]\n@classifiers = \"mask-classifier\"\nx0 = 0.1\ny0 = 0.1\nx1 = 0.9\ny1 = 0.9\nthreshold = 0.9\n</code></pre>"},{"location":"reference/edspdf/pipes/classifiers/mask/#edspdf.pipes.classifiers.mask.mask_classifier_factory","title":"<code>mask_classifier_factory</code>","text":"<p>A generalisation, wherein the user defines a number of regions.</p> <p>The following configuration produces exactly the same classifier as <code>mask.v1</code> example above.</p> <p>Any bloc that is not part of a mask is tagged as <code>pollution</code>.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p> TYPE: <code>str</code> DEFAULT: <code>'multi-mask-classifier'</code> </p> <code>threshold</code> <p>The threshold for the alignment</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> <code>masks</code> <p>The masks</p> <p> TYPE: <code>Box</code> DEFAULT: <code>{}</code> </p> <p>Examples:</p> API-basedConfiguration-based <pre><code>pipeline.add_pipe(\n    \"multi-mask-classifier\",\n    name=\"classifier\",\n    config={\n        \"threshold\": 0.9,\n        \"mymask\": {\"x0\": 0.1, \"y0\": 0.1, \"x1\": 0.9, \"y1\": 0.3, \"label\": \"body\"},\n    },\n)\n</code></pre> <pre><code>[components.classifier]\n@factory = \"multi-mask-classifier\"\nthreshold = 0.9\n\n[components.classifier.mymask]\nlabel = \"body\"\nx0 = 0.1\ny0 = 0.1\nx1 = 0.9\ny1 = 0.9\n</code></pre> <p>The following configuration defines a <code>header</code> region.</p> API-basedConfiguration-based <pre><code>pipeline.add_pipe(\n    \"multi-mask-classifier\",\n    name=\"classifier\",\n    config={\n        \"threshold\": 0.9,\n        \"body\": {\"x0\": 0.1, \"y0\": 0.1, \"x1\": 0.9, \"y1\": 0.3, \"label\": \"header\"},\n        \"header\": {\"x0\": 0.1, \"y0\": 0.3, \"x1\": 0.9, \"y1\": 0.9, \"label\": \"body\"},\n    },\n)\n</code></pre> <pre><code>[components.classifier]\n@factory = \"multi-mask-classifier\"\nthreshold = 0.9\n\n[components.classifier.header]\nlabel = \"header\"\nx0 = 0.1\ny0 = 0.1\nx1 = 0.9\ny1 = 0.3\n\n[components.classifier.body]\nlabel = \"body\"\nx0 = 0.1\ny0 = 0.3\nx1 = 0.9\ny1 = 0.9\n</code></pre>"},{"location":"reference/edspdf/pipes/classifiers/random/","title":"<code>edspdf.pipes.classifiers.random</code>","text":""},{"location":"reference/edspdf/pipes/classifiers/random/#edspdf.pipes.classifiers.random.RandomClassifier","title":"<code>RandomClassifier</code>","text":"<p>Random classifier, for chaos purposes. Classifies each box to a random element.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object.</p> <p> TYPE: <code>Pipeline</code> </p> <code>name</code> <p>The name of the component.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'random-classifier'</code> </p> <code>labels</code> <p>The labels to assign to each line. If a list is passed, each label is assigned with equal probability. If a dict is passed, the keys are the labels and the values are the probabilities.</p> <p> TYPE: <code>Union[List[str], Dict[str, float]]</code> </p>"},{"location":"reference/edspdf/pipes/classifiers/trainable/","title":"<code>edspdf.pipes.classifiers.trainable</code>","text":""},{"location":"reference/edspdf/pipes/classifiers/trainable/#edspdf.pipes.classifiers.trainable.TrainableClassifier","title":"<code>TrainableClassifier</code>","text":"<p>             Bases: <code>TrainablePipe[Dict[str, Any]]</code></p> <p>This component predicts a label for each box over the whole document using machine learning.</p> <p>Note</p> <p>You must train the model your model to use this classifier. See Model training for more information</p> <p>Examples:</p> <p>The classifier is composed of the following blocks:</p> <ul> <li>a configurable box embedding layer</li> <li>a linear classification layer</li> </ul> <p>In this example, we use a <code>box-embedding</code> layer to generate the embeddings of the boxes. It is composed of a text encoder that embeds the text features of the boxes and a layout encoder that embeds the layout features of the boxes. These two embeddings are summed and passed through an optional <code>contextualizer</code>, here a <code>box-transformer</code>.</p> API-basedConfiguration-based <pre><code>pipeline.add_pipe(\n    \"trainable-classifier\",\n    name=\"classifier\",\n    config={\n        # simple embedding computed by pooling embeddings of words in each box\n        \"embedding\": {\n            \"@factory\": \"sub-box-cnn-pooler\",\n            \"out_channels\": 64,\n            \"kernel_sizes\": (3, 4, 5),\n            \"embedding\": {\n                \"@factory\": \"simple-text-embedding\",\n                \"size\": 72,\n            },\n        },\n        \"labels\": [\"body\", \"pollution\"],\n    },\n)\n</code></pre> <pre><code>[components.classifier]\n@factory = \"trainable-classifier\"\nlabels = [\"body\", \"pollution\"]\n\n[components.classifier.embedding]\n@factory = \"sub-box-cnn-pooler\"\nout_channels = 64\nkernel_sizes = (3, 4, 5)\n\n[components.classifier.embedding.embedding]\n@factory = \"simple-text-embedding\"\nsize = 72\n</code></pre> PARAMETER  DESCRIPTION <code>labels</code> <p>Initial labels of the classifier (will be completed during initialization)</p> <p> TYPE: <code>Sequence[str]</code> DEFAULT: <code>('pollution')</code> </p> <code>embedding</code> <p>Embedding module to encode the PDF boxes</p> <p> TYPE: <code>TrainablePipe[EmbeddingOutput]</code> </p>"},{"location":"reference/edspdf/pipes/embeddings/","title":"<code>edspdf.pipes.embeddings</code>","text":""},{"location":"reference/edspdf/pipes/embeddings/box_layout_embedding/","title":"<code>edspdf.pipes.embeddings.box_layout_embedding</code>","text":""},{"location":"reference/edspdf/pipes/embeddings/box_layout_embedding/#edspdf.pipes.embeddings.box_layout_embedding.BoxLayoutEmbedding","title":"<code>BoxLayoutEmbedding</code>","text":"<p>             Bases: <code>TrainablePipe[EmbeddingOutput]</code></p> <p>This component encodes the geometrical features of a box, as extracted by the BoxLayoutPreprocessor module, into an embedding. For position modes, use:</p> <ul> <li><code>\"sin\"</code> to embed positions with a fixed   SinusoidalEmbedding</li> <li><code>\"learned\"</code> to embed positions using a learned standard pytorch embedding layer</li> </ul> <p>Each produces embedding is the concatenation of the box width, height and the top, left, bottom and right coordinates, each embedded depending on the <code>*_mode</code> param.</p> PARAMETER  DESCRIPTION <code>size</code> <p>Size of the output box embedding</p> <p> TYPE: <code>int</code> </p> <code>n_positions</code> <p>Number of position embeddings stored in the PositionEmbedding module</p> <p> TYPE: <code>int</code> </p> <code>x_mode</code> <p>Position embedding mode of the x coordinates</p> <p> TYPE: <code>Literal['sin', 'learned']</code> DEFAULT: <code>'sin'</code> </p> <code>y_mode</code> <p>Position embedding mode of the x coordinates</p> <p> TYPE: <code>Literal['sin', 'learned']</code> DEFAULT: <code>'sin'</code> </p> <code>w_mode</code> <p>Position embedding mode of the width features</p> <p> TYPE: <code>Literal['sin', 'learned']</code> DEFAULT: <code>'sin'</code> </p> <code>h_mode</code> <p>Position embedding mode of the height features</p> <p> TYPE: <code>Literal['sin', 'learned']</code> DEFAULT: <code>'sin'</code> </p>"},{"location":"reference/edspdf/pipes/embeddings/box_layout_preprocessor/","title":"<code>edspdf.pipes.embeddings.box_layout_preprocessor</code>","text":""},{"location":"reference/edspdf/pipes/embeddings/box_layout_preprocessor/#edspdf.pipes.embeddings.box_layout_preprocessor.BoxLayoutPreprocessor","title":"<code>BoxLayoutPreprocessor</code>","text":"<p>             Bases: <code>TrainablePipe[BoxLayoutBatch]</code></p> <p>The box preprocessor is singleton since its is not configurable. The following features of each box of an input PDFDoc document are encoded as 1D tensors:</p> <ul> <li><code>boxes_page</code>: page index of the box</li> <li><code>boxes_first_page</code>: is the box on the first page</li> <li><code>boxes_last_page</code>: is the box on the last page</li> <li><code>boxes_xmin</code>: left position of the box</li> <li><code>boxes_ymin</code>: bottom position of the box</li> <li><code>boxes_xmax</code>: right position of the box</li> <li><code>boxes_ymax</code>: top position of the box</li> <li><code>boxes_w</code>: width position of the box</li> <li><code>boxes_h</code>: height position of the box</li> </ul> <p>The preprocessor also returns an additional tensors:</p> <ul> <li><code>page_boxes_id</code>: box indices per page to index the   above 1D tensors (LongTensor: n_pages * n_boxes)</li> </ul>"},{"location":"reference/edspdf/pipes/embeddings/box_transformer/","title":"<code>edspdf.pipes.embeddings.box_transformer</code>","text":""},{"location":"reference/edspdf/pipes/embeddings/box_transformer/#edspdf.pipes.embeddings.box_transformer.BoxTransformer","title":"<code>BoxTransformer</code>","text":"<p>             Bases: <code>TrainablePipe[EmbeddingOutput]</code></p> <p>BoxTransformer using BoxTransformerModule under the hood.</p> <p>Note</p> <p>This module is a TrainablePipe and can be used in a Pipeline, while BoxTransformerModule is a standard PyTorch module, which does not take care of the preprocessing, collating, etc. of the input documents.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>Pipeline instance</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'box-transformer'</code> </p> <code>num_heads</code> <p>Number of attention heads in the attention layers</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>n_relative_positions</code> <p>Maximum range of embeddable relative positions between boxes (further distances are capped to \u00b1n_relative_positions // 2)</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>dropout_p</code> <p>Dropout probability both for the attention layers and embedding projections</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>head_size</code> <p>Head sizes of the attention layers</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>activation</code> <p>Activation function used in the linear-&gt;activation-&gt;linear transformations</p> <p> TYPE: <code>ActivationFunction</code> DEFAULT: <code>'gelu'</code> </p> <code>init_resweight</code> <p>Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge.</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>attention_mode</code> <p>Mode of relative position infused attention layer. See the relative attention documentation for more information.</p> <p> TYPE: <code>Sequence[Literal['c2c', 'c2p', 'p2c']]</code> DEFAULT: <code>('c2c', 'c2p', 'p2c')</code> </p> <code>n_layers</code> <p>Number of layers in the Transformer</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p>"},{"location":"reference/edspdf/pipes/embeddings/embedding_combiner/","title":"<code>edspdf.pipes.embeddings.embedding_combiner</code>","text":""},{"location":"reference/edspdf/pipes/embeddings/embedding_combiner/#edspdf.pipes.embeddings.embedding_combiner.EmbeddingCombiner","title":"<code>EmbeddingCombiner</code>","text":"<p>             Bases: <code>TrainablePipe[EmbeddingOutput]</code></p> <p>Encodes boxes using a combination of multiple encoders</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline object</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The name of the pipe</p> <p> TYPE: <code>str</code> DEFAULT: <code>'embedding-combiner'</code> </p> <code>mode</code> <p>The mode to use to combine the encoders:</p> <ul> <li><code>sum</code>: Sum the outputs of the encoders</li> <li><code>cat</code>: Concatenate the outputs of the encoders</li> </ul> <p> TYPE: <code>Literal['sum', 'cat']</code> DEFAULT: <code>'sum'</code> </p> <code>dropout_p</code> <p>Dropout probability used on the output of the box and textual encoders</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.0</code> </p> <code>encoders</code> <p>The encoders to use. The keys are the names of the encoders and the values are the encoders themselves.</p> <p> TYPE: <code>TrainablePipe[EmbeddingOutput]</code> DEFAULT: <code>{}</code> </p>"},{"location":"reference/edspdf/pipes/embeddings/huggingface_embedding/","title":"<code>edspdf.pipes.embeddings.huggingface_embedding</code>","text":""},{"location":"reference/edspdf/pipes/embeddings/huggingface_embedding/#edspdf.pipes.embeddings.huggingface_embedding.HuggingfaceEmbedding","title":"<code>HuggingfaceEmbedding</code>","text":"<p>             Bases: <code>TrainablePipe[EmbeddingOutput]</code></p> <p>The HuggingfaceEmbeddings component is a wrapper around the Huggingface multi-modal models. Such pre-trained models should offer better results than a model trained from scratch. Compared to using the raw Huggingface model, we offer a simple mechanism to split long documents into strided windows before feeding them to the model.</p>"},{"location":"reference/edspdf/pipes/embeddings/huggingface_embedding/#edspdf.pipes.embeddings.huggingface_embedding.HuggingfaceEmbedding--windowing","title":"Windowing","text":"<p>The HuggingfaceEmbedding component splits long documents into smaller windows before feeding them to the model. This is done to avoid hitting the maximum number of tokens that can be processed by the model on a single device. The window size and stride can be configured using the <code>window</code> and <code>stride</code> parameters. The default values are 510 and 255 respectively, which means that the model will process windows of 510 tokens, each separated by 255 tokens. Whenever a token appears in multiple windows, the embedding of the \"most contextualized\" occurrence is used, i.e. the occurrence that is the closest to the center of its window.</p> <p>Here is an overview how this works in a classifier model : </p> <p>Examples:</p> <p>Here is an example of how to define a pipeline with the HuggingfaceEmbedding component:</p> <pre><code>from edspdf import Pipeline\n\nmodel = Pipeline()\nmodel.add_pipe(\n    \"pdfminer-extractor\",\n    name=\"extractor\",\n    config={\n        \"render_pages\": True,\n    },\n)\nmodel.add_pipe(\n    \"huggingface-embedding\",\n    name=\"embedding\",\n    config={\n        \"model\": \"microsoft/layoutlmv3-base\",\n        \"use_image\": False,\n        \"window\": 128,\n        \"stride\": 64,\n        \"line_pooling\": \"mean\",\n    },\n)\nmodel.add_pipe(\n    \"trainable-classifier\",\n    name=\"classifier\",\n    config={\n        \"embedding\": model.get_pipe(\"embedding\"),\n        \"labels\": [],\n    },\n)\n</code></pre> <p>This model can then be trained following the training recipe.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>The pipeline instance</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>The component name</p> <p> TYPE: <code>str</code> DEFAULT: <code>'huggingface-embedding'</code> </p> <code>model</code> <p>The Huggingface model name or path</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>use_image</code> <p>Whether to use the image or not in the model</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>window</code> <p>The window size to use when splitting long documents into smaller windows before feeding them to the Transformer model (default: 510 = 512 - 2)</p> <p> TYPE: <code>int</code> DEFAULT: <code>510</code> </p> <code>stride</code> <p>The stride (distance between windows) to use when splitting long documents into smaller windows: (default: 510 / 2 = 255)</p> <p> TYPE: <code>int</code> DEFAULT: <code>255</code> </p> <code>line_pooling</code> <p>The pooling strategy to use when combining the embeddings of the tokens in a line into a single line embedding</p> <p> TYPE: <code>Literal['mean', 'max', 'sum']</code> DEFAULT: <code>'mean'</code> </p> <code>max_tokens_per_device</code> <p>The maximum number of tokens that can be processed by the model on a single device. This does not affect the results but can be used to reduce the memory usage of the model, at the cost of a longer processing time.</p> <p> TYPE: <code>int</code> DEFAULT: <code>128 * 128</code> </p>"},{"location":"reference/edspdf/pipes/embeddings/simple_text_embedding/","title":"<code>edspdf.pipes.embeddings.simple_text_embedding</code>","text":""},{"location":"reference/edspdf/pipes/embeddings/simple_text_embedding/#edspdf.pipes.embeddings.simple_text_embedding.SimpleTextEmbedding","title":"<code>SimpleTextEmbedding</code>","text":"<p>             Bases: <code>TrainablePipe[EmbeddingOutput]</code></p> <p>A module that embeds the textual features of the blocks</p> PARAMETER  DESCRIPTION <code>size</code> <p>Size of the output box embedding</p> <p> TYPE: <code>int</code> </p> <code>pipeline</code> <p>The pipeline object</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'simple-text-embedding'</code> </p>"},{"location":"reference/edspdf/pipes/embeddings/simple_text_embedding/#edspdf.pipes.embeddings.simple_text_embedding.word_shape","title":"<code>word_shape</code>","text":"<p>Converts a word into its shape following the algorithm used in the spaCy library.</p> <p>https://github.com/explosion/spaCy/blob/b69d249a/spacy/lang/lex_attrs.py#L118</p> PARAMETER  DESCRIPTION <code>text</code> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <code>The word shape</code>"},{"location":"reference/edspdf/pipes/embeddings/sub_box_cnn_pooler/","title":"<code>edspdf.pipes.embeddings.sub_box_cnn_pooler</code>","text":""},{"location":"reference/edspdf/pipes/embeddings/sub_box_cnn_pooler/#edspdf.pipes.embeddings.sub_box_cnn_pooler.SubBoxCNNPooler","title":"<code>SubBoxCNNPooler</code>","text":"<p>             Bases: <code>TrainablePipe[EmbeddingOutput]</code></p> <p>One dimension CNN encoding multi-kernel layer. Input embeddings are convoluted using linear kernels each parametrized with a (window) size of <code>kernel_size[kernel_i]</code> The output of the kernels are concatenated together, max-pooled and finally projected to a size of <code>output_size</code>.</p> PARAMETER  DESCRIPTION <code>pipeline</code> <p>Pipeline instance</p> <p> TYPE: <code>Pipeline</code> DEFAULT: <code>None</code> </p> <code>name</code> <p>Name of the component</p> <p> TYPE: <code>str</code> DEFAULT: <code>'sub-box-cnn-pooler'</code> </p> <code>output_size</code> <p>Size of the output embeddings Defaults to the <code>input_size</code></p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>out_channels</code> <p>Number of channels</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>kernel_sizes</code> <p>Window size of each kernel</p> <p> TYPE: <code>Sequence[int]</code> DEFAULT: <code>(3, 4, 5)</code> </p> <code>activation</code> <p>Activation function to use</p> <p> TYPE: <code>ActivationFunction</code> DEFAULT: <code>'relu'</code> </p>"},{"location":"reference/edspdf/pipes/extractors/","title":"<code>edspdf.pipes.extractors</code>","text":""},{"location":"reference/edspdf/pipes/extractors/pdfminer/","title":"<code>edspdf.pipes.extractors.pdfminer</code>","text":""},{"location":"reference/edspdf/pipes/extractors/pdfminer/#edspdf.pipes.extractors.pdfminer.PdfMinerExtractor","title":"<code>PdfMinerExtractor</code>","text":"<p>We provide a PDF line extractor built on top of PdfMiner.</p> <p>This is the most portable extractor, since it is pure-python and can therefore be run on any platform. Be sure to have a look at their documentation, especially the part providing a bird's eye view of the PDF extraction process.</p> <p>Examples:</p> API-basedConfiguration-based <pre><code>pipeline.add_pipe(\n    \"pdfminer-extractor\",\n    config=dict(\n        extract_style=False,\n    ),\n)\n</code></pre> <pre><code>[components.extractor]\n@factory = \"pdfminer-extractor\"\nextract_style = false\n</code></pre> <p>And use the pipeline on a PDF document:</p> <pre><code>from pathlib import Path\n\n# Apply on a new document\npipeline(Path(\"path/to/your/pdf/document\").read_bytes())\n</code></pre> PARAMETER  DESCRIPTION <code>line_overlap</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>char_margin</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>float</code> DEFAULT: <code>2.05</code> </p> <code>line_margin</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.5</code> </p> <code>word_margin</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.1</code> </p> <code>boxes_flow</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>Optional[float]</code> DEFAULT: <code>0.5</code> </p> <code>detect_vertical</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>all_texts</code> <p>See PDFMiner documentation</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>extract_style</code> <p>Whether to extract style (font, size, ...) information for each line of the document. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>render_pages</code> <p>Whether to extract the rendered page as a numpy array in the <code>page.image</code> attribute (defaults to False)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>render_dpi</code> <p>DPI to use when rendering the page (defaults to 200)</p> <p> TYPE: <code>int</code> DEFAULT: <code>200</code> </p> <code>raise_on_error</code> <p>Whether to raise an error if the PDF cannot be parsed. Default: False</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p>"},{"location":"reference/edspdf/utils/","title":"<code>edspdf.utils</code>","text":""},{"location":"reference/edspdf/utils/alignment/","title":"<code>edspdf.utils.alignment</code>","text":""},{"location":"reference/edspdf/utils/alignment/#edspdf.utils.alignment.align_box_labels","title":"<code>align_box_labels</code>","text":"<p>Align lines with possibly overlapping (and non-exhaustive) labels.</p> <p>Possible matches are sorted by covered area. Lines with no overlap at all</p> PARAMETER  DESCRIPTION <code>src_boxes</code> <p>The labelled boxes that will be used to determine the label of the dst_boxes</p> <p> TYPE: <code>Sequence[Box]</code> </p> <code>dst_boxes</code> <p>The non-labelled boxes that will be assigned a label</p> <p> TYPE: <code>Sequence[T]</code> </p> <code>threshold</code> <p>Threshold to use for discounting a label. Used if the <code>labels</code> DataFrame does not provide a <code>threshold</code> column, or to fill <code>NaN</code> values thereof.</p> <p> TYPE: <code>float</code> DEFAULT: <code>1</code> </p> <code>pollution_label</code> <p>The label to use for boxes that are not covered by any of the source boxes</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[Box]</code> <p>A copy of the boxes, with the labels mapped from the source boxes</p>"},{"location":"reference/edspdf/utils/collections/","title":"<code>edspdf.utils.collections</code>","text":""},{"location":"reference/edspdf/utils/collections/#edspdf.utils.collections.multi_tee","title":"<code>multi_tee</code>","text":"<p>Makes copies of an iterable such that every iteration over it starts from 0. If the iterable is a sequence (list, tuple), just returns it since every iter() over the object restart from the beginning</p>"},{"location":"reference/edspdf/utils/collections/#edspdf.utils.collections.FrozenDict","title":"<code>FrozenDict</code>","text":"<p>             Bases: <code>dict</code></p> <p>Copied from <code>spacy.util.SimpleFrozenDict</code> to ensure compatibility.</p> <p>Initialize the frozen dict. Can be initialized with pre-defined values.</p> <p>error (str): The error message when user tries to assign to dict.</p>"},{"location":"reference/edspdf/utils/collections/#edspdf.utils.collections.FrozenList","title":"<code>FrozenList</code>","text":"<p>             Bases: <code>list</code></p> <p>Copied from <code>spacy.util.SimpleFrozenDict</code> to ensure compatibility</p> <p>Initialize the frozen list.</p> <p>error (str): The error message when user tries to mutate the list.</p>"},{"location":"reference/edspdf/utils/optimization/","title":"<code>edspdf.utils.optimization</code>","text":""},{"location":"reference/edspdf/utils/package/","title":"<code>edspdf.utils.package</code>","text":""},{"location":"reference/edspdf/utils/package/#edspdf.utils.package.PoetryPackager","title":"<code>PoetryPackager</code>","text":""},{"location":"reference/edspdf/utils/package/#edspdf.utils.package.PoetryPackager.ensure_pyproject","title":"<code>ensure_pyproject</code>","text":"<p>Generates a Poetry based pyproject.toml</p>"},{"location":"reference/edspdf/utils/random/","title":"<code>edspdf.utils.random</code>","text":""},{"location":"reference/edspdf/utils/random/#edspdf.utils.random.set_seed","title":"<code>set_seed</code>","text":"<p>Set seed values for random generators. If used as a context, restore the random state used before entering the context.</p> PARAMETER  DESCRIPTION <code>seed</code> <p>Value used as a seed.</p> <p> </p> <code>cuda</code> <p>Saves the cuda random states too</p> <p> DEFAULT: <code>is_available()</code> </p>"},{"location":"reference/edspdf/utils/random/#edspdf.utils.random.get_random_generator_state","title":"<code>get_random_generator_state</code>","text":"<p>Get the <code>torch</code>, <code>numpy</code> and <code>random</code> random generator state.</p> PARAMETER  DESCRIPTION <code>cuda</code> <p>Saves the cuda random states too</p> <p> DEFAULT: <code>is_available()</code> </p> RETURNS DESCRIPTION <code>RandomGeneratorState</code>"},{"location":"reference/edspdf/utils/random/#edspdf.utils.random.set_random_generator_state","title":"<code>set_random_generator_state</code>","text":"<p>Set the <code>torch</code>, <code>numpy</code> and <code>random</code> random generator state.</p> PARAMETER  DESCRIPTION <code>state</code> <p> </p>"},{"location":"reference/edspdf/utils/torch/","title":"<code>edspdf.utils.torch</code>","text":""},{"location":"reference/edspdf/utils/torch/#edspdf.utils.torch.compute_pdf_relative_positions","title":"<code>compute_pdf_relative_positions</code>","text":"<p>Compute relative positions between boxes. Input boxes must be split between pages with the shape n_pages * n_boxes</p> PARAMETER  DESCRIPTION <code>x0</code> <p> </p> <code>y0</code> <p> </p> <code>x1</code> <p> </p> <code>y1</code> <p> </p> <code>width</code> <p> </p> <code>height</code> <p> </p> <code>n_relative_positions</code> <p>Maximum range of embeddable relative positions between boxes (further distances will be capped to \u00b1n_relative_positions // 2)</p> <p> </p> RETURNS DESCRIPTION <code>LongTensor</code> <p>Shape: n_pages * n_boxes * n_boxes * 2</p>"},{"location":"reference/edspdf/visualization/","title":"<code>edspdf.visualization</code>","text":""},{"location":"reference/edspdf/visualization/annotations/","title":"<code>edspdf.visualization.annotations</code>","text":""},{"location":"reference/edspdf/visualization/annotations/#edspdf.visualization.annotations.show_annotations","title":"<code>show_annotations</code>","text":"<p>Show Box annotations on a PDF document.</p> PARAMETER  DESCRIPTION <code>pdf</code> <p>Bytes content of the PDF document</p> <p> TYPE: <code>bytes</code> </p> <code>annotations</code> <p>List of Box annotations to show</p> <p> TYPE: <code>Sequence[Box]</code> </p> <code>colors</code> <p>Colors to use for each label. If a list is provided, it will be used to color the first <code>len(colors)</code> unique labels. If a dictionary is provided, it will be used to color the labels in the dictionary. If None, a default color scheme will be used.</p> <p> TYPE: <code>Optional[Union[Dict[str, str], List[str]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[PpmImageFile]</code> <p>List of PIL images with the annotations. You can display them in a notebook with <code>display(*pages)</code>.</p>"},{"location":"reference/edspdf/visualization/annotations/#edspdf.visualization.annotations.compare_results","title":"<code>compare_results</code>","text":"<p>Compare two sets of annotations on a PDF document.</p> PARAMETER  DESCRIPTION <code>pdf</code> <p>Bytes content of the PDF document</p> <p> TYPE: <code>bytes</code> </p> <code>pred</code> <p>List of Box annotations to show on the left side</p> <p> TYPE: <code>Sequence[Box]</code> </p> <code>gold</code> <p>List of Box annotations to show on the right side</p> <p> TYPE: <code>Sequence[Box]</code> </p> <code>colors</code> <p>Colors to use for each label. If a list is provided, it will be used to color the first <code>len(colors)</code> unique labels. If a dictionary is provided, it will be used to color the labels in the dictionary. If None, a default color scheme will be used.</p> <p> TYPE: <code>Optional[Union[Dict[str, str], List[str]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[PpmImageFile]</code> <p>List of PIL images with the annotations. You can display them in a notebook with <code>display(*pages)</code>.</p>"},{"location":"reference/edspdf/visualization/merge/","title":"<code>edspdf.visualization.merge</code>","text":""},{"location":"reference/edspdf/visualization/merge/#edspdf.visualization.merge.merge_boxes","title":"<code>merge_boxes</code>","text":"<p>Recursively merge boxes that have the same label to form larger non-overlapping boxes.</p> PARAMETER  DESCRIPTION <code>boxes</code> <p>List of boxes to merge</p> <p> TYPE: <code>Sequence[Box]</code> </p> RETURNS DESCRIPTION <code>List[Box]</code> <p>List of merged boxes</p>"},{"location":"utilities/","title":"Overview","text":"<p>EDS-PDF provides a few utilities help annotate PDF documents, and debug the output of an extraction pipeline.</p>"},{"location":"utilities/alignment/","title":"Alignment","text":"<p>To simplify the annotation process, EDS-PDF provides a utility that aligns bounding boxes with text blocs extracted from a PDF document. This is particularly useful for annotating documents.</p> BlocsBlocs + AnnotationAlignedMerged Blocs <p></p> <p></p> <p></p> <p></p>"},{"location":"utilities/visualisation/","title":"Visualisation","text":"<p>EDS-PDF provides utilities to help you visualise the output of the pipeline.</p>"},{"location":"utilities/visualisation/#visualising-a-pipelines-output","title":"Visualising a pipeline's output","text":"<p>You can use EDS-PDF to overlay labelled bounding boxes on top of a PDF document.</p> <pre><code>import edspdf\nfrom confit import Config\nfrom pathlib import Path\nfrom edspdf.visualization import show_annotations\n\nconfig = \"\"\"\n[pipeline]\npipeline = [\"extractor\", \"classifier\"]\n\n[components]\n\n[components.extractor]\n@factory = \"pdfminer-extractor\"\nextract_style = true\n\n[components.classifier]\n@factory = \"mask-classifier\"\nx0 = 0.25\nx1 = 0.95\ny0 = 0.3\ny1 = 0.9\nthreshold = 0.1\n\"\"\"\n\nmodel = edspdf.load(Config.from_str(config))\n\n# Get a PDF\npdf = Path(\"/Users/perceval/Development/edspdf/tests/resources/letter.pdf\").read_bytes()\n\n# Construct the DataFrame of blocs\ndoc = model(pdf)\n\n# Compute an image representation of each page of the PDF\n# overlaid with the predicted bounding boxes\nimgs = show_annotations(pdf=pdf, annotations=doc.text_boxes)\n\nimgs[0]\n</code></pre> <p>If you run this code in a Jupyter notebook, you'll see the following:</p> <p></p>"},{"location":"utilities/visualisation/#merging-blocs-together","title":"Merging blocs together","text":"<p>To help debug a pipeline (or a labelled dataset), you might want to merge blocs together according to their labels. EDS-PDF provides a <code>merge_lines</code> method that does just that.</p> <pre><code># \u2191 Omitted code above \u2191\nfrom edspdf.visualization import merge_boxes, show_annotations\n\nmerged = merge_boxes(doc.text_boxes)\n\nimgs = show_annotations(pdf=pdf, annotations=merged)\nimgs[0]\n</code></pre> <p>See the difference:</p> OriginalMerged <p></p> <p></p> <p>The <code>merge_boxes</code> method uses the notion of maximal cliques to compute merges. It forbids the combined blocs from overlapping with any bloc from another label.</p>"}]}