
<!DOCTYPE html>

<html class="no-js" lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width,initial-scale=1" name="viewport"/>
<link href="../../../assets/logo/edspdf-blue.svg" rel="icon"/>
<meta content="mkdocs-1.3.0, mkdocs-material-8.5.4" name="generator"/>
<title>relative_attention - EDS-PDF</title>
<link href="../../../assets/stylesheets/main.80dcb947.min.css" rel="stylesheet"/>
<link href="../../../assets/stylesheets/palette.cbb835fc.min.css" rel="stylesheet"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/>
<style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
<link href="../../../assets/_mkdocstrings.css" rel="stylesheet"/>
<link href="../../../assets/stylesheets/extra.css" rel="stylesheet"/>
<link href="../../../assets/termynal/termynal.css" rel="stylesheet"/>
<script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
<link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>html.glightbox-open { overflow: initial; height: 100%; }</style><script src="../../../assets/javascripts/glightbox.min.js"></script></head>
<body data-md-color-accent="" data-md-color-primary="" data-md-color-scheme="default" dir="ltr">
<script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
<input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/>
<input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/>
<label class="md-overlay" for="__drawer"></label>
<div data-md-component="skip">
<a class="md-skip" href="#edspdflayersrelative_attention">
          Skip to content
        </a>
</div>
<div data-md-component="announce">
</div>
<div data-md-component="outdated" hidden="">
</div>
<header class="md-header" data-md-component="header">
<nav aria-label="Header" class="md-header__inner md-grid">
<a aria-label="EDS-PDF" class="md-header__button md-logo" data-md-component="logo" href="../../.." title="EDS-PDF">
<img alt="logo" src="../../../assets/logo/edspdf-white.svg"/>
</a>
<label class="md-header__button md-icon" for="__drawer">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg>
</label>
<div class="md-header__title" data-md-component="header-title">
<div class="md-header__ellipsis">
<div class="md-header__topic">
<span class="md-ellipsis">
            EDS-PDF
          </span>
</div>
<div class="md-header__topic" data-md-component="header-topic">
<span class="md-ellipsis">
            
              relative_attention
            
          </span>
</div>
</div>
</div>
<form class="md-header__option" data-md-component="palette">
<input aria-label="Switch to dark mode" class="md-option" data-md-color-accent="" data-md-color-media="" data-md-color-primary="" data-md-color-scheme="default" id="__palette_1" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_2" hidden="" title="Switch to dark mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
</label>
<input aria-label="Switch to light mode" class="md-option" data-md-color-accent="" data-md-color-media="" data-md-color-primary="" data-md-color-scheme="slate" id="__palette_2" name="__palette" type="radio"/>
<label class="md-header__button md-icon" for="__palette_1" hidden="" title="Switch to light mode">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg>
</label>
</form>
<label class="md-header__button md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
</label>
<div class="md-search" data-md-component="search" role="dialog">
<label class="md-search__overlay" for="__search"></label>
<div class="md-search__inner" role="search">
<form class="md-search__form" name="search">
<input aria-label="Search" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="Search" required="" spellcheck="false" type="text"/>
<label class="md-search__icon md-icon" for="__search">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg>
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</label>
<nav aria-label="Search" class="md-search__options">
<button aria-label="Clear" class="md-search__icon md-icon" tabindex="-1" title="Clear" type="reset">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg>
</button>
</nav>
</form>
<div class="md-search__output">
<div class="md-search__scrollwrap" data-md-scrollfix="">
<div class="md-search-result" data-md-component="search-result">
<div class="md-search-result__meta">
            Initializing search
          </div>
<ol class="md-search-result__list"></ol>
</div>
</div>
</div>
</div>
</div>
<div class="md-header__source">
<a class="md-source" data-md-component="source" href="https://github.com/aphp/edspdf" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
</nav>
</header>
<div class="md-container" data-md-component="container">
<main class="md-main" data-md-component="main">
<div class="md-main__inner md-grid">
<div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Navigation" class="md-nav md-nav--primary" data-md-level="0">
<label class="md-nav__title" for="__drawer">
<a aria-label="EDS-PDF" class="md-nav__button md-logo" data-md-component="logo" href="../../.." title="EDS-PDF">
<img alt="logo" src="../../../assets/logo/edspdf-white.svg"/>
</a>
    EDS-PDF
  </label>
<div class="md-nav__source">
<a class="md-source" data-md-component="source" href="https://github.com/aphp/edspdf" title="Go to repository">
<div class="md-source__icon md-icon">
<svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg>
</div>
<div class="md-source__repository">
    GitHub
  </div>
</a>
</div>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../..">
        Overview
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="https://share.streamlit.io/aphp/edspdf/demo/app.py" target="_blank">
        Demo ðŸš€
      </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" id="__nav_3" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../../concepts/">Concepts</a>
<label for="__nav_3">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Concepts" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_3">
<span class="md-nav__icon md-icon"></span>
          Concepts
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../concepts/pipeline/">
        Pipeline
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../concepts/configuration/">
        Configuration
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../concepts/trainable-components/">
        Trainable components
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" id="__nav_4" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../../recipes/">Recipes</a>
<label for="__nav_4">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Recipes" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_4">
<span class="md-nav__icon md-icon"></span>
          Recipes
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../recipes/rule-based/">
        Rule-based extraction
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../recipes/training/">
        Training a Pipeline
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../recipes/extension/">
        Extending EDS-PDF
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../recipes/annotation/">
        PDF Annotation
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" id="__nav_5" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../../components/">Components</a>
<label for="__nav_5">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Components" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_5">
<span class="md-nav__icon md-icon"></span>
          Components
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_1" id="__nav_5_1" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../../components/extractors/">Extractors</a>
<label for="__nav_5_1">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Extractors" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_1">
<span class="md-nav__icon md-icon"></span>
          Extractors
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../components/extractors/pdfminer/">
        PdfMinerExtractor
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../components/extractors/mupdf/">
        MuPdfExtractor
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../components/extractors/poppler/">
        PopplerExtractor
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_2" id="__nav_5_2" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../../components/classifiers/">Classifiers</a>
<label for="__nav_5_2">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Classifiers" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_2">
<span class="md-nav__icon md-icon"></span>
          Classifiers
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../components/classifiers/deep-classifier/">
        Deep learning classifier
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../components/classifiers/mask/">
        Mask Classification
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_3" id="__nav_5_3" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../../components/aggregators/">Aggregators</a>
</div>
<nav aria-label="Aggregators" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_5_3">
<span class="md-nav__icon md-icon"></span>
          Aggregators
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" id="__nav_6" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../../layers/">Layers</a>
</div>
<nav aria-label="Layers" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_6">
<span class="md-nav__icon md-icon"></span>
          Layers
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_7" id="__nav_7" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../../utilities/">Utilities</a>
<label for="__nav_7">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="Utilities" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_7">
<span class="md-nav__icon md-icon"></span>
          Utilities
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../../utilities/visualisation/">
        Visualisation
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../utilities/alignment/">
        Alignment
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" id="__nav_8" type="checkbox"/>
<label class="md-nav__link" for="__nav_8">
          Code Reference
          <span class="md-nav__icon md-icon"></span>
</label>
<nav aria-label="Code Reference" class="md-nav" data-md-level="1">
<label class="md-nav__title" for="__nav_8">
<span class="md-nav__icon md-icon"></span>
          Code Reference
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_1" id="__nav_8_1" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../">edspdf</a>
<label for="__nav_8_1">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="edspdf" class="md-nav" data-md-level="2">
<label class="md-nav__title" for="__nav_8_1">
<span class="md-nav__icon md-icon"></span>
          edspdf
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../cli/">
        cli
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../component/">
        component
      </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_1_4" id="__nav_8_1_4" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../components/">components</a>
<label for="__nav_8_1_4">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="components" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_8_1_4">
<span class="md-nav__icon md-icon"></span>
          components
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_1_4_2" id="__nav_8_1_4_2" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../components/aggregators/">aggregators</a>
<label for="__nav_8_1_4_2">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="aggregators" class="md-nav" data-md-level="4">
<label class="md-nav__title" for="__nav_8_1_4_2">
<span class="md-nav__icon md-icon"></span>
          aggregators
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../components/aggregators/simple/">
        simple
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../components/aggregators/styled/">
        styled
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_1_4_3" id="__nav_8_1_4_3" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../components/classifiers/">classifiers</a>
<label for="__nav_8_1_4_3">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="classifiers" class="md-nav" data-md-level="4">
<label class="md-nav__title" for="__nav_8_1_4_3">
<span class="md-nav__icon md-icon"></span>
          classifiers
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../components/classifiers/deep_classifier/">
        deep_classifier
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../components/classifiers/dummy/">
        dummy
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../components/classifiers/harmonized_classifier/">
        harmonized_classifier
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../components/classifiers/mask/">
        mask
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../components/classifiers/random/">
        random
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_1_4_4" id="__nav_8_1_4_4" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../components/extractors/">extractors</a>
<label for="__nav_8_1_4_4">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="extractors" class="md-nav" data-md-level="4">
<label class="md-nav__title" for="__nav_8_1_4_4">
<span class="md-nav__icon md-icon"></span>
          extractors
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../components/extractors/pdfminer/">
        pdfminer
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../config/">
        config
      </a>
</li>
<li class="md-nav__item md-nav__item--active md-nav__item--nested">
<input checked="" class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_1_6" id="__nav_8_1_6" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../">layers</a>
<label for="__nav_8_1_6">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="layers" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_8_1_6">
<span class="md-nav__icon md-icon"></span>
          layers
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../box_embedding/">
        box_embedding
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../box_layout_embedding/">
        box_layout_embedding
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../box_layout_preprocessor/">
        box_layout_preprocessor
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../box_text_embedding/">
        box_text_embedding
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../box_transformer/">
        box_transformer
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../cnn_pooler/">
        cnn_pooler
      </a>
</li>
<li class="md-nav__item md-nav__item--active">
<input class="md-nav__toggle md-toggle" data-md-toggle="toc" id="__toc" type="checkbox"/>
<label class="md-nav__link md-nav__link--active" for="__toc">
          relative_attention
          <span class="md-nav__icon md-icon"></span>
</label>
<a class="md-nav__link md-nav__link--active" href="./">
        relative_attention
      </a>
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#edspdf.layers.relative_attention.RelativeAttention">
    RelativeAttention
  </a>
<nav aria-label="RelativeAttention" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#edspdf.layers.relative_attention.RelativeAttention.__init__">
    __init__()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#edspdf.layers.relative_attention.RelativeAttention.forward">
    forward()
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../sinusoidal_embedding/">
        sinusoidal_embedding
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../vocabulary/">
        vocabulary
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../model/">
        model
      </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_1_8" id="__nav_8_1_8" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../models/">models</a>
<label for="__nav_8_1_8">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="models" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_8_1_8">
<span class="md-nav__icon md-icon"></span>
          models
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../models/box/">
        box
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../models/doc/">
        doc
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../models/style/">
        style
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../models/text_box/">
        text_box
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../pipeline/">
        pipeline
      </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_1_10" id="__nav_8_1_10" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../recipes/">recipes</a>
<label for="__nav_8_1_10">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="recipes" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_8_1_10">
<span class="md-nav__icon md-icon"></span>
          recipes
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../recipes/train/">
        train
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../registry/">
        registry
      </a>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_1_12" id="__nav_8_1_12" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../utils/">utils</a>
<label for="__nav_8_1_12">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="utils" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_8_1_12">
<span class="md-nav__icon md-icon"></span>
          utils
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../utils/alignment/">
        alignment
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../utils/collections/">
        collections
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../utils/optimization/">
        optimization
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../utils/random/">
        random
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../utils/torch/">
        torch
      </a>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item md-nav__item--nested">
<input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8_1_13" id="__nav_8_1_13" type="checkbox"/>
<div class="md-nav__link md-nav__link--index">
<a href="../../visualization/">visualization</a>
<label for="__nav_8_1_13">
<span class="md-nav__icon md-icon"></span>
</label>
</div>
<nav aria-label="visualization" class="md-nav" data-md-level="3">
<label class="md-nav__title" for="__nav_8_1_13">
<span class="md-nav__icon md-icon"></span>
          visualization
        </label>
<ul class="md-nav__list" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="../../visualization/annotations/">
        annotations
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../visualization/merge/">
        merge
      </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../alternatives/">
        Alternatives &amp; Comparison
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../contributing/">
        Contributing to EDS-PDF
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../changelog/">
        Changelog
      </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="../../../roadmap/">
        Roadmap
      </a>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc">
<div class="md-sidebar__scrollwrap">
<div class="md-sidebar__inner">
<nav aria-label="Table of contents" class="md-nav md-nav--secondary">
<label class="md-nav__title" for="__toc">
<span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
<ul class="md-nav__list" data-md-component="toc" data-md-scrollfix="">
<li class="md-nav__item">
<a class="md-nav__link" href="#edspdf.layers.relative_attention.RelativeAttention">
    RelativeAttention
  </a>
<nav aria-label="RelativeAttention" class="md-nav">
<ul class="md-nav__list">
<li class="md-nav__item">
<a class="md-nav__link" href="#edspdf.layers.relative_attention.RelativeAttention.__init__">
    __init__()
  </a>
</li>
<li class="md-nav__item">
<a class="md-nav__link" href="#edspdf.layers.relative_attention.RelativeAttention.forward">
    forward()
  </a>
</li>
</ul>
</nav>
</li>
</ul>
</nav>
</div>
</div>
</div>
<div class="md-content" data-md-component="content">
<article class="md-content__inner md-typeset">
<a class="md-content__button md-icon" href="https://github.com/aphp/edspdf/edit/master/docs/edspdf/layers/relative_attention.py" title="Edit this page">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"></path></svg>
</a>
<h1 id="edspdflayersrelative_attention"><code>edspdf.layers.relative_attention</code></h1>
<div class="doc doc-object doc-module">
<div class="doc doc-contents first">
<div class="doc doc-children">
<div class="doc doc-object doc-class">
<h2 class="doc doc-heading" id="edspdf.layers.relative_attention.RelativeAttention">
<code>RelativeAttention</code>
</h2>
<div class="doc doc-contents">
<p class="doc doc-class-bases">
        Bases: <code>torch.<span title="torch.nn">nn</span>.<span title="torch.nn.Module">Module</span></code></p>
<p>A self/cross-attention layer that takes relative position of elements into
account to compute the attention weights.
When running a relative attention layer, key and queries are represented using
content and position embeddings, where position embeddings are retrieved using
the relative position of keys relative to queries</p>
<details class="quote">
<summary>Source code in <code>edspdf/layers/relative_attention.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="nd">@registry</span><span class="o">.</span><span class="n">factory</span><span class="o">.</span><span class="n">register</span><span class="p">(</span><span class="s2">"relative-attention"</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">RelativeAttention</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    A self/cross-attention layer that takes relative position of elements into</span>
<span class="sd">    account to compute the attention weights.</span>
<span class="sd">    When running a relative attention layer, key and queries are represented using</span>
<span class="sd">    content and position embeddings, where position embeddings are retrieved using</span>
<span class="sd">    the relative position of keys relative to queries</span>
<span class="sd">    """</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">query_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">key_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">value_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">head_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">position_embedding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
        <span class="n">same_key_query_proj</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">same_positional_key_query_proj</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">n_coordinates</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">head_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">do_pooling</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">mode</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">RelativeAttentionMode</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"c2c"</span><span class="p">,</span> <span class="s2">"p2c"</span><span class="p">,</span> <span class="s2">"c2p"</span><span class="p">),</span>
        <span class="n">n_additional_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        size: int</span>
<span class="sd">            The size of the output embeddings</span>
<span class="sd">            Also serves as default if query_size, pos_size, or key_size is None</span>
<span class="sd">        n_heads: int</span>
<span class="sd">            The number of attention heads</span>
<span class="sd">        query_size: Optional[int]</span>
<span class="sd">            The size of the query embeddings.</span>
<span class="sd">        key_size: Optional[int]</span>
<span class="sd">            The size of the key embeddings.</span>
<span class="sd">        value_size: Optional[int]</span>
<span class="sd">            The size of the value embeddings</span>
<span class="sd">        head_size: Optional[int]</span>
<span class="sd">            The size of each query / key / value chunk used in the attention dot product</span>
<span class="sd">            Default: `key_size / n_heads`</span>
<span class="sd">        position_embedding: Optional[torch.FloatTensor]</span>
<span class="sd">            The position embedding used as key and query embeddings</span>
<span class="sd">        dropout_p: float</span>
<span class="sd">            Dropout probability applied on the attention weights</span>
<span class="sd">            Default: 0.1</span>
<span class="sd">        same_key_query_proj: bool</span>
<span class="sd">            Whether to use the same projection operator for content key and queries</span>
<span class="sd">            when computing the pre-attention key and query embedding chunks</span>
<span class="sd">            Default: False</span>
<span class="sd">        same_positional_key_query_proj</span>
<span class="sd">            Whether to use the same projection operator for content key and queries</span>
<span class="sd">            when computing the pre-attention key and query embedding chunks</span>
<span class="sd">            Default: False</span>
<span class="sd">        n_coordinates: int</span>
<span class="sd">            The number of positional coordinates</span>
<span class="sd">            For instance, text is 1D so 1 coordinate, images are 2D so 2 coordinates ...</span>
<span class="sd">            Default: 1</span>
<span class="sd">        head_bias: bool</span>
<span class="sd">            Whether to learn a bias term to add to the attention logits</span>
<span class="sd">            This is only useful if you plan to use the attention logits for subsequent</span>
<span class="sd">            operations, since attention weights are unaffected by bias terms.</span>
<span class="sd">        do_pooling: bool</span>
<span class="sd">            Whether to compute the output embedding.</span>
<span class="sd">            If you only plan to use attention logits, you should disable this parameter.</span>
<span class="sd">            Default: True</span>
<span class="sd">        mode: Sequence[RelativeAttentionMode]</span>
<span class="sd">            Whether to compute content to content (c2c), content to position (c2p)</span>
<span class="sd">            or position to content (p2c) attention terms.</span>
<span class="sd">            Setting `mode=('c2c")` disable relative position attention terms: this is</span>
<span class="sd">            the standard attention layer.</span>
<span class="sd">            To get a better intuition about these different types of attention, here is</span>
<span class="sd">            a formulation as fictitious search samples from a word in a (1D) text:</span>
<span class="sd">            â€” content-content : "my content is â€™ultrasoundâ€™ so Iâ€™m looking for other</span>
<span class="sd">              words whose content contains information about temporality"</span>
<span class="sd">            â€” content-position: "my content is â€™ultrasoundâ€™ so Iâ€™m looking for other</span>
<span class="sd">              words that are 3 positions after of me"</span>
<span class="sd">            â€” position-content : "regardless of my content, I will attend to the word</span>
<span class="sd">              one position after from me if it contains information about temporality,</span>
<span class="sd">              two words after me if it contains information about location, etc."</span>
<span class="sd">        n_additional_heads: int</span>
<span class="sd">            The number of additional head logits to compute.</span>
<span class="sd">            Those are not used to compute output embeddings, but may be useful in</span>
<span class="sd">            subsequent operation.</span>
<span class="sd">            Default: 0</span>
<span class="sd">        """</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">query_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">query_size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="k">if</span> <span class="n">key_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key_size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="k">if</span> <span class="n">value_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">value_size</span> <span class="o">=</span> <span class="n">key_size</span>
        <span class="k">if</span> <span class="n">head_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">key_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">key_size</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="n">head_size</span> <span class="o">=</span> <span class="n">key_size</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="n">value_head_size</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">do_pooling</span> <span class="ow">and</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">size</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="n">value_head_size</span> <span class="o">=</span> <span class="n">size</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_coordinates</span> <span class="o">=</span> <span class="n">n_coordinates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span> <span class="o">+</span> <span class="n">n_additional_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_additional_heads</span> <span class="o">=</span> <span class="n">n_additional_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="n">n_query_heads</span> <span class="o">=</span> <span class="n">n_heads</span> <span class="o">+</span> <span class="n">n_additional_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">content_key_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">key_size</span><span class="p">,</span> <span class="n">n_query_heads</span> <span class="o">*</span> <span class="n">head_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">position_embedding</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="n">position_embedding</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"position_embedding"</span><span class="p">,</span> <span class="n">position_embedding</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">same_key_query_proj</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">content_query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">content_key_proj</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">content_query_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
                <span class="n">query_size</span><span class="p">,</span>
                <span class="n">n_query_heads</span> <span class="o">*</span> <span class="n">head_size</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">do_pooling</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">content_value_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
                <span class="n">value_size</span><span class="p">,</span> <span class="n">value_head_size</span> <span class="o">*</span> <span class="n">n_heads</span>
            <span class="p">)</span>

        <span class="n">pos_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_key_proj</span> <span class="o">=</span> <span class="n">GroupedLinear</span><span class="p">(</span>
            <span class="n">pos_size</span> <span class="o">//</span> <span class="n">n_coordinates</span><span class="p">,</span>
            <span class="n">head_size</span> <span class="o">*</span> <span class="n">n_query_heads</span> <span class="o">//</span> <span class="n">n_coordinates</span><span class="p">,</span>
            <span class="n">n_groups</span><span class="o">=</span><span class="n">n_coordinates</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">same_key_query_proj</span> <span class="ow">or</span> <span class="n">same_positional_key_query_proj</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">position_query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_key_proj</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">position_query_proj</span> <span class="o">=</span> <span class="n">GroupedLinear</span><span class="p">(</span>
                <span class="n">pos_size</span> <span class="o">//</span> <span class="n">n_coordinates</span><span class="p">,</span>
                <span class="n">head_size</span> <span class="o">*</span> <span class="n">n_query_heads</span> <span class="o">//</span> <span class="n">n_coordinates</span><span class="p">,</span>
                <span class="n">n_groups</span><span class="o">=</span><span class="n">n_coordinates</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">head_bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_query_heads</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">size</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">content_queries</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span>
        <span class="n">content_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">content_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">relative_positions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">no_position_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">base_attn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">"""</span>
<span class="sd">        Forward pass of the RelativeAttention layer.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        content_queries: torch.FloatTensor</span>
<span class="sd">            The content query embedding to use in the attention computation</span>
<span class="sd">            Shape: `n_samples * n_queries * query_size`</span>
<span class="sd">        content_keys: Optional[torch.FloatTensor]</span>
<span class="sd">            The content key embedding to use in the attention computation.</span>
<span class="sd">            If None, defaults to the `content_queries`</span>
<span class="sd">            Shape: `n_samples * n_keys * query_size`</span>
<span class="sd">        content_values: Optional[torch.FloatTensor]</span>
<span class="sd">            The content values embedding to use in the final pooling computation.</span>
<span class="sd">            If None, pooling won't be performed.</span>
<span class="sd">            Shape: `n_samples * n_keys * query_size`</span>
<span class="sd">        mask: Optional[torch.BoolTensor]</span>
<span class="sd">            The content key embedding to use in the attention computation.</span>
<span class="sd">            If None, defaults to the `content_queries`</span>
<span class="sd">            Shape: either</span>
<span class="sd">            - `n_samples * n_keys`</span>
<span class="sd">            - `n_samples * n_queries * n_keys`</span>
<span class="sd">            - `n_samples * n_queries * n_keys * n_heads`</span>
<span class="sd">        relative_positions: Optional[torch.LongTensor]</span>
<span class="sd">            The relative position of keys relative to queries</span>
<span class="sd">            If None, positional attention terms won't be computed.</span>
<span class="sd">            Shape: `n_samples * n_queries * n_keys * n_coordinates`</span>
<span class="sd">        no_position_mask: Optional[torch.BoolTensor]</span>
<span class="sd">            Key / query pairs for which the position attention terms should</span>
<span class="sd">            be disabled.</span>
<span class="sd">            Shape: `n_samples * n_queries * n_keys`</span>
<span class="sd">        base_attn: Optional[torch.FloatTensor]</span>
<span class="sd">            Attention logits to add to the computed attention logits</span>
<span class="sd">            Shape: `n_samples * n_queries * n_keys * n_heads`</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Union[Tuple[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor]</span>
<span class="sd">            - the output contextualized embeddings (only if content_values is not None</span>
<span class="sd">              and the `do_pooling` attribute is set to True)</span>
<span class="sd">              Shape: n_sample * n_keys * `size`</span>
<span class="sd">            - the attention logits</span>
<span class="sd">              Shape: n_sample * n_keys * n_queries * (n_heads + n_additional_heads)</span>
<span class="sd">        """</span>
        <span class="k">if</span> <span class="n">content_keys</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">content_keys</span> <span class="o">=</span> <span class="n">content_queries</span>

        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">content_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">content_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">content_keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="n">content_queries</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">base_attn</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="n">base_attn</span>
        <span class="p">)</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="mi">0</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">content_queries</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="mi">0</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">content_keys</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="n">content_keys</span> <span class="o">=</span> <span class="n">make_heads</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">content_key_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">content_keys</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span>
            <span class="p">)</span>
            <span class="n">content_queries</span> <span class="o">=</span> <span class="n">make_heads</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">content_query_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">content_queries</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">content_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">content_values</span> <span class="o">=</span> <span class="n">make_heads</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">content_value_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">content_values</span><span class="p">)),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_additional_heads</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">size</span> <span class="o">=</span> <span class="n">content_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">if</span> <span class="s2">"c2c"</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span>
                <span class="n">content_to_content_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                    <span class="s2">"nihd,njhd-&gt;nijh"</span><span class="p">,</span> <span class="n">content_queries</span><span class="p">,</span> <span class="n">content_keys</span>
                <span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
                <span class="n">attn_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">content_to_content_attn</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">relative_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span>
                <span class="s2">"p2c"</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="ow">or</span> <span class="s2">"c2p"</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span>
            <span class="p">):</span>
                <span class="n">position_keys</span> <span class="o">=</span> <span class="n">make_heads</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">position_key_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span><span class="p">)),</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_coordinates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="n">position_queries</span> <span class="o">=</span> <span class="n">make_heads</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">position_query_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span><span class="p">)),</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_coordinates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="n">relative_positions</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">position_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">relative_positions</span>
                <span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">position_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

                <span class="k">if</span> <span class="s2">"c2p"</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span>
                    <span class="n">content_to_position_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                        <span class="s2">"nihxd,zxhd-&gt;nizhx"</span><span class="p">,</span>
                        <span class="n">make_heads</span><span class="p">(</span><span class="n">content_queries</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_coordinates</span><span class="p">),</span>
                        <span class="n">position_keys</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">content_to_position_attn</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span>
                        <span class="n">content_to_position_attn</span><span class="p">,</span>
                        <span class="n">index</span><span class="o">=</span><span class="n">relative_positions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span>
                        <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">no_position_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">content_to_position_attn</span> <span class="o">=</span> <span class="n">content_to_position_attn</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
                            <span class="n">no_position_mask</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="mi">0</span>
                        <span class="p">)</span>
                    <span class="n">attn_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">content_to_position_attn</span><span class="p">)</span>

                <span class="k">if</span> <span class="s2">"p2c"</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span>
                    <span class="n">position_to_content_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                        <span class="s2">"zxhd,njhxd-&gt;nzjhx"</span><span class="p">,</span>
                        <span class="n">position_queries</span><span class="p">,</span>
                        <span class="n">make_heads</span><span class="p">(</span><span class="n">content_keys</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_coordinates</span><span class="p">),</span>
                    <span class="p">)</span>
                    <span class="n">position_to_content_attn</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span>
                        <span class="n">position_to_content_attn</span><span class="p">,</span>
                        <span class="n">index</span><span class="o">=</span><span class="n">relative_positions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span>
                        <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">no_position_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">position_to_content_attn</span> <span class="o">=</span> <span class="n">position_to_content_attn</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
                            <span class="n">no_position_mask</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="mi">0</span>
                        <span class="p">)</span>
                    <span class="n">attn_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">position_to_content_attn</span><span class="p">)</span>

        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">+</span> <span class="nb">sum</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">))</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">"bias"</span><span class="p">):</span>
            <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="k">if</span> <span class="n">content_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>

            <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span>
                <span class="n">attn</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_additional_heads</span> <span class="p">:]</span>
                <span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="n">IMPOSSIBLE</span><span class="p">)</span>
                <span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">pooled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"nijh,njhd-&gt;nihd"</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">content_values</span><span class="p">)</span>
            <span class="n">pooled</span> <span class="o">=</span> <span class="n">pooled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">pooled</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">pooled</span><span class="p">,</span> <span class="n">attn</span>

        <span class="k">return</span> <span class="n">attn</span>
</code></pre></div></td></tr></table></div>
</details>
<div class="doc doc-children">
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="edspdf.layers.relative_attention.RelativeAttention.__init__">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">query_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">key_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">value_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">head_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_embedding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">same_key_query_proj</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">same_positional_key_query_proj</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_coordinates</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">head_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">do_pooling</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="p">(</span><span class="s1">'c2c'</span><span class="p">,</span> <span class="s1">'p2c'</span><span class="p">,</span> <span class="s1">'c2p'</span><span class="p">),</span> <span class="n">n_additional_heads</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<table>
<thead>
<tr>
<th><b>PARAMETER</b></th>
<th><b>DESCRIPTION</b></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>size</code></td>
<td class="doc-param-details">
<p>The size of the output embeddings
Also serves as default if query_size, pos_size, or key_size is None</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code>int</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>n_heads</code></td>
<td class="doc-param-details">
<p>The number of attention heads</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code>int</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>query_size</code></td>
<td class="doc-param-details">
<p>The size of the query embeddings.</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Optional">Optional</span>[int]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>None</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>key_size</code></td>
<td class="doc-param-details">
<p>The size of the key embeddings.</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Optional">Optional</span>[int]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>None</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>value_size</code></td>
<td class="doc-param-details">
<p>The size of the value embeddings</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Optional">Optional</span>[int]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>None</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>head_size</code></td>
<td class="doc-param-details">
<p>The size of each query / key / value chunk used in the attention dot product
Default: <code>key_size / n_heads</code></p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Optional">Optional</span>[int]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>None</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>position_embedding</code></td>
<td class="doc-param-details">
<p>The position embedding used as key and query embeddings</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Optional">Optional</span>[<span title="typing.Union">Union</span>[<span title="torch.FloatTensor">FloatTensor</span>, <span title="torch.nn.Parameter">Parameter</span>]]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>None</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>dropout_p</code></td>
<td class="doc-param-details">
<p>Dropout probability applied on the attention weights
Default: 0.1</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code>float</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>0.1</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>same_key_query_proj</code></td>
<td class="doc-param-details">
<p>Whether to use the same projection operator for content key and queries
when computing the pre-attention key and query embedding chunks
Default: False</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code>bool</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>False</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>same_positional_key_query_proj</code></td>
<td class="doc-param-details">
<p>Whether to use the same projection operator for content key and queries
when computing the pre-attention key and query embedding chunks
Default: False</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code>bool</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>False</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>n_coordinates</code></td>
<td class="doc-param-details">
<p>The number of positional coordinates
For instance, text is 1D so 1 coordinate, images are 2D so 2 coordinates ...
Default: 1</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code>int</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>1</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>head_bias</code></td>
<td class="doc-param-details">
<p>Whether to learn a bias term to add to the attention logits
This is only useful if you plan to use the attention logits for subsequent
operations, since attention weights are unaffected by bias terms.</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code>bool</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>True</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>do_pooling</code></td>
<td class="doc-param-details">
<p>Whether to compute the output embedding.
If you only plan to use attention logits, you should disable this parameter.
Default: True</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code>bool</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>True</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>mode</code></td>
<td class="doc-param-details">
<p>Whether to compute content to content (c2c), content to position (c2p)
or position to content (p2c) attention terms.
Setting <code>mode=('c2c")</code> disable relative position attention terms: this is
the standard attention layer.
To get a better intuition about these different types of attention, here is
a formulation as fictitious search samples from a word in a (1D) text:
â€” content-content : "my content is â€™ultrasoundâ€™ so Iâ€™m looking for other
words whose content contains information about temporality"
â€” content-position: "my content is â€™ultrasoundâ€™ so Iâ€™m looking for other
words that are 3 positions after of me"
â€” position-content : "regardless of my content, I will attend to the word
one position after from me if it contains information about temporality,
two words after me if it contains information about location, etc."</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Sequence">Sequence</span>[<span title="edspdf.layers.relative_attention.RelativeAttentionMode">RelativeAttentionMode</span>]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>('c2c', 'p2c', 'c2p')</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>n_additional_heads</code></td>
<td class="doc-param-details">
<p>The number of additional head logits to compute.
Those are not used to compute output embeddings, but may be useful in
subsequent operation.
Default: 0</p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code>int</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>0</code>
</span>
</p>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>edspdf/layers/relative_attention.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">query_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">key_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">value_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">head_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">position_embedding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">Parameter</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">same_key_query_proj</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">same_positional_key_query_proj</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">n_coordinates</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">head_bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">do_pooling</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">mode</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">RelativeAttentionMode</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="s2">"c2c"</span><span class="p">,</span> <span class="s2">"p2c"</span><span class="p">,</span> <span class="s2">"c2p"</span><span class="p">),</span>
    <span class="n">n_additional_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    size: int</span>
<span class="sd">        The size of the output embeddings</span>
<span class="sd">        Also serves as default if query_size, pos_size, or key_size is None</span>
<span class="sd">    n_heads: int</span>
<span class="sd">        The number of attention heads</span>
<span class="sd">    query_size: Optional[int]</span>
<span class="sd">        The size of the query embeddings.</span>
<span class="sd">    key_size: Optional[int]</span>
<span class="sd">        The size of the key embeddings.</span>
<span class="sd">    value_size: Optional[int]</span>
<span class="sd">        The size of the value embeddings</span>
<span class="sd">    head_size: Optional[int]</span>
<span class="sd">        The size of each query / key / value chunk used in the attention dot product</span>
<span class="sd">        Default: `key_size / n_heads`</span>
<span class="sd">    position_embedding: Optional[torch.FloatTensor]</span>
<span class="sd">        The position embedding used as key and query embeddings</span>
<span class="sd">    dropout_p: float</span>
<span class="sd">        Dropout probability applied on the attention weights</span>
<span class="sd">        Default: 0.1</span>
<span class="sd">    same_key_query_proj: bool</span>
<span class="sd">        Whether to use the same projection operator for content key and queries</span>
<span class="sd">        when computing the pre-attention key and query embedding chunks</span>
<span class="sd">        Default: False</span>
<span class="sd">    same_positional_key_query_proj</span>
<span class="sd">        Whether to use the same projection operator for content key and queries</span>
<span class="sd">        when computing the pre-attention key and query embedding chunks</span>
<span class="sd">        Default: False</span>
<span class="sd">    n_coordinates: int</span>
<span class="sd">        The number of positional coordinates</span>
<span class="sd">        For instance, text is 1D so 1 coordinate, images are 2D so 2 coordinates ...</span>
<span class="sd">        Default: 1</span>
<span class="sd">    head_bias: bool</span>
<span class="sd">        Whether to learn a bias term to add to the attention logits</span>
<span class="sd">        This is only useful if you plan to use the attention logits for subsequent</span>
<span class="sd">        operations, since attention weights are unaffected by bias terms.</span>
<span class="sd">    do_pooling: bool</span>
<span class="sd">        Whether to compute the output embedding.</span>
<span class="sd">        If you only plan to use attention logits, you should disable this parameter.</span>
<span class="sd">        Default: True</span>
<span class="sd">    mode: Sequence[RelativeAttentionMode]</span>
<span class="sd">        Whether to compute content to content (c2c), content to position (c2p)</span>
<span class="sd">        or position to content (p2c) attention terms.</span>
<span class="sd">        Setting `mode=('c2c")` disable relative position attention terms: this is</span>
<span class="sd">        the standard attention layer.</span>
<span class="sd">        To get a better intuition about these different types of attention, here is</span>
<span class="sd">        a formulation as fictitious search samples from a word in a (1D) text:</span>
<span class="sd">        â€” content-content : "my content is â€™ultrasoundâ€™ so Iâ€™m looking for other</span>
<span class="sd">          words whose content contains information about temporality"</span>
<span class="sd">        â€” content-position: "my content is â€™ultrasoundâ€™ so Iâ€™m looking for other</span>
<span class="sd">          words that are 3 positions after of me"</span>
<span class="sd">        â€” position-content : "regardless of my content, I will attend to the word</span>
<span class="sd">          one position after from me if it contains information about temporality,</span>
<span class="sd">          two words after me if it contains information about location, etc."</span>
<span class="sd">    n_additional_heads: int</span>
<span class="sd">        The number of additional head logits to compute.</span>
<span class="sd">        Those are not used to compute output embeddings, but may be useful in</span>
<span class="sd">        subsequent operation.</span>
<span class="sd">        Default: 0</span>
<span class="sd">    """</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">query_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">query_size</span> <span class="o">=</span> <span class="n">size</span>
    <span class="k">if</span> <span class="n">key_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">key_size</span> <span class="o">=</span> <span class="n">size</span>
    <span class="k">if</span> <span class="n">value_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">value_size</span> <span class="o">=</span> <span class="n">key_size</span>
    <span class="k">if</span> <span class="n">head_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">key_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">key_size</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">head_size</span> <span class="o">=</span> <span class="n">key_size</span> <span class="o">//</span> <span class="n">n_heads</span>
    <span class="n">value_head_size</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">do_pooling</span> <span class="ow">and</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">size</span> <span class="o">%</span> <span class="n">n_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">value_head_size</span> <span class="o">=</span> <span class="n">size</span> <span class="o">//</span> <span class="n">n_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_coordinates</span> <span class="o">=</span> <span class="n">n_coordinates</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span> <span class="o">+</span> <span class="n">n_additional_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_additional_heads</span> <span class="o">=</span> <span class="n">n_additional_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
    <span class="n">n_query_heads</span> <span class="o">=</span> <span class="n">n_heads</span> <span class="o">+</span> <span class="n">n_additional_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">content_key_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">key_size</span><span class="p">,</span> <span class="n">n_query_heads</span> <span class="o">*</span> <span class="n">head_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">position_embedding</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="n">position_embedding</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">"position_embedding"</span><span class="p">,</span> <span class="n">position_embedding</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">same_key_query_proj</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">content_query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">content_key_proj</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">content_query_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">query_size</span><span class="p">,</span>
            <span class="n">n_query_heads</span> <span class="o">*</span> <span class="n">head_size</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">do_pooling</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">content_value_proj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">value_size</span><span class="p">,</span> <span class="n">value_head_size</span> <span class="o">*</span> <span class="n">n_heads</span>
        <span class="p">)</span>

    <span class="n">pos_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">position_key_proj</span> <span class="o">=</span> <span class="n">GroupedLinear</span><span class="p">(</span>
        <span class="n">pos_size</span> <span class="o">//</span> <span class="n">n_coordinates</span><span class="p">,</span>
        <span class="n">head_size</span> <span class="o">*</span> <span class="n">n_query_heads</span> <span class="o">//</span> <span class="n">n_coordinates</span><span class="p">,</span>
        <span class="n">n_groups</span><span class="o">=</span><span class="n">n_coordinates</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">same_key_query_proj</span> <span class="ow">or</span> <span class="n">same_positional_key_query_proj</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_query_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_key_proj</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_query_proj</span> <span class="o">=</span> <span class="n">GroupedLinear</span><span class="p">(</span>
            <span class="n">pos_size</span> <span class="o">//</span> <span class="n">n_coordinates</span><span class="p">,</span>
            <span class="n">head_size</span> <span class="o">*</span> <span class="n">n_query_heads</span> <span class="o">//</span> <span class="n">n_coordinates</span><span class="p">,</span>
            <span class="n">n_groups</span><span class="o">=</span><span class="n">n_coordinates</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout_p</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">head_bias</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_query_heads</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">size</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
<div class="doc doc-object doc-function">
<h3 class="doc doc-heading" id="edspdf.layers.relative_attention.RelativeAttention.forward">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="n">content_queries</span><span class="p">,</span> <span class="n">content_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">content_values</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">relative_positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">no_position_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">base_attn</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>
</h3>
<div class="doc doc-contents">
<p>Forward pass of the RelativeAttention layer.</p>
<table>
<thead>
<tr>
<th><b>PARAMETER</b></th>
<th><b>DESCRIPTION</b></th>
</tr>
</thead>
<tbody>
<tr>
<td><code>content_queries</code></td>
<td class="doc-param-details">
<p>The content query embedding to use in the attention computation
Shape: <code>n_samples * n_queries * query_size</code></p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code>torch.<span title="torch.FloatTensor">FloatTensor</span></code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>content_keys</code></td>
<td class="doc-param-details">
<p>The content key embedding to use in the attention computation.
If None, defaults to the <code>content_queries</code>
Shape: <code>n_samples * n_keys * query_size</code></p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Optional">Optional</span>[torch.<span title="torch.FloatTensor">FloatTensor</span>]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>None</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>content_values</code></td>
<td class="doc-param-details">
<p>The content values embedding to use in the final pooling computation.
If None, pooling won't be performed.
Shape: <code>n_samples * n_keys * query_size</code></p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Optional">Optional</span>[torch.<span title="torch.FloatTensor">FloatTensor</span>]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>None</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>mask</code></td>
<td class="doc-param-details">
<p>The content key embedding to use in the attention computation.
If None, defaults to the <code>content_queries</code>
Shape: either
- <code>n_samples * n_keys</code>
- <code>n_samples * n_queries * n_keys</code>
- <code>n_samples * n_queries * n_keys * n_heads</code></p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Optional">Optional</span>[torch.<span title="torch.BoolTensor">BoolTensor</span>]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>None</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>relative_positions</code></td>
<td class="doc-param-details">
<p>The relative position of keys relative to queries
If None, positional attention terms won't be computed.
Shape: <code>n_samples * n_queries * n_keys * n_coordinates</code></p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Optional">Optional</span>[torch.<span title="torch.LongTensor">LongTensor</span>]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>None</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>no_position_mask</code></td>
<td class="doc-param-details">
<p>Key / query pairs for which the position attention terms should
be disabled.
Shape: <code>n_samples * n_queries * n_keys</code></p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Optional">Optional</span>[torch.<span title="torch.BoolTensor">BoolTensor</span>]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>None</code>
</span>
</p>
</td>
</tr>
<tr>
<td><code>base_attn</code></td>
<td class="doc-param-details">
<p>Attention logits to add to the computed attention logits
Shape: <code>n_samples * n_queries * n_keys * n_heads</code></p>
<p>
<span class="doc-param-annotation">
<b>TYPE:</b>
<code><span title="typing.Optional">Optional</span>[torch.<span title="torch.FloatTensor">FloatTensor</span>]</code>
</span>
<span class="doc-param-default">
<b>DEFAULT:</b>
<code>None</code>
</span>
</p>
</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th><b>RETURNS</b></th>
<th><b>DESCRIPTION</b></th>
</tr>
</thead>
<tbody>
<tr>
<td>
<span class="doc-returns-annotation">
<code><span title="typing.Union">Union</span>[<span title="typing.Tuple">Tuple</span>[torch.<span title="torch.FloatTensor">FloatTensor</span>, torch.<span title="torch.FloatTensor">FloatTensor</span>], torch.<span title="torch.FloatTensor">FloatTensor</span>]</code>
</span>
</td>
<td class="doc-returns-details">
<ul>
<li>the output contextualized embeddings (only if content_values is not None
and the <code>do_pooling</code> attribute is set to True)
Shape: n_sample * n_keys * <code>size</code></li>
<li>the attention logits
Shape: n_sample * n_keys * n_queries * (n_heads + n_additional_heads)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<details class="quote">
<summary>Source code in <code>edspdf/layers/relative_attention.py</code></summary>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">content_queries</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span>
    <span class="n">content_keys</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">content_values</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">relative_positions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">no_position_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">BoolTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">base_attn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">"""</span>
<span class="sd">    Forward pass of the RelativeAttention layer.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    content_queries: torch.FloatTensor</span>
<span class="sd">        The content query embedding to use in the attention computation</span>
<span class="sd">        Shape: `n_samples * n_queries * query_size`</span>
<span class="sd">    content_keys: Optional[torch.FloatTensor]</span>
<span class="sd">        The content key embedding to use in the attention computation.</span>
<span class="sd">        If None, defaults to the `content_queries`</span>
<span class="sd">        Shape: `n_samples * n_keys * query_size`</span>
<span class="sd">    content_values: Optional[torch.FloatTensor]</span>
<span class="sd">        The content values embedding to use in the final pooling computation.</span>
<span class="sd">        If None, pooling won't be performed.</span>
<span class="sd">        Shape: `n_samples * n_keys * query_size`</span>
<span class="sd">    mask: Optional[torch.BoolTensor]</span>
<span class="sd">        The content key embedding to use in the attention computation.</span>
<span class="sd">        If None, defaults to the `content_queries`</span>
<span class="sd">        Shape: either</span>
<span class="sd">        - `n_samples * n_keys`</span>
<span class="sd">        - `n_samples * n_queries * n_keys`</span>
<span class="sd">        - `n_samples * n_queries * n_keys * n_heads`</span>
<span class="sd">    relative_positions: Optional[torch.LongTensor]</span>
<span class="sd">        The relative position of keys relative to queries</span>
<span class="sd">        If None, positional attention terms won't be computed.</span>
<span class="sd">        Shape: `n_samples * n_queries * n_keys * n_coordinates`</span>
<span class="sd">    no_position_mask: Optional[torch.BoolTensor]</span>
<span class="sd">        Key / query pairs for which the position attention terms should</span>
<span class="sd">        be disabled.</span>
<span class="sd">        Shape: `n_samples * n_queries * n_keys`</span>
<span class="sd">    base_attn: Optional[torch.FloatTensor]</span>
<span class="sd">        Attention logits to add to the computed attention logits</span>
<span class="sd">        Shape: `n_samples * n_queries * n_keys * n_heads`</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Union[Tuple[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor]</span>
<span class="sd">        - the output contextualized embeddings (only if content_values is not None</span>
<span class="sd">          and the `do_pooling` attribute is set to True)</span>
<span class="sd">          Shape: n_sample * n_keys * `size`</span>
<span class="sd">        - the attention logits</span>
<span class="sd">          Shape: n_sample * n_keys * n_queries * (n_heads + n_additional_heads)</span>
<span class="sd">    """</span>
    <span class="k">if</span> <span class="n">content_keys</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">content_keys</span> <span class="o">=</span> <span class="n">content_queries</span>

    <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">content_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">content_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">content_keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">content_queries</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">base_attn</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">else</span> <span class="n">base_attn</span>
    <span class="p">)</span>

    <span class="n">attn_weights</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="mi">0</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">content_queries</span><span class="o">.</span><span class="n">shape</span> <span class="ow">and</span> <span class="mi">0</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">content_keys</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="n">content_keys</span> <span class="o">=</span> <span class="n">make_heads</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">content_key_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">content_keys</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span>
        <span class="p">)</span>
        <span class="n">content_queries</span> <span class="o">=</span> <span class="n">make_heads</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">content_query_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">content_queries</span><span class="p">)),</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">content_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">content_values</span> <span class="o">=</span> <span class="n">make_heads</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">content_value_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">content_values</span><span class="p">)),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_additional_heads</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">size</span> <span class="o">=</span> <span class="n">content_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="s2">"c2c"</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span>
            <span class="n">content_to_content_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                <span class="s2">"nihd,njhd-&gt;nijh"</span><span class="p">,</span> <span class="n">content_queries</span><span class="p">,</span> <span class="n">content_keys</span>
            <span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
            <span class="n">attn_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">content_to_content_attn</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">relative_positions</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="s2">"p2c"</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="ow">or</span> <span class="s2">"c2p"</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span>
        <span class="p">):</span>
            <span class="n">position_keys</span> <span class="o">=</span> <span class="n">make_heads</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">position_key_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span><span class="p">)),</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_coordinates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">position_queries</span> <span class="o">=</span> <span class="n">make_heads</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">position_query_proj</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span><span class="p">)),</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_coordinates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">relative_positions</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">position_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">relative_positions</span>
            <span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">position_queries</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="s2">"c2p"</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span>
                <span class="n">content_to_position_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                    <span class="s2">"nihxd,zxhd-&gt;nizhx"</span><span class="p">,</span>
                    <span class="n">make_heads</span><span class="p">(</span><span class="n">content_queries</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_coordinates</span><span class="p">),</span>
                    <span class="n">position_keys</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">content_to_position_attn</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span>
                    <span class="n">content_to_position_attn</span><span class="p">,</span>
                    <span class="n">index</span><span class="o">=</span><span class="n">relative_positions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span>
                    <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">no_position_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">content_to_position_attn</span> <span class="o">=</span> <span class="n">content_to_position_attn</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
                        <span class="n">no_position_mask</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="mi">0</span>
                    <span class="p">)</span>
                <span class="n">attn_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">content_to_position_attn</span><span class="p">)</span>

            <span class="k">if</span> <span class="s2">"p2c"</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span><span class="p">:</span>
                <span class="n">position_to_content_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
                    <span class="s2">"zxhd,njhxd-&gt;nzjhx"</span><span class="p">,</span>
                    <span class="n">position_queries</span><span class="p">,</span>
                    <span class="n">make_heads</span><span class="p">(</span><span class="n">content_keys</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_coordinates</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="n">position_to_content_attn</span> <span class="o">=</span> <span class="n">gather</span><span class="p">(</span>
                    <span class="n">position_to_content_attn</span><span class="p">,</span>
                    <span class="n">index</span><span class="o">=</span><span class="n">relative_positions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span>
                    <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">no_position_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">position_to_content_attn</span> <span class="o">=</span> <span class="n">position_to_content_attn</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
                        <span class="n">no_position_mask</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="mi">0</span>
                    <span class="p">)</span>
                <span class="n">attn_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">position_to_content_attn</span><span class="p">)</span>

    <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">+</span> <span class="nb">sum</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">"bias"</span><span class="p">):</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
    <span class="k">if</span> <span class="n">content_values</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">]</span>

        <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span>
            <span class="n">attn</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_additional_heads</span> <span class="p">:]</span>
            <span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="n">IMPOSSIBLE</span><span class="p">)</span>
            <span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">pooled</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">"nijh,njhd-&gt;nihd"</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">content_values</span><span class="p">)</span>
        <span class="n">pooled</span> <span class="o">=</span> <span class="n">pooled</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">*</span><span class="n">pooled</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pooled</span><span class="p">,</span> <span class="n">attn</span>

    <span class="k">return</span> <span class="n">attn</span>
</code></pre></div></td></tr></table></div>
</details>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</article>
</div>
</div>
</main>
<footer class="md-footer">
<nav aria-label="Footer" class="md-footer__inner md-grid">
<a aria-label="Previous: cnn_pooler" class="md-footer__link md-footer__link--prev" href="../cnn_pooler/" rel="prev">
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg>
</div>
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                Previous
              </span>
              cnn_pooler
            </div>
</div>
</a>
<a aria-label="Next: sinusoidal_embedding" class="md-footer__link md-footer__link--next" href="../sinusoidal_embedding/" rel="next">
<div class="md-footer__title">
<div class="md-ellipsis">
<span class="md-footer__direction">
                Next
              </span>
              sinusoidal_embedding
            </div>
</div>
<div class="md-footer__button md-icon">
<svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"></path></svg>
</div>
</a>
</nav>
<div class="md-footer-meta md-typeset">
<div class="md-footer-meta__inner md-grid">
<div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank">
      Material for MkDocs
    </a>
</div>
</div>
</div>
</footer>
</div>
<div class="md-dialog" data-md-component="dialog">
<div class="md-dialog__inner md-typeset"></div>
</div>
<script id="__config" type="application/json">{"base": "../../..", "features": ["navigation.tracking", "navigation.instant", "navigation.indexes", "content.code.annotate"], "search": "../../../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "version": {"provider": "mike"}}</script>
<script src="../../../assets/javascripts/bundle.078830c0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/vega@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-lite@5"></script>
<script src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>
<script src="../../../assets/termynal/termynal.js"></script>
<script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "width": "100%", "height": "auto", "zoomable": true, "draggable": true, "openEffect": "none", "closeEffect": "none"});})</script></body>
</html>