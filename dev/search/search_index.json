{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview EDS-PDF provides modular framework to extract text information from PDF documents. You can use it out-of-the-box, or extend it to fit your use-case. Getting started Installation Install the library with pip: $ pip install edspdf ---> 100% color:green Installation successful Extracting text Let's build a simple PDF extractor that uses a rule-based classifier, using the following configuration: config.cfg [pipeline] components = [ \"extractor\" , \"classifier\" , \"aggregator\" ] components_config = $ { components } [components.extractor] @ factory = \"pdfminer-extractor\" [components.classifier] @ factory = \"mask-classifier\" x0 = 0.2 x1 = 0.9 y0 = 0.3 y1 = 0.6 threshold = 0.1 [components.aggregator] @ factory = \"simple-aggregator\" The PDF Pipeline can be instantiated and applied (for instance with this PDF ): import edspdf from pathlib import Path model = edspdf . load ( \"config.cfg\" ) # (1) # Get a PDF pdf = Path ( \"letter.pdf\" ) . read_bytes () texts = model ( pdf ) texts [ \"body\" ] # Out: Cher Pr ABC, Cher DEF,\\n... The Pipeline instance is loaded from the configuration directly. See the rule-based recipe for a step-by-step explanation of what is happening. Citation If you use EDS-PDF, please cite us as below. @software { edspdf , author = {Dura, Basile and Wajsburt, Perceval and Calliger, Alice and G\u00e9rardin, Christel and Bey, Romain} , doi = {10.5281/zenodo.6902977} , license = {BSD-3-Clause} , title = {{EDS-PDF: Smart text extraction from PDF documents}} , url = {https://github.com/aphp/edspdf} } Acknowledgement We would like to thank Assistance Publique \u2013 H\u00f4pitaux de Paris and AP-HP Foundation for funding this project.","title":"Overview"},{"location":"#overview","text":"EDS-PDF provides modular framework to extract text information from PDF documents. You can use it out-of-the-box, or extend it to fit your use-case.","title":"Overview"},{"location":"#getting-started","text":"","title":"Getting started"},{"location":"#installation","text":"Install the library with pip: $ pip install edspdf ---> 100% color:green Installation successful","title":"Installation"},{"location":"#extracting-text","text":"Let's build a simple PDF extractor that uses a rule-based classifier, using the following configuration: config.cfg [pipeline] components = [ \"extractor\" , \"classifier\" , \"aggregator\" ] components_config = $ { components } [components.extractor] @ factory = \"pdfminer-extractor\" [components.classifier] @ factory = \"mask-classifier\" x0 = 0.2 x1 = 0.9 y0 = 0.3 y1 = 0.6 threshold = 0.1 [components.aggregator] @ factory = \"simple-aggregator\" The PDF Pipeline can be instantiated and applied (for instance with this PDF ): import edspdf from pathlib import Path model = edspdf . load ( \"config.cfg\" ) # (1) # Get a PDF pdf = Path ( \"letter.pdf\" ) . read_bytes () texts = model ( pdf ) texts [ \"body\" ] # Out: Cher Pr ABC, Cher DEF,\\n... The Pipeline instance is loaded from the configuration directly. See the rule-based recipe for a step-by-step explanation of what is happening.","title":"Extracting text"},{"location":"#citation","text":"If you use EDS-PDF, please cite us as below. @software { edspdf , author = {Dura, Basile and Wajsburt, Perceval and Calliger, Alice and G\u00e9rardin, Christel and Bey, Romain} , doi = {10.5281/zenodo.6902977} , license = {BSD-3-Clause} , title = {{EDS-PDF: Smart text extraction from PDF documents}} , url = {https://github.com/aphp/edspdf} }","title":"Citation"},{"location":"#acknowledgement","text":"We would like to thank Assistance Publique \u2013 H\u00f4pitaux de Paris and AP-HP Foundation for funding this project.","title":"Acknowledgement"},{"location":"alternatives/","text":"Alternatives & Comparison EDS-PDF was developed to propose a more modular and extendable approach to PDF extraction than PDFBox , the legacy implementation at APHP's clinical data warehouse. EDS-PDF takes inspiration from Explosion's spaCy pipelining system and closely follows its API. Therefore, the core object within EDS-PDF is the Pipeline, which organises the processing of PDF documents into multiple components. However, unlike spaCy, the library is built around a single deep learning framework, pytorch, which makes model development easier. Similar to spaCy and thinc , EDS-PDF also relies on catalogue entry points and a custom powerful custom configuration system. This allows complex pipeline models to be built and trained using either the library API or configuration files.","title":"Alternatives & Comparison"},{"location":"alternatives/#alternatives-comparison","text":"EDS-PDF was developed to propose a more modular and extendable approach to PDF extraction than PDFBox , the legacy implementation at APHP's clinical data warehouse. EDS-PDF takes inspiration from Explosion's spaCy pipelining system and closely follows its API. Therefore, the core object within EDS-PDF is the Pipeline, which organises the processing of PDF documents into multiple components. However, unlike spaCy, the library is built around a single deep learning framework, pytorch, which makes model development easier. Similar to spaCy and thinc , EDS-PDF also relies on catalogue entry points and a custom powerful custom configuration system. This allows complex pipeline models to be built and trained using either the library API or configuration files.","title":"Alternatives &amp; Comparison"},{"location":"changelog/","text":"Changelog v0.6.3 - 2023-01-23 Fixed Allow corrupted PDF to not raise an error by default (they are treated as empty PDFs) Fix classification and aggregation for empty PDFs v0.6.2 - 2022-12-07 Cast bytes-like extractor inputs as bytes v0.6.1 - 2022-12-07 Performance and cuda related fixes. v0.6.0 - 2022-12-05 Many, many changes: - added torch as the main deep learning framework instead of spaCy and thinc - added poppler and mupdf as alternatives to pdfminer - new pipeline / config / registry system to facilitate consistency between training and inference - standardization of the exchange format between components with dataclass models (attrs more specifically) instead of pandas dataframes v0.5.3 - 2022-08-31 Added Add label mapping parameter to aggregators (to merge different types of blocks such as title and body ) Improved line aggregation formula v0.5.2 - 2022-08-30 Fixed Fix aggregation for empty documents v0.5.1 - 2022-07-26 Changed Drop the pdf2image dependency, replacing it with pypdfium2 (easier installation) v0.5.0 - 2022-07-25 Changed Major refactoring of the library. Moved from concepts ( aggregation ) to plural names ( aggregators ). v0.4.3 - 2022-07-20 Fixed Multi page boxes alignment v0.4.2 - 2022-07-06 Added package-resource.v1 in the misc registry v0.4.1 - 2022-06-14 Fixed Remove importlib.metadata dependency, which led to issues with Python 3.7 v0.4.0 - 2022-06-14 Added Python 3.7 support, by relaxing dependency constraints Support for package-resource pipeline for sklearn-pipeline.v1 v0.3.2 - 2022-06-03 Added compare_results in visualisation v0.3.1 - 2022-06-02 Fixed Rescale transform now keeps origin on top-left corner v0.3.0 - 2022-06-01 Added Styles management within the extractor styled.v1 aggregator, to handle styles rescale.v1 transform, to go back to the original height and width Changed Styles and text extraction is handled by the extractor directly The PDFMiner line object is not carried around any more Removed Outdated params entry in the EDS-PDF registry. v0.2.2 - 2022-05-12 Changed Fixed merge_lines bug when lines were empty Modified the demo consequently v0.2.1 - 2022-05-09 Changed The extractor always returns a pandas DataFrame, be it empty. It enhances robustness and stability. v0.2.0 - 2022-05-09 Added aggregation submodule to handle the specifics of aggregating text blocs Base classes for better-defined modules Uniformise the columns to labels Add arbitrary contextual information Removed typer legacy dependency models submodule, which handled the configurations for Spark distribution (deferred to another package) specific orbis context, which was APHP-specific v0.1.0 - 2022-05-06 Inception ! Features spaCy-like configuration system Available classifiers : dummy.v1 , that classifies everything to body mask.v1 , for simple rule-based classification sklearn.v1 , that uses a Scikit-Learn pipeline random.v1 , to better sow chaos Merge different blocs together for easier visualisation Streamlit demo with visualisation","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#v063-2023-01-23","text":"","title":"v0.6.3 - 2023-01-23"},{"location":"changelog/#fixed","text":"Allow corrupted PDF to not raise an error by default (they are treated as empty PDFs) Fix classification and aggregation for empty PDFs","title":"Fixed"},{"location":"changelog/#v062-2022-12-07","text":"Cast bytes-like extractor inputs as bytes","title":"v0.6.2 - 2022-12-07"},{"location":"changelog/#v061-2022-12-07","text":"Performance and cuda related fixes.","title":"v0.6.1 - 2022-12-07"},{"location":"changelog/#v060-2022-12-05","text":"Many, many changes: - added torch as the main deep learning framework instead of spaCy and thinc - added poppler and mupdf as alternatives to pdfminer - new pipeline / config / registry system to facilitate consistency between training and inference - standardization of the exchange format between components with dataclass models (attrs more specifically) instead of pandas dataframes","title":"v0.6.0 - 2022-12-05"},{"location":"changelog/#v053-2022-08-31","text":"","title":"v0.5.3 - 2022-08-31"},{"location":"changelog/#added","text":"Add label mapping parameter to aggregators (to merge different types of blocks such as title and body ) Improved line aggregation formula","title":"Added"},{"location":"changelog/#v052-2022-08-30","text":"","title":"v0.5.2 - 2022-08-30"},{"location":"changelog/#fixed_1","text":"Fix aggregation for empty documents","title":"Fixed"},{"location":"changelog/#v051-2022-07-26","text":"","title":"v0.5.1 - 2022-07-26"},{"location":"changelog/#changed","text":"Drop the pdf2image dependency, replacing it with pypdfium2 (easier installation)","title":"Changed"},{"location":"changelog/#v050-2022-07-25","text":"","title":"v0.5.0 - 2022-07-25"},{"location":"changelog/#changed_1","text":"Major refactoring of the library. Moved from concepts ( aggregation ) to plural names ( aggregators ).","title":"Changed"},{"location":"changelog/#v043-2022-07-20","text":"","title":"v0.4.3 - 2022-07-20"},{"location":"changelog/#fixed_2","text":"Multi page boxes alignment","title":"Fixed"},{"location":"changelog/#v042-2022-07-06","text":"","title":"v0.4.2 - 2022-07-06"},{"location":"changelog/#added_1","text":"package-resource.v1 in the misc registry","title":"Added"},{"location":"changelog/#v041-2022-06-14","text":"","title":"v0.4.1 - 2022-06-14"},{"location":"changelog/#fixed_3","text":"Remove importlib.metadata dependency, which led to issues with Python 3.7","title":"Fixed"},{"location":"changelog/#v040-2022-06-14","text":"","title":"v0.4.0 - 2022-06-14"},{"location":"changelog/#added_2","text":"Python 3.7 support, by relaxing dependency constraints Support for package-resource pipeline for sklearn-pipeline.v1","title":"Added"},{"location":"changelog/#v032-2022-06-03","text":"","title":"v0.3.2 - 2022-06-03"},{"location":"changelog/#added_3","text":"compare_results in visualisation","title":"Added"},{"location":"changelog/#v031-2022-06-02","text":"","title":"v0.3.1 - 2022-06-02"},{"location":"changelog/#fixed_4","text":"Rescale transform now keeps origin on top-left corner","title":"Fixed"},{"location":"changelog/#v030-2022-06-01","text":"","title":"v0.3.0 - 2022-06-01"},{"location":"changelog/#added_4","text":"Styles management within the extractor styled.v1 aggregator, to handle styles rescale.v1 transform, to go back to the original height and width","title":"Added"},{"location":"changelog/#changed_2","text":"Styles and text extraction is handled by the extractor directly The PDFMiner line object is not carried around any more","title":"Changed"},{"location":"changelog/#removed","text":"Outdated params entry in the EDS-PDF registry.","title":"Removed"},{"location":"changelog/#v022-2022-05-12","text":"","title":"v0.2.2 - 2022-05-12"},{"location":"changelog/#changed_3","text":"Fixed merge_lines bug when lines were empty Modified the demo consequently","title":"Changed"},{"location":"changelog/#v021-2022-05-09","text":"","title":"v0.2.1 - 2022-05-09"},{"location":"changelog/#changed_4","text":"The extractor always returns a pandas DataFrame, be it empty. It enhances robustness and stability.","title":"Changed"},{"location":"changelog/#v020-2022-05-09","text":"","title":"v0.2.0 - 2022-05-09"},{"location":"changelog/#added_5","text":"aggregation submodule to handle the specifics of aggregating text blocs Base classes for better-defined modules Uniformise the columns to labels Add arbitrary contextual information","title":"Added"},{"location":"changelog/#removed_1","text":"typer legacy dependency models submodule, which handled the configurations for Spark distribution (deferred to another package) specific orbis context, which was APHP-specific","title":"Removed"},{"location":"changelog/#v010-2022-05-06","text":"Inception !","title":"v0.1.0 - 2022-05-06"},{"location":"changelog/#features","text":"spaCy-like configuration system Available classifiers : dummy.v1 , that classifies everything to body mask.v1 , for simple rule-based classification sklearn.v1 , that uses a Scikit-Learn pipeline random.v1 , to better sow chaos Merge different blocs together for easier visualisation Streamlit demo with visualisation","title":"Features"},{"location":"concepts/","text":"Concepts Trainable components Anatomy of a trainable component In the trainable components of EDS-PDF, preprocessing and postprocessing code is collocated with the forward method. This is achieved by splitting the class of a trainable component into five methods: preprocess : converts a doc into features that will be consumed by the forward method, e.g. building arrays of features, encoding words into indices, etc collate : concatenates the preprocessed features of multiple documents into pytorch tensors forward : applies transformations over the collated features to compute new embeddings, probabilities, etc postprocess : use these predictions to annotate the document, for instance converting label probabilities into label attributes on the document lines initialize : completes the attributes of a component by looking at some docs : useful to build vocabularies or detect labels of a classification task Here is an example of a trainable component: from typing import Any , Dict , Iterable , Sequence import torch from tqdm import tqdm from edspdf import Module , TrainableComponent , registry from edspdf.models import PDFDoc @registry . factory . register ( \"my-component\" ) class MyComponent ( TrainableComponent ): def __init__ ( self , # A subcomponent embedding : Module , ): super () . __init__ () self . embedding : Module = embedding def initialize ( self , gold_data : Iterable [ PDFDoc ]): # Initialize the component with the gold documents with self . label_vocabulary . initialization (): for doc in tqdm ( gold_data , desc = \"Initializing classifier\" ): # Do something like learning a vocabulary over the initialization # documents ... # And initialize the subcomponent self . embedding . initialize ( gold_data ) # Initialize any layer that might be missing from the module self . classifier = torch . nn . Linear ( ... ) def preprocess ( self , doc : PDFDoc , supervision : bool = False ) -> Dict [ str , Any ]: # Preprocess the doc to extract features required to run the embedding # subcomponent, and this component return { \"embedding\" : self . embedding . preprocess ( doc , supervision = supervision ), \"my-feature\" : ... ( doc ), } def collate ( self , batch , device : torch . device ) -> Dict : # Collate the features of the \"embedding\" subcomponent # and the features of this component as well return { \"embedding\" : self . embedding . collate ( batch [ \"embedding\" ], device ), \"my-feature\" : torch . as_tensor ( batch [ \"my-feature\" ], device = device ), } def forward ( self , batch : Dict , supervision = False ) -> Dict : # Call the embedding subcomponent embeds = self . embedding ( batch [ \"embedding\" ]) # Do something with the embedding tensors output = ... ( embeds ) return output def postprocess ( self , docs : Sequence [ PDFDoc ], output : Dict ) -> Sequence [ PDFDoc ]: # Annotate the docs with the outputs of the forward method ... return docs Nesting trainable components Like pytorch modules, you can compose trainable components together to build complex architectures. For instance, a deep classifier component can call an embedding component. Nesting components allows switching parts of the neural networks to test various architectures and keeping the modelling logic modular. Sharing subcomponents Sharing parts of a neural network while training components on different tasks can be an effective way to improve the network efficiency. For instance, it is common to share an embedding layer between multiple tasks that require embedding the same inputs. In EDS-PDF, sharing a subcomponent is simply done by sharing the object between the multiple components. You can either refer to an existing subcomponent when configuring a new component in Python, or use the interpolation mechanism of our configuration system. API-based Configuration-based pipeline . add_pipe ( \"my-component-1\" , name = \"first\" , config = { \"embedding\" : { \"@factory\" : \"box-embedding\" , ... } }) pipeline . add_pipe ( \"my-component-2\" , name = \"second\" , config = { \"embedding\" : pipeline . components . first . embedding , }) [components.first] @factory = \"my-component-1\" [components.first.embedding] @factory = \"box-embedding\" ... [components.second] @factory = \"my-component-2\" embedding = ${components.first.embedding} To avoid recomputing the preprocess / forward and collate in the multiple components that use it, we rely on a transparent cache system. Cache During the training loop, the cache must be emptied at every step to release CPU and GPU memory occupied by the cached methods outputs. This is done by calling reset_cache method on the pipeline.","title":"Concepts"},{"location":"concepts/#concepts","text":"","title":"Concepts"},{"location":"concepts/#trainable-components","text":"","title":"Trainable components"},{"location":"concepts/#anatomy-of-a-trainable-component","text":"In the trainable components of EDS-PDF, preprocessing and postprocessing code is collocated with the forward method. This is achieved by splitting the class of a trainable component into five methods: preprocess : converts a doc into features that will be consumed by the forward method, e.g. building arrays of features, encoding words into indices, etc collate : concatenates the preprocessed features of multiple documents into pytorch tensors forward : applies transformations over the collated features to compute new embeddings, probabilities, etc postprocess : use these predictions to annotate the document, for instance converting label probabilities into label attributes on the document lines initialize : completes the attributes of a component by looking at some docs : useful to build vocabularies or detect labels of a classification task Here is an example of a trainable component: from typing import Any , Dict , Iterable , Sequence import torch from tqdm import tqdm from edspdf import Module , TrainableComponent , registry from edspdf.models import PDFDoc @registry . factory . register ( \"my-component\" ) class MyComponent ( TrainableComponent ): def __init__ ( self , # A subcomponent embedding : Module , ): super () . __init__ () self . embedding : Module = embedding def initialize ( self , gold_data : Iterable [ PDFDoc ]): # Initialize the component with the gold documents with self . label_vocabulary . initialization (): for doc in tqdm ( gold_data , desc = \"Initializing classifier\" ): # Do something like learning a vocabulary over the initialization # documents ... # And initialize the subcomponent self . embedding . initialize ( gold_data ) # Initialize any layer that might be missing from the module self . classifier = torch . nn . Linear ( ... ) def preprocess ( self , doc : PDFDoc , supervision : bool = False ) -> Dict [ str , Any ]: # Preprocess the doc to extract features required to run the embedding # subcomponent, and this component return { \"embedding\" : self . embedding . preprocess ( doc , supervision = supervision ), \"my-feature\" : ... ( doc ), } def collate ( self , batch , device : torch . device ) -> Dict : # Collate the features of the \"embedding\" subcomponent # and the features of this component as well return { \"embedding\" : self . embedding . collate ( batch [ \"embedding\" ], device ), \"my-feature\" : torch . as_tensor ( batch [ \"my-feature\" ], device = device ), } def forward ( self , batch : Dict , supervision = False ) -> Dict : # Call the embedding subcomponent embeds = self . embedding ( batch [ \"embedding\" ]) # Do something with the embedding tensors output = ... ( embeds ) return output def postprocess ( self , docs : Sequence [ PDFDoc ], output : Dict ) -> Sequence [ PDFDoc ]: # Annotate the docs with the outputs of the forward method ... return docs","title":"Anatomy of a trainable component"},{"location":"concepts/#nesting-trainable-components","text":"Like pytorch modules, you can compose trainable components together to build complex architectures. For instance, a deep classifier component can call an embedding component. Nesting components allows switching parts of the neural networks to test various architectures and keeping the modelling logic modular.","title":"Nesting trainable components"},{"location":"concepts/#sharing-subcomponents","text":"Sharing parts of a neural network while training components on different tasks can be an effective way to improve the network efficiency. For instance, it is common to share an embedding layer between multiple tasks that require embedding the same inputs. In EDS-PDF, sharing a subcomponent is simply done by sharing the object between the multiple components. You can either refer to an existing subcomponent when configuring a new component in Python, or use the interpolation mechanism of our configuration system. API-based Configuration-based pipeline . add_pipe ( \"my-component-1\" , name = \"first\" , config = { \"embedding\" : { \"@factory\" : \"box-embedding\" , ... } }) pipeline . add_pipe ( \"my-component-2\" , name = \"second\" , config = { \"embedding\" : pipeline . components . first . embedding , }) [components.first] @factory = \"my-component-1\" [components.first.embedding] @factory = \"box-embedding\" ... [components.second] @factory = \"my-component-2\" embedding = ${components.first.embedding} To avoid recomputing the preprocess / forward and collate in the multiple components that use it, we rely on a transparent cache system. Cache During the training loop, the cache must be emptied at every step to release CPU and GPU memory occupied by the cached methods outputs. This is done by calling reset_cache method on the pipeline.","title":"Sharing subcomponents"},{"location":"contributing/","text":"Contributing to EDS-PDF We welcome contributions ! There are many ways to help. For example, you can: Help us track bugs by filing issues Suggest and help prioritise new functionalities Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you. Development installation To be able to run the test suite and develop your own pipeline, you should clone the repo and install it locally. We use Poetry to manage dependencies. See the installation instructions . color:gray # Clone the repository and change directory $ git clone ssh://git@github.com/aphp/edspdf.git ---> 100% $ cd edspdf color:gray # Install the dependencies $ poetry install To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the pre-commit Python library. To use it, simply install it: $ pre-commit install The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong. The hooks only run on staged changes. To force-run it on all files, run: $ pre-commit run --all-files ---> 100% color:green All good ! Proposing a merge request At the very least, your changes should : Be well-documented ; Pass every tests, and preferably implement its own ; Follow the style guide. Testing your code We use the Pytest test suite. The following command will run the test suite. Writing your own tests is encouraged ! poetry run pytest Should your contribution propose a bug fix, we require the bug be thoroughly tested. Style Guide We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short : Black reformats entire files in place. It is not configurable. Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use pre-commit to keep our codebase clean. Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save. Documentation Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be. We use MkDocs for EDS-PDF's documentation. You can checkout the changes you make with: color:gray # Run the documentation $ mkdocs serve Go to localhost:8000 to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page.","title":"Contributing to EDS-PDF"},{"location":"contributing/#contributing-to-eds-pdf","text":"We welcome contributions ! There are many ways to help. For example, you can: Help us track bugs by filing issues Suggest and help prioritise new functionalities Help us make the library as straightforward as possible, by simply asking questions on whatever does not seem clear to you.","title":"Contributing to EDS-PDF"},{"location":"contributing/#development-installation","text":"To be able to run the test suite and develop your own pipeline, you should clone the repo and install it locally. We use Poetry to manage dependencies. See the installation instructions . color:gray # Clone the repository and change directory $ git clone ssh://git@github.com/aphp/edspdf.git ---> 100% $ cd edspdf color:gray # Install the dependencies $ poetry install To make sure the pipeline will not fail because of formatting errors, we added pre-commit hooks using the pre-commit Python library. To use it, simply install it: $ pre-commit install The pre-commit hooks defined in the configuration will automatically run when you commit your changes, letting you know if something went wrong. The hooks only run on staged changes. To force-run it on all files, run: $ pre-commit run --all-files ---> 100% color:green All good !","title":"Development installation"},{"location":"contributing/#proposing-a-merge-request","text":"At the very least, your changes should : Be well-documented ; Pass every tests, and preferably implement its own ; Follow the style guide.","title":"Proposing a merge request"},{"location":"contributing/#testing-your-code","text":"We use the Pytest test suite. The following command will run the test suite. Writing your own tests is encouraged ! poetry run pytest Should your contribution propose a bug fix, we require the bug be thoroughly tested.","title":"Testing your code"},{"location":"contributing/#style-guide","text":"We use Black to reformat the code. While other formatter only enforce PEP8 compliance, Black also makes the code uniform. In short : Black reformats entire files in place. It is not configurable. Moreover, the CI/CD pipeline enforces a number of checks on the \"quality\" of the code. To wit, non black-formatted code will make the test pipeline fail. We use pre-commit to keep our codebase clean. Refer to the development install tutorial for tips on how to format your files automatically. Most modern editors propose extensions that will format files on save.","title":"Style Guide"},{"location":"contributing/#documentation","text":"Make sure to document your improvements, both within the code with comprehensive docstrings, as well as in the documentation itself if need be. We use MkDocs for EDS-PDF's documentation. You can checkout the changes you make with: color:gray # Run the documentation $ mkdocs serve Go to localhost:8000 to see your changes. MkDocs watches for changes in the documentation folder and automatically reloads the page.","title":"Documentation"},{"location":"roadmap/","text":"Roadmap Style extraction Custom hybrid torch-based pipeline & configuration system Drop pandas DataFrame in favour of a ~~Cython~~ attr wrapper around PDF documents? Add training capabilities with a CLI to automate the annotation/preparation/training loop. Again, draw inspiration from spaCy, and maybe add the notion of a TrainableClassifier ... Add complete serialisation capabilities, to save a full pipeline to disk. Draw inspiration from spaCy, which took great care to solve these issues: add save and load methods to every pipeline component Multiple-column extraction Table detector Integrate third-party OCR module","title":"Roadmap"},{"location":"roadmap/#roadmap","text":"Style extraction Custom hybrid torch-based pipeline & configuration system Drop pandas DataFrame in favour of a ~~Cython~~ attr wrapper around PDF documents? Add training capabilities with a CLI to automate the annotation/preparation/training loop. Again, draw inspiration from spaCy, and maybe add the notion of a TrainableClassifier ... Add complete serialisation capabilities, to save a full pipeline to disk. Draw inspiration from spaCy, which took great care to solve these issues: add save and load methods to every pipeline component Multiple-column extraction Table detector Integrate third-party OCR module","title":"Roadmap"},{"location":"components/","text":"Pipelines overview EDS-PDF provides easy-to-use components for defining PDF processing pipelines. Box extractors Box classifiers Aggregators Pipeline Description pdfminer-extractor Extracts text lines with the pdfminer library mupdf-extractor Extracts text lines with the pymupdf library poppler-extractor Extracts text lines with the poppler software Pipeline Description deep-classifier Trainable box classification model mask-classifier Simple rule-based classification multi-mask-classifier Simple rule-based classification dummy-classifier Dummy classifier, for testing purposes. random-classifier To sow chaos Method Description simple-aggregator Returns a dictionary with one key for each detected class styled-aggregator Returns the same dictionary, as well as the information on styles You can add them to your EDS-PDF pipeline by simply calling add_pipe , for instance: # \u2191 Omitted code that defines the pipeline object \u2191 pipeline . add_pipe ( \"pdfminer-extractor\" , name = \"component-name\" , config =... )","title":"Pipelines overview"},{"location":"components/#pipelines-overview","text":"EDS-PDF provides easy-to-use components for defining PDF processing pipelines. Box extractors Box classifiers Aggregators Pipeline Description pdfminer-extractor Extracts text lines with the pdfminer library mupdf-extractor Extracts text lines with the pymupdf library poppler-extractor Extracts text lines with the poppler software Pipeline Description deep-classifier Trainable box classification model mask-classifier Simple rule-based classification multi-mask-classifier Simple rule-based classification dummy-classifier Dummy classifier, for testing purposes. random-classifier To sow chaos Method Description simple-aggregator Returns a dictionary with one key for each detected class styled-aggregator Returns the same dictionary, as well as the information on styles You can add them to your EDS-PDF pipeline by simply calling add_pipe , for instance: # \u2191 Omitted code that defines the pipeline object \u2191 pipeline . add_pipe ( \"pdfminer-extractor\" , name = \"component-name\" , config =... )","title":"Pipelines overview"},{"location":"components/aggregators/","text":"Aggregation The aggregation step compiles extracted text blocs together according to their detected class. Method Description simple-aggregator Returns a dictionary with one key for each detected class styled-aggregator Returns the same dictionary, as well as the information on styles","title":"Aggregation"},{"location":"components/aggregators/#aggregation","text":"The aggregation step compiles extracted text blocs together according to their detected class. Method Description simple-aggregator Returns a dictionary with one key for each detected class styled-aggregator Returns the same dictionary, as well as the information on styles","title":"Aggregation"},{"location":"components/classifiers/","text":"Classifiers We developed EDS-PDF with modularity in mind. To that end, you can choose between multiple classification methods. Method Description mask-classifier Simple rule-based classification dummy-classifier Dummy classifier, for testing purposes. Classifies every bloc to body random.v1 To sow chaos","title":"Classifiers"},{"location":"components/classifiers/#classifiers","text":"We developed EDS-PDF with modularity in mind. To that end, you can choose between multiple classification methods. Method Description mask-classifier Simple rule-based classification dummy-classifier Dummy classifier, for testing purposes. Classifies every bloc to body random.v1 To sow chaos","title":"Classifiers"},{"location":"components/classifiers/deep-classifier/","text":"Deep learning classifier This component predicts the label of each line over the whole document using machine learning. Note You must train the model your model to use this classifier. See Model training for more information Configuration example The classifier is composed of the following blocks: a configurable box embedding layer a linear classification layer In this example, we use a box-embedding layer to generate the embeddings of the boxes. It is composed of a text encoder that embeds the text features of the boxes and a layout encoder that embeds the layout features of the boxes. These two embeddings are summed and passed through an optional contextualizer , here a box-transformer . API-based Configuration-based pipeline . add_pipe ( \"deep-classifier\" , name = \"classifier\" , config = { \"embedding\" : { \"@factory\" : \"box-embedding\" , \"size\" : 72 , \"dropout_p\" : 0.1 , \"text_encoder\" : { \"@factory\" : \"box-text-embedding\" , \"pooler\" : { \"@factory\" : \"cnn-pooler\" , \"out_channels\" : 64 , \"kernel_sizes\" : ( 3 , 4 , 5 ), }, }, \"layout_encoder\" : { \"@factory\" : \"box-layout-embedding\" , \"n_positions\" : 64 , \"x_mode\" : \"learned\" , \"y_mode\" : \"learned\" , \"w_mode\" : \"learned\" , \"h_mode\" : \"learned\" , }, \"contextualizer\" : { \"@factory\" : \"box-transformer\" , \"n_relative_positions\" : 64 , \"input_size\" : 72 , \"num_heads\" : 4 , \"dropout_p\" : 0.1 , \"activation\" : \"gelu\" , \"init_resweight\" : 0.01 , \"head_size\" : 16 , \"attention_mode\" : ( \"c2c\" , \"c2p\" , \"p2c\" ), \"n_layers\" : 1 , }, }, \"labels\" : [], \"activation\" : \"relu\" , }, ) [components.classifier] @factory = \"deep-classifier\" labels = [] activation = \"relu\" [components.classifier.embedding] @factory = \"box-embedding\" size = 72 dropout_p = 0.1 [components.classifier.embedding.layout_encoder] n_positions = 64 x_mode = \"learned\" y_mode = \"learned\" w_mode = \"learned\" h_mode = \"learned\" [components.classifier.embedding.text_encoder] [components.classifier.embedding.text_encoder.pooler] out_channels = 64 kernel_sizes = [3,4,5] [components.classifier.embedding.contextualizer] @factory = \"box-transformer\" n_relative_positions = 64 num_heads = 4 dropout_p = 0.1 activation = \"gelu\" init_resweight = 0.01 head_size = 16 attention_mode = \"c2c,c2p,p2c\" n_layers = 1","title":"Deep learning classifier"},{"location":"components/classifiers/deep-classifier/#deep-learning-classifier","text":"This component predicts the label of each line over the whole document using machine learning. Note You must train the model your model to use this classifier. See Model training for more information","title":"Deep learning classifier"},{"location":"components/classifiers/deep-classifier/#configuration-example","text":"The classifier is composed of the following blocks: a configurable box embedding layer a linear classification layer In this example, we use a box-embedding layer to generate the embeddings of the boxes. It is composed of a text encoder that embeds the text features of the boxes and a layout encoder that embeds the layout features of the boxes. These two embeddings are summed and passed through an optional contextualizer , here a box-transformer . API-based Configuration-based pipeline . add_pipe ( \"deep-classifier\" , name = \"classifier\" , config = { \"embedding\" : { \"@factory\" : \"box-embedding\" , \"size\" : 72 , \"dropout_p\" : 0.1 , \"text_encoder\" : { \"@factory\" : \"box-text-embedding\" , \"pooler\" : { \"@factory\" : \"cnn-pooler\" , \"out_channels\" : 64 , \"kernel_sizes\" : ( 3 , 4 , 5 ), }, }, \"layout_encoder\" : { \"@factory\" : \"box-layout-embedding\" , \"n_positions\" : 64 , \"x_mode\" : \"learned\" , \"y_mode\" : \"learned\" , \"w_mode\" : \"learned\" , \"h_mode\" : \"learned\" , }, \"contextualizer\" : { \"@factory\" : \"box-transformer\" , \"n_relative_positions\" : 64 , \"input_size\" : 72 , \"num_heads\" : 4 , \"dropout_p\" : 0.1 , \"activation\" : \"gelu\" , \"init_resweight\" : 0.01 , \"head_size\" : 16 , \"attention_mode\" : ( \"c2c\" , \"c2p\" , \"p2c\" ), \"n_layers\" : 1 , }, }, \"labels\" : [], \"activation\" : \"relu\" , }, ) [components.classifier] @factory = \"deep-classifier\" labels = [] activation = \"relu\" [components.classifier.embedding] @factory = \"box-embedding\" size = 72 dropout_p = 0.1 [components.classifier.embedding.layout_encoder] n_positions = 64 x_mode = \"learned\" y_mode = \"learned\" w_mode = \"learned\" h_mode = \"learned\" [components.classifier.embedding.text_encoder] [components.classifier.embedding.text_encoder.pooler] out_channels = 64 kernel_sizes = [3,4,5] [components.classifier.embedding.contextualizer] @factory = \"box-transformer\" n_relative_positions = 64 num_heads = 4 dropout_p = 0.1 activation = \"gelu\" init_resweight = 0.01 head_size = 16 attention_mode = \"c2c,c2p,p2c\" n_layers = 1","title":"Configuration example"},{"location":"components/classifiers/mask/","text":"Mask Classification We developed a simple classifier that roughly uses the same strategy as PDFBox, namely: define a \"mask\" on the PDF documents ; keep every text bloc within that mask, tag everything else as pollution. Factories Two factories are available in the classifiers registry: mask-classifier and multi-mask-classifier . mask-classifier The simplest form. You define the mask, everything else is tagged as pollution. Example configuration : API-based Configuration-based pipeline . add_pipe ( \"mask-classifier\" , name = \"classifier\" , config = { \"threshold\" : 0.9 , \"x0\" : 0.1 , \"y0\" : 0.1 , \"x1\" : 0.9 , \"y1\" : 0.9 , }, ) [components.classifier] @classifiers = \"mask-classifier\" x0 = 0.1 y0 = 0.1 x1 = 0.9 y1 = 0.9 threshold = 0.9 multi-mask-classifier A generalisation, wherein the user defines a number of regions. The following configuration produces exactly the same classifier as mask.v1 example above. API-based Configuration-based pipeline . add_pipe ( \"multi-mask-classifier\" , name = \"classifier\" , config = { \"threshold\" : 0.9 , \"body\" : { \"x0\" : 0.1 , \"y0\" : 0.1 , \"x1\" : 0.9 , \"y1\" : 0.3 , \"label\" : \"header\" }, }, ) [components.classifier] @factory = \"multi-mask-classifier\" threshold = 0.9 [components.classifier.body] label = \"body\" x0 = 0.1 y0 = 0.1 x1 = 0.9 y1 = 0.9 The following configuration defines a header region. API-based Configuration-based pipeline . add_pipe ( \"multi-mask-classifier\" , name = \"classifier\" , config = { \"threshold\" : 0.9 , \"body\" : { \"x0\" : 0.1 , \"y0\" : 0.1 , \"x1\" : 0.9 , \"y1\" : 0.3 , \"label\" : \"header\" }, \"header\" : { \"x0\" : 0.1 , \"y0\" : 0.3 , \"x1\" : 0.9 , \"y1\" : 0.9 , \"label\" : \"body\" }, }, ) [components.classifier] @factory = \"multi-mask-classifier\" threshold = 0.9 [components.classifier.header] label = \"header\" x0 = 0.1 y0 = 0.1 x1 = 0.9 y1 = 0.3 [components.classifier.body] label = \"body\" x0 = 0.1 y0 = 0.3 x1 = 0.9 y1 = 0.9 Any bloc that is not part of a mask is tagged as pollution .","title":"Mask Classification"},{"location":"components/classifiers/mask/#mask-classification","text":"We developed a simple classifier that roughly uses the same strategy as PDFBox, namely: define a \"mask\" on the PDF documents ; keep every text bloc within that mask, tag everything else as pollution.","title":"Mask Classification"},{"location":"components/classifiers/mask/#factories","text":"Two factories are available in the classifiers registry: mask-classifier and multi-mask-classifier .","title":"Factories"},{"location":"components/classifiers/mask/#mask-classifier","text":"The simplest form. You define the mask, everything else is tagged as pollution. Example configuration : API-based Configuration-based pipeline . add_pipe ( \"mask-classifier\" , name = \"classifier\" , config = { \"threshold\" : 0.9 , \"x0\" : 0.1 , \"y0\" : 0.1 , \"x1\" : 0.9 , \"y1\" : 0.9 , }, ) [components.classifier] @classifiers = \"mask-classifier\" x0 = 0.1 y0 = 0.1 x1 = 0.9 y1 = 0.9 threshold = 0.9","title":"mask-classifier"},{"location":"components/classifiers/mask/#multi-mask-classifier","text":"A generalisation, wherein the user defines a number of regions. The following configuration produces exactly the same classifier as mask.v1 example above. API-based Configuration-based pipeline . add_pipe ( \"multi-mask-classifier\" , name = \"classifier\" , config = { \"threshold\" : 0.9 , \"body\" : { \"x0\" : 0.1 , \"y0\" : 0.1 , \"x1\" : 0.9 , \"y1\" : 0.3 , \"label\" : \"header\" }, }, ) [components.classifier] @factory = \"multi-mask-classifier\" threshold = 0.9 [components.classifier.body] label = \"body\" x0 = 0.1 y0 = 0.1 x1 = 0.9 y1 = 0.9 The following configuration defines a header region. API-based Configuration-based pipeline . add_pipe ( \"multi-mask-classifier\" , name = \"classifier\" , config = { \"threshold\" : 0.9 , \"body\" : { \"x0\" : 0.1 , \"y0\" : 0.1 , \"x1\" : 0.9 , \"y1\" : 0.3 , \"label\" : \"header\" }, \"header\" : { \"x0\" : 0.1 , \"y0\" : 0.3 , \"x1\" : 0.9 , \"y1\" : 0.9 , \"label\" : \"body\" }, }, ) [components.classifier] @factory = \"multi-mask-classifier\" threshold = 0.9 [components.classifier.header] label = \"header\" x0 = 0.1 y0 = 0.1 x1 = 0.9 y1 = 0.3 [components.classifier.body] label = \"body\" x0 = 0.1 y0 = 0.3 x1 = 0.9 y1 = 0.9 Any bloc that is not part of a mask is tagged as pollution .","title":"multi-mask-classifier"},{"location":"components/extractors/","text":"Extraction The extraction phase consists of reading the PDF document and gather text blocs, along with their dimensions and position within the document. Said blocs will go on to the classification phase to separate the body from the rest. Text-based PDF We provide a multiple extractor architectures for text-based PDFs : Component Description pdfminer-extractor Text-based PDF extraction using PDFMiner mupdf-extractor Text-based PDF extraction using MuPDF poppler-extractor Text-based PDF extraction using Poppler Image-based PDF Image-based PDF documents require an OCR 1 step, which is not natively supported by EDS-PDF. However, you can easily extend EDS-PDF by adding such a method to the registry. We plan on adding such an OCR extractor component in the future. Optical Character Recognition, or OCR, is the process of extracting characters and words from an image. \u21a9","title":"Extraction"},{"location":"components/extractors/#extraction","text":"The extraction phase consists of reading the PDF document and gather text blocs, along with their dimensions and position within the document. Said blocs will go on to the classification phase to separate the body from the rest.","title":"Extraction"},{"location":"components/extractors/#text-based-pdf","text":"We provide a multiple extractor architectures for text-based PDFs : Component Description pdfminer-extractor Text-based PDF extraction using PDFMiner mupdf-extractor Text-based PDF extraction using MuPDF poppler-extractor Text-based PDF extraction using Poppler","title":"Text-based PDF"},{"location":"components/extractors/#image-based-pdf","text":"Image-based PDF documents require an OCR 1 step, which is not natively supported by EDS-PDF. However, you can easily extend EDS-PDF by adding such a method to the registry. We plan on adding such an OCR extractor component in the future. Optical Character Recognition, or OCR, is the process of extracting characters and words from an image. \u21a9","title":"Image-based PDF"},{"location":"components/extractors/mupdf/","text":"MuPdfExtractor We provide a PDF line extractor built on top of PyMuPdf . This extractor is the fastest but may not be as portable as PdfMiner . However, it should also be relatively easy to install on a wide range of architectures, Linux, OS X and Windows. License Beware, PyMuPdf is distributed under the AGPL license, therefore so is this component, and any model depending on this component must be too. Installation For the licensing reason mentioned above, the mupdf component is distributed in a separate package edspdf-mupdf . To install it, use your favorite Python package manager : poetry add edspdf-mupdf # or pip install edspdf-mupdf Usage from edspdf import Pipeline from pathlib import Path # Add the component to a new pipeline model = Pipeline () model . add_pipe ( \"mupdf-extractor\" , config = dict ( extract_style = False , ), ) # Apply on a new document model ( Path ( \"path/to/your/pdf/document\" ) . read_bytes ()) Configuration Parameter Description Default extract_style Whether to extract style (font, size, ...) information for each line of the document. False","title":"MuPdfExtractor"},{"location":"components/extractors/mupdf/#mupdfextractor","text":"We provide a PDF line extractor built on top of PyMuPdf . This extractor is the fastest but may not be as portable as PdfMiner . However, it should also be relatively easy to install on a wide range of architectures, Linux, OS X and Windows. License Beware, PyMuPdf is distributed under the AGPL license, therefore so is this component, and any model depending on this component must be too.","title":"MuPdfExtractor"},{"location":"components/extractors/mupdf/#installation","text":"For the licensing reason mentioned above, the mupdf component is distributed in a separate package edspdf-mupdf . To install it, use your favorite Python package manager : poetry add edspdf-mupdf # or pip install edspdf-mupdf","title":"Installation"},{"location":"components/extractors/mupdf/#usage","text":"from edspdf import Pipeline from pathlib import Path # Add the component to a new pipeline model = Pipeline () model . add_pipe ( \"mupdf-extractor\" , config = dict ( extract_style = False , ), ) # Apply on a new document model ( Path ( \"path/to/your/pdf/document\" ) . read_bytes ())","title":"Usage"},{"location":"components/extractors/mupdf/#configuration","text":"Parameter Description Default extract_style Whether to extract style (font, size, ...) information for each line of the document. False","title":"Configuration"},{"location":"components/extractors/pdfminer/","text":"PdfMinerExtractor We provide a PDF line extractor built on top of PdfMiner . This is the most portable extractor, since it is pure-python and can therefore be run on any platform. Be sure to have a look at their documentation, especially the part providing a bird's eye view of the PDF extraction process . Usage from edspdf import Pipeline from pathlib import Path # Add the component to a new pipeline model = Pipeline () model . add_pipe ( \"pdfminer-extractor\" , config = dict ( extract_style = False , ), ) # Apply on a new document model ( Path ( \"path/to/your/pdf/document\" ) . read_bytes ()) Configuration Parameter Description Default line_overlap See PDFMiner documentation 0.5 char_margin See PDFMiner documentation 2.05 line_margin See PDFMiner documentation 0.5 word_margin See PDFMiner documentation 0.1 boxes_flow See PDFMiner documentation 0.5 detect_vertical See PDFMiner documentation False all_texts See PDFMiner documentation False extract_style Whether to extract style (font, size, ...) information for each line of the document. False","title":"PdfMinerExtractor"},{"location":"components/extractors/pdfminer/#pdfminerextractor","text":"We provide a PDF line extractor built on top of PdfMiner . This is the most portable extractor, since it is pure-python and can therefore be run on any platform. Be sure to have a look at their documentation, especially the part providing a bird's eye view of the PDF extraction process .","title":"PdfMinerExtractor"},{"location":"components/extractors/pdfminer/#usage","text":"from edspdf import Pipeline from pathlib import Path # Add the component to a new pipeline model = Pipeline () model . add_pipe ( \"pdfminer-extractor\" , config = dict ( extract_style = False , ), ) # Apply on a new document model ( Path ( \"path/to/your/pdf/document\" ) . read_bytes ())","title":"Usage"},{"location":"components/extractors/pdfminer/#configuration","text":"Parameter Description Default line_overlap See PDFMiner documentation 0.5 char_margin See PDFMiner documentation 2.05 line_margin See PDFMiner documentation 0.5 word_margin See PDFMiner documentation 0.1 boxes_flow See PDFMiner documentation 0.5 detect_vertical See PDFMiner documentation False all_texts See PDFMiner documentation False extract_style Whether to extract style (font, size, ...) information for each line of the document. False","title":"Configuration"},{"location":"components/extractors/poppler/","text":"PopplerExtractor We provide a PDF line extractor built on top of PyMuPdf . The poppler software is more difficult to install than its pdfminer and mupdf counterparts. In particular, the bindings we provide have not been tested on Windows. License Beware, Poppler is distributed under the GPL license, therefore so is this component, and any model depending on this component must be too. Installation For the licensing reason mentioned above, the poppler component is distributed in a separate package edspdf-poppler . To install it, use your favorite Python package manager : poetry add edspdf-poppler # or pip install edspdf-poppler Usage from edspdf import Pipeline from pathlib import Path # Add the component to a new pipeline model = Pipeline () model . add_pipe ( \"poppler-extractor\" , config = dict ( extract_style = False , ), ) # Apply on a new document model ( Path ( \"path/to/your/pdf/document\" ) . read_bytes ()) Configuration Parameter Description Default extract_style Whether to extract style (font, size, ...) information for each line of the document. False","title":"PopplerExtractor"},{"location":"components/extractors/poppler/#popplerextractor","text":"We provide a PDF line extractor built on top of PyMuPdf . The poppler software is more difficult to install than its pdfminer and mupdf counterparts. In particular, the bindings we provide have not been tested on Windows. License Beware, Poppler is distributed under the GPL license, therefore so is this component, and any model depending on this component must be too.","title":"PopplerExtractor"},{"location":"components/extractors/poppler/#installation","text":"For the licensing reason mentioned above, the poppler component is distributed in a separate package edspdf-poppler . To install it, use your favorite Python package manager : poetry add edspdf-poppler # or pip install edspdf-poppler","title":"Installation"},{"location":"components/extractors/poppler/#usage","text":"from edspdf import Pipeline from pathlib import Path # Add the component to a new pipeline model = Pipeline () model . add_pipe ( \"poppler-extractor\" , config = dict ( extract_style = False , ), ) # Apply on a new document model ( Path ( \"path/to/your/pdf/document\" ) . read_bytes ())","title":"Usage"},{"location":"components/extractors/poppler/#configuration","text":"Parameter Description Default extract_style Whether to extract style (font, size, ...) information for each line of the document. False","title":"Configuration"},{"location":"concepts/","text":"Concepts The goal of EDS-PDF is to provide a framework for processing PDF documents, along with some utilities and a few components, stitched together by a robust pipeline and configuration system. Processing PDFs usually involves many steps such as extracting lines, running OCR models, detecting and classifying boxes, filtering and aggregating parts of the extracted texts, etc. Organising these steps together, combining static and deep learning components, while remaining modular and efficient is a challenge. This is why EDS-PDF is built on top of a new pipelining system. At the moment, three types of components are implemented in the library: extraction components extract lines from a raw PDF and return a PDFDoc object filled with these text boxes. classification components classify each box with labels, such as body , header , footer ... aggregation components compiles the lines together according to their classes to re-create the original text. EDS-PDF takes inspiration from Explosion's spaCy pipelining system and closely follows its API. Therefore, the core object within EDS-PDF is the Pipeline, which organises the processing of PDF documents into multiple components. However, unlike spaCy, the library is built around a single deep learning framework, pytorch, which makes model development easier. Similar to spaCy and thinc , EDS-PDF also relies on catalogue entry points and a custom powerful custom configuration system. This allows complex pipeline models to be built and trained using either the library API or configuration files. In the next chapters, we will go over some core concepts of EDS-PDF: the pipelining system the configuration system deep learning support with trainable components","title":"Concepts"},{"location":"concepts/#concepts","text":"The goal of EDS-PDF is to provide a framework for processing PDF documents, along with some utilities and a few components, stitched together by a robust pipeline and configuration system. Processing PDFs usually involves many steps such as extracting lines, running OCR models, detecting and classifying boxes, filtering and aggregating parts of the extracted texts, etc. Organising these steps together, combining static and deep learning components, while remaining modular and efficient is a challenge. This is why EDS-PDF is built on top of a new pipelining system. At the moment, three types of components are implemented in the library: extraction components extract lines from a raw PDF and return a PDFDoc object filled with these text boxes. classification components classify each box with labels, such as body , header , footer ... aggregation components compiles the lines together according to their classes to re-create the original text. EDS-PDF takes inspiration from Explosion's spaCy pipelining system and closely follows its API. Therefore, the core object within EDS-PDF is the Pipeline, which organises the processing of PDF documents into multiple components. However, unlike spaCy, the library is built around a single deep learning framework, pytorch, which makes model development easier. Similar to spaCy and thinc , EDS-PDF also relies on catalogue entry points and a custom powerful custom configuration system. This allows complex pipeline models to be built and trained using either the library API or configuration files. In the next chapters, we will go over some core concepts of EDS-PDF: the pipelining system the configuration system deep learning support with trainable components","title":"Concepts"},{"location":"concepts/configuration/","text":"Configuration Following the example of spaCy, EDS-PDF is organised a powerful configuration system and registries organised with Explosion's catalogue library. The following catalogues are included within EDS-PDF: Section Description factory Components factories (most often classes) adapter Raw data preprocessing functions Much like spaCy pipelines, EDS-PDF pipelines are meant to be reproducible and serialisable, such that you can always define a pipeline through the configuration system. To wit, compare the API-based approach to the configuration-based approach (the two are strictly equivalent): API-based Configuration-based from edspdf import aggregation , reading , extraction , classification from pathlib import Path model = spacy . Pipeline () model . add_pipe ( \"pdfminer-extractor\" , name = \"extractor\" ) model . add_pipe ( \"mask-classifier\" , name = \"classifier\" , config = dict ( x0 = 0.2 , x1 = 0.9 , y0 = 0.3 , y1 = 0.6 , threshold = 0.1 , ) model . add_pipe ( \"simple-aggregator\" , name = \"aggregator\" ) # Get a PDF pdf = Path ( \"letter.pdf\" ) . read_bytes () texts = model ( pdf ) texts [ \"body\" ] # Out: Cher Pr ABC, Cher DEF,\\n... config.cfg [pipeline] components = [ \"extractor\" , \"classifier\" , \"aggregator\" ] components_config = $ { components } [components.extractor] @ factory = \"pdfminer-extractor\" [components.classifier] @ factory = \"mask-classifier\" x0 = 0.2 x1 = 0.9 y0 = 0.3 y1 = 0.6 threshold = 0.1 [components.aggregator] @ aggregators = \"simple-extractor\" from edspdf import registry , Config from pathlib import Path config = Config () . from_disk ( \"config.cfg\" ) pipeline = config . resolve ()[ \"pipeline\" ] # Get a PDF pdf = Path ( \"letter.pdf\" ) . read_bytes () texts = pipeline ( pdf ) texts [ \"body\" ] # Out: Cher Pr ABC, Cher DEF,\\n... The configuration-based approach strictly separates the definition of the pipeline to its application and avoids tucking away important configuration details. Changes to the pipeline are transparent as there is a single source of truth: the configuration file. Interpolation Our configuration system relies heavily on interpolation to make it easier to define complex architectures within a single configuration file. However, unlike confection , EDS-PDF replaces interpolated variables after they are resolved using registries, which allows sharing model parts between components when defining pipelines.","title":"Configuration"},{"location":"concepts/configuration/#configuration","text":"Following the example of spaCy, EDS-PDF is organised a powerful configuration system and registries organised with Explosion's catalogue library. The following catalogues are included within EDS-PDF: Section Description factory Components factories (most often classes) adapter Raw data preprocessing functions Much like spaCy pipelines, EDS-PDF pipelines are meant to be reproducible and serialisable, such that you can always define a pipeline through the configuration system. To wit, compare the API-based approach to the configuration-based approach (the two are strictly equivalent): API-based Configuration-based from edspdf import aggregation , reading , extraction , classification from pathlib import Path model = spacy . Pipeline () model . add_pipe ( \"pdfminer-extractor\" , name = \"extractor\" ) model . add_pipe ( \"mask-classifier\" , name = \"classifier\" , config = dict ( x0 = 0.2 , x1 = 0.9 , y0 = 0.3 , y1 = 0.6 , threshold = 0.1 , ) model . add_pipe ( \"simple-aggregator\" , name = \"aggregator\" ) # Get a PDF pdf = Path ( \"letter.pdf\" ) . read_bytes () texts = model ( pdf ) texts [ \"body\" ] # Out: Cher Pr ABC, Cher DEF,\\n... config.cfg [pipeline] components = [ \"extractor\" , \"classifier\" , \"aggregator\" ] components_config = $ { components } [components.extractor] @ factory = \"pdfminer-extractor\" [components.classifier] @ factory = \"mask-classifier\" x0 = 0.2 x1 = 0.9 y0 = 0.3 y1 = 0.6 threshold = 0.1 [components.aggregator] @ aggregators = \"simple-extractor\" from edspdf import registry , Config from pathlib import Path config = Config () . from_disk ( \"config.cfg\" ) pipeline = config . resolve ()[ \"pipeline\" ] # Get a PDF pdf = Path ( \"letter.pdf\" ) . read_bytes () texts = pipeline ( pdf ) texts [ \"body\" ] # Out: Cher Pr ABC, Cher DEF,\\n... The configuration-based approach strictly separates the definition of the pipeline to its application and avoids tucking away important configuration details. Changes to the pipeline are transparent as there is a single source of truth: the configuration file.","title":"Configuration"},{"location":"concepts/configuration/#interpolation","text":"Our configuration system relies heavily on interpolation to make it easier to define complex architectures within a single configuration file. However, unlike confection , EDS-PDF replaces interpolated variables after they are resolved using registries, which allows sharing model parts between components when defining pipelines.","title":"Interpolation"},{"location":"concepts/pipeline/","text":"Pipeline The core object within EDS-PDF is the Pipeline, which organises the processing of PDF documents into multiple components. Deep learning frameworks The EDS-PDF trainable components are built around the PyTorch framework. While you can use any technology in static components, we do not provide tools to train components built with other deep learning frameworks. A component is a processing block (like a function) that applies a transformation on its input and returns a modified object. At the moment, three types of components are implemented in the library: extraction components extract lines from a raw PDF and return a PDFDoc object filled with these text boxes. classification components classify each box with labels, such as body , header , footer ... aggregation components compiles the lines together according to their classes to re-create the original text. To create your first pipeline, execute the following code: from edspdf import Pipeline model = Pipeline () # will extract text lines from a document model . add_pipe ( \"pdfminer-extractor\" , config = dict ( extract_style = False , ), ) # classify everything inside the `body` bounding box as `body` model . add_pipe ( \"mask-classifier\" , config = dict ( body = { \"x0\" : 0.1 , \"y0\" : 0.1 , \"x1\" : 0.9 , \"y1\" : 0.9 }) ) # aggregates the lines together to re-create the original text model . add_pipe ( \"simple-aggregator\" ) This pipeline can then be run on one or more PDF documents. As the pipeline process documents, components will be called in the order they were added to the pipeline. from pathlib import Path pdf_bytes = Path ( \"path/to/your/pdf\" ) . read_bytes () # Processing one document model ( pdf_bytes ) # Processing multiple documents model . pipe ([ pdf_bytes , ... ]) Hybrid models EDS-PDF was designed to facilitate the training and inference of hybrid models that arbitrarily chain static components or trained deep learning components. Static components are callable objects that take a PDFDoc object as input, perform arbitrary transformations over the input, and return the modified object. Trainable components , on the other hand, allow for deep learning operations to be performed on the PDFDoc object and must be trained to be used.","title":"Pipeline"},{"location":"concepts/pipeline/#pipeline","text":"The core object within EDS-PDF is the Pipeline, which organises the processing of PDF documents into multiple components. Deep learning frameworks The EDS-PDF trainable components are built around the PyTorch framework. While you can use any technology in static components, we do not provide tools to train components built with other deep learning frameworks. A component is a processing block (like a function) that applies a transformation on its input and returns a modified object. At the moment, three types of components are implemented in the library: extraction components extract lines from a raw PDF and return a PDFDoc object filled with these text boxes. classification components classify each box with labels, such as body , header , footer ... aggregation components compiles the lines together according to their classes to re-create the original text. To create your first pipeline, execute the following code: from edspdf import Pipeline model = Pipeline () # will extract text lines from a document model . add_pipe ( \"pdfminer-extractor\" , config = dict ( extract_style = False , ), ) # classify everything inside the `body` bounding box as `body` model . add_pipe ( \"mask-classifier\" , config = dict ( body = { \"x0\" : 0.1 , \"y0\" : 0.1 , \"x1\" : 0.9 , \"y1\" : 0.9 }) ) # aggregates the lines together to re-create the original text model . add_pipe ( \"simple-aggregator\" ) This pipeline can then be run on one or more PDF documents. As the pipeline process documents, components will be called in the order they were added to the pipeline. from pathlib import Path pdf_bytes = Path ( \"path/to/your/pdf\" ) . read_bytes () # Processing one document model ( pdf_bytes ) # Processing multiple documents model . pipe ([ pdf_bytes , ... ])","title":"Pipeline"},{"location":"concepts/pipeline/#hybrid-models","text":"EDS-PDF was designed to facilitate the training and inference of hybrid models that arbitrarily chain static components or trained deep learning components. Static components are callable objects that take a PDFDoc object as input, perform arbitrary transformations over the input, and return the modified object. Trainable components , on the other hand, allow for deep learning operations to be performed on the PDFDoc object and must be trained to be used.","title":"Hybrid models"},{"location":"concepts/trainable-components/","text":"Trainable components Trainable components allow for deep learning operations to be performed on the PDFDoc object and must be trained to be used. Such components can be used to train a model to predict the label of the lines extracted from a PDF document. Anatomy of a trainable component Building and running deep learning models usually requires preprocessing the input sample into features, batching or \"collating\" these features together to process multiple samples at once, running deep learning operations over these features (in Pytorch, this step is done in the forward method) and postprocessing the outputs of these operation to complete the original sample. In the trainable components of EDS-PDF, preprocessing and postprocessing are decoupled from the deep learning code but collocated with the forward method. This is achieved by splitting the class of a trainable component into four methods: preprocess : converts a doc into features that will be consumed by the forward method, e.g. building arrays of features, encoding words into indices, etc collate : concatenates the preprocessed features of multiple documents into pytorch tensors forward : applies transformations over the collated features to compute new embeddings, probabilities, etc postprocess : use these predictions to annotate the document, for instance converting label probabilities into label attributes on the document lines This code organization allows us to keep the development of new deep-learning components simple while ensuring efficient models both during training and inference. Additionally, there is a fifth method initialize that is only called before training a new model, to complete the attributes of a component by looking at some of documents. It is especially useful to build vocabularies or detect the labels of a classification task. Here is an example of a trainable component: from typing import Any , Dict , Iterable , Sequence import torch from tqdm import tqdm from edspdf import Module , TrainableComponent , registry from edspdf.models import PDFDoc @registry . factory . register ( \"my-component\" ) class MyComponent ( TrainableComponent ): def __init__ ( self , # A subcomponent embedding : Module , ): super () . __init__ () self . embedding : Module = embedding def initialize ( self , gold_data : Iterable [ PDFDoc ]): # Initialize the component with the gold documents with self . label_vocabulary . initialization (): for doc in tqdm ( gold_data , desc = \"Initializing the component\" ): # Do something like learning a vocabulary over the initialization # documents ... # And initialize the subcomponent self . embedding . initialize ( gold_data ) # Initialize any layer that might be missing from the module self . classifier = torch . nn . Linear ( ... ) def preprocess ( self , doc : PDFDoc , supervision : bool = False ) -> Dict [ str , Any ]: # Preprocess the doc to extract features required to run the embedding # subcomponent, and this component return { \"embedding\" : self . embedding . preprocess ( doc , supervision = supervision ), \"my-feature\" : ... ( doc ), } def collate ( self , batch , device : torch . device ) -> Dict : # Collate the features of the \"embedding\" subcomponent # and the features of this component as well return { \"embedding\" : self . embedding . collate ( batch [ \"embedding\" ], device ), \"my-feature\" : torch . as_tensor ( batch [ \"my-feature\" ], device = device ), } def forward ( self , batch : Dict , supervision = False ) -> Dict : # Call the embedding subcomponent embeds = self . embedding ( batch [ \"embedding\" ]) # Do something with the embedding tensors output = ... ( embeds ) return output def postprocess ( self , docs : Sequence [ PDFDoc ], output : Dict ) -> Sequence [ PDFDoc ]: # Annotate the docs with the outputs of the forward method ... return docs Nesting trainable components Like pytorch modules, you can compose trainable components together to build complex architectures. For instance, a deep classifier component may delegate some of its logic to an embedding component, which will only be responsible for converting PDF lines into multidimensional arrays of numbers. Nesting components allows switching parts of the neural networks to test various architectures and keeping the modelling logic modular. Sharing subcomponents Sharing parts of a neural network while training components on different tasks can be an effective way to improve the network efficiency. For instance, it is common to share an embedding layer between multiple tasks that require embedding the same inputs. In EDS-PDF, sharing a subcomponent is simply done by sharing the object between the multiple components. You can either refer to an existing subcomponent when configuring a new component in Python, or use the interpolation mechanism of our configuration system. API-based Configuration-based pipeline . add_pipe ( \"my-component-1\" , name = \"first\" , config = { \"embedding\" : { \"@factory\" : \"box-embedding\" , # ... } }, ) pipeline . add_pipe ( \"my-component-2\" , name = \"second\" , config = { \"embedding\" : pipeline . components . first . embedding , }, ) [components.first] @factory = \"my-component-1\" [components.first.embedding] @factory = \"box-embedding\" ... [components.second] @factory = \"my-component-2\" embedding = ${components.first.embedding} To avoid recomputing the preprocess / forward and collate in the multiple components that use it, we rely on a transparent cache system. Cache During the training loop, the cache must be emptied at every step to release CPU and GPU memory occupied by the cached methods outputs. This is done by calling reset_cache method on the pipeline.","title":"Trainable components"},{"location":"concepts/trainable-components/#trainable-components","text":"Trainable components allow for deep learning operations to be performed on the PDFDoc object and must be trained to be used. Such components can be used to train a model to predict the label of the lines extracted from a PDF document.","title":"Trainable components"},{"location":"concepts/trainable-components/#anatomy-of-a-trainable-component","text":"Building and running deep learning models usually requires preprocessing the input sample into features, batching or \"collating\" these features together to process multiple samples at once, running deep learning operations over these features (in Pytorch, this step is done in the forward method) and postprocessing the outputs of these operation to complete the original sample. In the trainable components of EDS-PDF, preprocessing and postprocessing are decoupled from the deep learning code but collocated with the forward method. This is achieved by splitting the class of a trainable component into four methods: preprocess : converts a doc into features that will be consumed by the forward method, e.g. building arrays of features, encoding words into indices, etc collate : concatenates the preprocessed features of multiple documents into pytorch tensors forward : applies transformations over the collated features to compute new embeddings, probabilities, etc postprocess : use these predictions to annotate the document, for instance converting label probabilities into label attributes on the document lines This code organization allows us to keep the development of new deep-learning components simple while ensuring efficient models both during training and inference. Additionally, there is a fifth method initialize that is only called before training a new model, to complete the attributes of a component by looking at some of documents. It is especially useful to build vocabularies or detect the labels of a classification task. Here is an example of a trainable component: from typing import Any , Dict , Iterable , Sequence import torch from tqdm import tqdm from edspdf import Module , TrainableComponent , registry from edspdf.models import PDFDoc @registry . factory . register ( \"my-component\" ) class MyComponent ( TrainableComponent ): def __init__ ( self , # A subcomponent embedding : Module , ): super () . __init__ () self . embedding : Module = embedding def initialize ( self , gold_data : Iterable [ PDFDoc ]): # Initialize the component with the gold documents with self . label_vocabulary . initialization (): for doc in tqdm ( gold_data , desc = \"Initializing the component\" ): # Do something like learning a vocabulary over the initialization # documents ... # And initialize the subcomponent self . embedding . initialize ( gold_data ) # Initialize any layer that might be missing from the module self . classifier = torch . nn . Linear ( ... ) def preprocess ( self , doc : PDFDoc , supervision : bool = False ) -> Dict [ str , Any ]: # Preprocess the doc to extract features required to run the embedding # subcomponent, and this component return { \"embedding\" : self . embedding . preprocess ( doc , supervision = supervision ), \"my-feature\" : ... ( doc ), } def collate ( self , batch , device : torch . device ) -> Dict : # Collate the features of the \"embedding\" subcomponent # and the features of this component as well return { \"embedding\" : self . embedding . collate ( batch [ \"embedding\" ], device ), \"my-feature\" : torch . as_tensor ( batch [ \"my-feature\" ], device = device ), } def forward ( self , batch : Dict , supervision = False ) -> Dict : # Call the embedding subcomponent embeds = self . embedding ( batch [ \"embedding\" ]) # Do something with the embedding tensors output = ... ( embeds ) return output def postprocess ( self , docs : Sequence [ PDFDoc ], output : Dict ) -> Sequence [ PDFDoc ]: # Annotate the docs with the outputs of the forward method ... return docs","title":"Anatomy of a trainable component"},{"location":"concepts/trainable-components/#nesting-trainable-components","text":"Like pytorch modules, you can compose trainable components together to build complex architectures. For instance, a deep classifier component may delegate some of its logic to an embedding component, which will only be responsible for converting PDF lines into multidimensional arrays of numbers. Nesting components allows switching parts of the neural networks to test various architectures and keeping the modelling logic modular.","title":"Nesting trainable components"},{"location":"concepts/trainable-components/#sharing-subcomponents","text":"Sharing parts of a neural network while training components on different tasks can be an effective way to improve the network efficiency. For instance, it is common to share an embedding layer between multiple tasks that require embedding the same inputs. In EDS-PDF, sharing a subcomponent is simply done by sharing the object between the multiple components. You can either refer to an existing subcomponent when configuring a new component in Python, or use the interpolation mechanism of our configuration system. API-based Configuration-based pipeline . add_pipe ( \"my-component-1\" , name = \"first\" , config = { \"embedding\" : { \"@factory\" : \"box-embedding\" , # ... } }, ) pipeline . add_pipe ( \"my-component-2\" , name = \"second\" , config = { \"embedding\" : pipeline . components . first . embedding , }, ) [components.first] @factory = \"my-component-1\" [components.first.embedding] @factory = \"box-embedding\" ... [components.second] @factory = \"my-component-2\" embedding = ${components.first.embedding} To avoid recomputing the preprocess / forward and collate in the multiple components that use it, we rely on a transparent cache system. Cache During the training loop, the cache must be emptied at every step to release CPU and GPU memory occupied by the cached methods outputs. This is done by calling reset_cache method on the pipeline.","title":"Sharing subcomponents"},{"location":"layers/","text":"Deep learning layers EDS-PDF provides a set of deep learning layers that can be used to build trainable components. These layers are built on top of the PyTorch framework and can be used in any PyTorch model. Layer Description box-embedding High level layer combining multiple box embedding layers together box-layout-embedding Embeds the layout features (x/y/w/h) features of a box box-text-embedding Embeds the textual features (shape/prefix/suffix) features of a box box-layout-preprocessor Performs common preprocessing of box layout features to be used / shared by other components box-transformer Contextualize box embeddings with a 2d Transformer with relative position representations cnn-pooler A pytorch component that aggregates its inputs by running convolution and max-pooling ops relative-attention A 2d attention layer that optionally uses relative position to compute its attention scores sinusoidal-embedding A position embedding that uses trigonometric functions to encode positions vocabulary A non deep learning layer to encodes / decode vocabularies","title":"Deep learning layers"},{"location":"layers/#deep-learning-layers","text":"EDS-PDF provides a set of deep learning layers that can be used to build trainable components. These layers are built on top of the PyTorch framework and can be used in any PyTorch model. Layer Description box-embedding High level layer combining multiple box embedding layers together box-layout-embedding Embeds the layout features (x/y/w/h) features of a box box-text-embedding Embeds the textual features (shape/prefix/suffix) features of a box box-layout-preprocessor Performs common preprocessing of box layout features to be used / shared by other components box-transformer Contextualize box embeddings with a 2d Transformer with relative position representations cnn-pooler A pytorch component that aggregates its inputs by running convolution and max-pooling ops relative-attention A 2d attention layer that optionally uses relative position to compute its attention scores sinusoidal-embedding A position embedding that uses trigonometric functions to encode positions vocabulary A non deep learning layer to encodes / decode vocabularies","title":"Deep learning layers"},{"location":"recipes/","text":"EDS-PDF Recipes This section goes over a few use-cases for PDF extraction. It is meant as a more hands-on tutorial to get a grip on the library.","title":"EDS-PDF Recipes"},{"location":"recipes/#eds-pdf-recipes","text":"This section goes over a few use-cases for PDF extraction. It is meant as a more hands-on tutorial to get a grip on the library.","title":"EDS-PDF Recipes"},{"location":"recipes/annotation/","text":"PDF Annotation In this section, we will cover one methodology to annotate PDF documents. Data annotation at AP-HP's CDW At AP-HP's CDW 1 , we recently moved away from a rule- and Java-based PDF extraction pipeline (using PDFBox) to one using EDS-PDF. Hence, EDS-PDF is used in production, helping extract text from around 100k PDF documents every day. To train our pipeline presently in production, we annotated around 270 documents , and reached a f1-score of 0.98 on the body classification. Preparing the data for annotation We will frame the annotation phase as an image segmentation task, where annotators are asked to draw bounding boxes around the different sections. Hence, the very first step is to convert PDF documents to images. We suggest using the library pdf2image for that step. The following script will convert the PDF documents located in a data/pdfs directory to PNG images inside the data/images folder. import pdf2image from pathlib import Path DATA_DIR = Path ( \"data\" ) PDF_DIR = DATA_DIR / \"pdfs\" IMAGE_DIR = DATA_DIR / \"images\" for pdf in PDF_DIR . glob ( \"*.pdf\" ): imgs = pdf2image . convert_from_bytes ( pdf ) for page , img in enumerate ( imgs ): path = IMAGE_DIR / f \" { pdf . stem } _ { page } .png\" img . save ( path ) You can use any annotation tool to annotate the images. If you're looking for a simple way to annotate from within a Jupyter Notebook, ipyannotations might be a good fit. You will need to post-process the output to convert the annotations to the following format: Key Description page Page within the PDF (0-indexed) x0 Horizontal position of the top-left corner of the bounding box x1 Horizontal position of the bottom-right corner of the bounding box y0 Vertical position of the top-left corner of the bounding box y1 Vertical position of the bottom-right corner of the bounding box label Class of the bounding box (eg body , header ...) All dimensions should be normalised by the height and width of the page. Saving the dataset Once the annotation phase is complete, make sure the train/test split is performed once and for all when you create the dataset. We suggest the following structure: Directory structure dataset/ \u251c\u2500\u2500 train/ \u2502 \u251c\u2500\u2500 <note_id_1>.pdf \u2502 \u251c\u2500\u2500 <note_id_1>.json \u2502 \u251c\u2500\u2500 <note_id_2>.pdf \u2502 \u251c\u2500\u2500 <note_id_2>.json \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 test/ \u251c\u2500\u2500 <note_id_n>.pdf \u251c\u2500\u2500 <note_id_n>.json \u2514\u2500\u2500 ... Where the normalised annotation resides in a JSON file living next to the related PDF, and uses the following schema: Key Description note_id Reference to the document <properties> Optional property of the document itself annotations List of annotations, following the schema above This structure presents the advantage of being machine- and human-friendly. The JSON file contains annotated regions as well as any document property that could be useful to adapt the pipeline (typically for the classification step). Extracting annotations The following snippet extracts the annotations into a workable format: from pathlib import Path import pandas as pd def get_annotations ( directory : Path , ) -> pd . DataFrame : \"\"\" Read annotations from the dataset directory. Parameters ---------- directory : Path Dataset directory Returns ------- pd.DataFrame Pandas DataFrame containing the annotations. \"\"\" dfs = [] iterator = tqdm ( list ( directory . glob ( \"*.json\" ))) for path in iterator : meta = json . loads ( path . read_text ()) df = pd . DataFrame . from_records ( meta . pop ( \"annotations\" )) for k , v in meta . items (): # (1) df [ k ] = v dfs . append ( df ) return pd . concat ( dfs ) train_path = Path ( \"dataset/train\" ) annotations = get_annotations ( train_path ) Add a column for each additional property saved in the dataset. The annotations compiled this way can be used to train a pipeline. See the trained pipeline recipe for more detail. Greater Paris University Hospital's Clinical Data Warehouse \u21a9","title":"PDF Annotation"},{"location":"recipes/annotation/#pdf-annotation","text":"In this section, we will cover one methodology to annotate PDF documents. Data annotation at AP-HP's CDW At AP-HP's CDW 1 , we recently moved away from a rule- and Java-based PDF extraction pipeline (using PDFBox) to one using EDS-PDF. Hence, EDS-PDF is used in production, helping extract text from around 100k PDF documents every day. To train our pipeline presently in production, we annotated around 270 documents , and reached a f1-score of 0.98 on the body classification.","title":"PDF Annotation"},{"location":"recipes/annotation/#preparing-the-data-for-annotation","text":"We will frame the annotation phase as an image segmentation task, where annotators are asked to draw bounding boxes around the different sections. Hence, the very first step is to convert PDF documents to images. We suggest using the library pdf2image for that step. The following script will convert the PDF documents located in a data/pdfs directory to PNG images inside the data/images folder. import pdf2image from pathlib import Path DATA_DIR = Path ( \"data\" ) PDF_DIR = DATA_DIR / \"pdfs\" IMAGE_DIR = DATA_DIR / \"images\" for pdf in PDF_DIR . glob ( \"*.pdf\" ): imgs = pdf2image . convert_from_bytes ( pdf ) for page , img in enumerate ( imgs ): path = IMAGE_DIR / f \" { pdf . stem } _ { page } .png\" img . save ( path ) You can use any annotation tool to annotate the images. If you're looking for a simple way to annotate from within a Jupyter Notebook, ipyannotations might be a good fit. You will need to post-process the output to convert the annotations to the following format: Key Description page Page within the PDF (0-indexed) x0 Horizontal position of the top-left corner of the bounding box x1 Horizontal position of the bottom-right corner of the bounding box y0 Vertical position of the top-left corner of the bounding box y1 Vertical position of the bottom-right corner of the bounding box label Class of the bounding box (eg body , header ...) All dimensions should be normalised by the height and width of the page.","title":"Preparing the data for annotation"},{"location":"recipes/annotation/#saving-the-dataset","text":"Once the annotation phase is complete, make sure the train/test split is performed once and for all when you create the dataset. We suggest the following structure: Directory structure dataset/ \u251c\u2500\u2500 train/ \u2502 \u251c\u2500\u2500 <note_id_1>.pdf \u2502 \u251c\u2500\u2500 <note_id_1>.json \u2502 \u251c\u2500\u2500 <note_id_2>.pdf \u2502 \u251c\u2500\u2500 <note_id_2>.json \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 test/ \u251c\u2500\u2500 <note_id_n>.pdf \u251c\u2500\u2500 <note_id_n>.json \u2514\u2500\u2500 ... Where the normalised annotation resides in a JSON file living next to the related PDF, and uses the following schema: Key Description note_id Reference to the document <properties> Optional property of the document itself annotations List of annotations, following the schema above This structure presents the advantage of being machine- and human-friendly. The JSON file contains annotated regions as well as any document property that could be useful to adapt the pipeline (typically for the classification step).","title":"Saving the dataset"},{"location":"recipes/annotation/#extracting-annotations","text":"The following snippet extracts the annotations into a workable format: from pathlib import Path import pandas as pd def get_annotations ( directory : Path , ) -> pd . DataFrame : \"\"\" Read annotations from the dataset directory. Parameters ---------- directory : Path Dataset directory Returns ------- pd.DataFrame Pandas DataFrame containing the annotations. \"\"\" dfs = [] iterator = tqdm ( list ( directory . glob ( \"*.json\" ))) for path in iterator : meta = json . loads ( path . read_text ()) df = pd . DataFrame . from_records ( meta . pop ( \"annotations\" )) for k , v in meta . items (): # (1) df [ k ] = v dfs . append ( df ) return pd . concat ( dfs ) train_path = Path ( \"dataset/train\" ) annotations = get_annotations ( train_path ) Add a column for each additional property saved in the dataset. The annotations compiled this way can be used to train a pipeline. See the trained pipeline recipe for more detail. Greater Paris University Hospital's Clinical Data Warehouse \u21a9","title":"Extracting annotations"},{"location":"recipes/extension/","text":"Extending EDS-PDF EDS-PDF is organised around a function registry powered by catalogue and a custom configuration system. The result is a powerful framework that is easy to extend - and we'll see how in this section. For this recipe, let's imagine we're not entirely satisfied with the aggregation proposed by EDS-PDF. For instance, we might want an aggregator that outputs the text in Markdown format. Note Properly converting to markdown is no easy task. For this example, we will limit ourselves to detecting bold and italics sections. Developing the new aggregator Our aggregator will inherit from the StyledAggregator , and use the style to detect italics and bold sections. markdown_aggregator.py from typing import Dict , Tuple from edspdf import registry from edspdf.components.aggregators.styled import StyledAggregator from edspdf.models import PDFDoc @registry . factory . register ( \"markdown-aggregator\" ) # (1) class MarkdownAggregator ( StyledAggregator ): def __call__ ( self , doc : PDFDoc ) -> Tuple [ Dict [ str , str ]]: texts , styles = super ( self ) . aggregate ( doc ) body = texts . get ( \"body\" , \"\" ) style = styles . get ( \"body\" , []) fragments = [] for s in style : text = body [ s [ \"begin\" ]: s [ \"end\" ]] if s [ \"bold\" ]: text = f \"** { text } **\" if s [ \"italic\" ]: text = f \"_ { text } _\" fragments . append ( text ) return \"\" . join ( fragments ) The new aggregator is registered via this line The new aggregator redefines the __call__ method. It will output a single string, corresponding to the markdown-formatted output. That's it! You can use this new aggregator with the API: from edspdf import Pipeline from markdown_aggregator import MarkdownAggregator # (1) model = Pipeline () # will extract text lines from a document model . add_pipe ( \"pdfminer-extractor\" , config = dict ( extract_style = False , ), ) # classify everything inside the `body` bounding box as `body` model . add_pipe ( \"mask-classifier\" , config = dict ( body = { \"x0\" : 0.1 , \"y0\" : 0.1 , \"x1\" : 0.9 , \"y1\" : 0.9 }) ) # aggregates the lines together to re-create the original text model . add_pipe ( \"markdown-aggregator\" ) We're importing the aggregator that we just defined. It all works relatively smoothly! Making the aggregator discoverable Now, how can we instantiate the pipeline using the configuration system? The registry needs to be aware of the new function, but we shouldn't have to import mardown_aggregator.py just so that the module is registered as a side-effect... Catalogue solves this problem by using Python entry points . pyproject.toml (Poetry) setup.py [tool.poetry.plugins.\"edspdf_factory\"] \"markdown-aggregator\" = \"markdown_aggregator:MarkdownAggregator\" from setuptools import setup setup ( name = \"edspdf-markdown-aggregator\" , entry_points = { \"edspdf_factory\" : [ \"markdown-aggregator = markdown_aggregator:MarkdownAggregator\" ] }, ) By declaring the new aggregator as an entrypoint, it will become discoverable by EDS-PDF as long as it is installed in your environment!","title":"Extending EDS-PDF"},{"location":"recipes/extension/#extending-eds-pdf","text":"EDS-PDF is organised around a function registry powered by catalogue and a custom configuration system. The result is a powerful framework that is easy to extend - and we'll see how in this section. For this recipe, let's imagine we're not entirely satisfied with the aggregation proposed by EDS-PDF. For instance, we might want an aggregator that outputs the text in Markdown format. Note Properly converting to markdown is no easy task. For this example, we will limit ourselves to detecting bold and italics sections.","title":"Extending EDS-PDF"},{"location":"recipes/extension/#developing-the-new-aggregator","text":"Our aggregator will inherit from the StyledAggregator , and use the style to detect italics and bold sections. markdown_aggregator.py from typing import Dict , Tuple from edspdf import registry from edspdf.components.aggregators.styled import StyledAggregator from edspdf.models import PDFDoc @registry . factory . register ( \"markdown-aggregator\" ) # (1) class MarkdownAggregator ( StyledAggregator ): def __call__ ( self , doc : PDFDoc ) -> Tuple [ Dict [ str , str ]]: texts , styles = super ( self ) . aggregate ( doc ) body = texts . get ( \"body\" , \"\" ) style = styles . get ( \"body\" , []) fragments = [] for s in style : text = body [ s [ \"begin\" ]: s [ \"end\" ]] if s [ \"bold\" ]: text = f \"** { text } **\" if s [ \"italic\" ]: text = f \"_ { text } _\" fragments . append ( text ) return \"\" . join ( fragments ) The new aggregator is registered via this line The new aggregator redefines the __call__ method. It will output a single string, corresponding to the markdown-formatted output. That's it! You can use this new aggregator with the API: from edspdf import Pipeline from markdown_aggregator import MarkdownAggregator # (1) model = Pipeline () # will extract text lines from a document model . add_pipe ( \"pdfminer-extractor\" , config = dict ( extract_style = False , ), ) # classify everything inside the `body` bounding box as `body` model . add_pipe ( \"mask-classifier\" , config = dict ( body = { \"x0\" : 0.1 , \"y0\" : 0.1 , \"x1\" : 0.9 , \"y1\" : 0.9 }) ) # aggregates the lines together to re-create the original text model . add_pipe ( \"markdown-aggregator\" ) We're importing the aggregator that we just defined. It all works relatively smoothly!","title":"Developing the new aggregator"},{"location":"recipes/extension/#making-the-aggregator-discoverable","text":"Now, how can we instantiate the pipeline using the configuration system? The registry needs to be aware of the new function, but we shouldn't have to import mardown_aggregator.py just so that the module is registered as a side-effect... Catalogue solves this problem by using Python entry points . pyproject.toml (Poetry) setup.py [tool.poetry.plugins.\"edspdf_factory\"] \"markdown-aggregator\" = \"markdown_aggregator:MarkdownAggregator\" from setuptools import setup setup ( name = \"edspdf-markdown-aggregator\" , entry_points = { \"edspdf_factory\" : [ \"markdown-aggregator = markdown_aggregator:MarkdownAggregator\" ] }, ) By declaring the new aggregator as an entrypoint, it will become discoverable by EDS-PDF as long as it is installed in your environment!","title":"Making the aggregator discoverable"},{"location":"recipes/rule-based/","text":"Rule-based extraction Let's create a rule-based extractor for PDF documents. Note This pipeline will likely perform poorly as soon as your PDF documents come in varied forms. In that case, even a very simple trained pipeline may give you a substantial performance boost (see next section ). First, download this example PDF . We will use the following configuration: config.cfg [pipeline] components = [\"extractor\", \"classifier\", \"aggregator\"] components_config = ${components} [components.extractor] @factory = \"pdfminer-extractor\" # (2) extract_style = true [components.classifier] @factory = \"mask-classifier\" # (3) x0 = 0.2 x1 = 0.9 y0 = 0.3 y1 = 0.6 threshold = 0.1 [components.aggregator] @factory = \"styled-aggregator\" # (4) This is the top-level object, which organises the entire extraction process. Here we use the provided text-based extractor, based on the PDFMiner library This is where we define the rule-based classifier. Here, we use a \"mask\", meaning that every text bloc that falls within the boundaries will be assigned the body label, everything else will be tagged as pollution. This aggregator returns a tuple of dictionaries. The first contains compiled text for each label, the second exports their style. Save the configuration as config.cfg and run the following snippet: from edspdf import Config from pathlib import Path model = Config . from_disk ( \"config.cfg\" ) . resolve ()[ \"pipeline\" ] # (1) # Get a PDF pdf = Path ( \"letter.pdf\" ) . read_bytes () texts , styles = model ( pdf ) This code will output the following results: Visualisation Extracted Text Extracted Style Cher Pr ABC, Cher DEF, Nous souhaitons remercier le CSE pour son avis favorable quant \u00e0 l\u2019acc\u00e8s aux donn\u00e9es de l\u2019Entrep\u00f4t de Donn\u00e9es de Sant\u00e9 du projet n\u00b0 XXXX. Nous avons bien pris connaissance des conditions requises pour cet avis favorable, c\u2019est pourquoi nous nous engageons par la pr\u00e9sente \u00e0 : \u2022 Informer individuellement les patients concern\u00e9s par la recherche, admis \u00e0 l'AP-HP avant juillet 2017, sortis vivants, et non r\u00e9admis depuis. \u2022 Effectuer une demande d'autorisation \u00e0 la CNIL en cas d'appariement avec d\u2019autres cohortes. Bien cordialement, The start and end columns refer to the character indices within the extracted text. italic bold fontname start end False False BCDFEE+Calibri 0 22 False False BCDFEE+Calibri 24 90 False False BCDHEE+Calibri 90 91 False False BCDFEE+Calibri 91 111 False False BCDFEE+Calibri 112 113 False False BCDHEE+Calibri 113 114 False False BCDFEE+Calibri 114 161 False False BCDFEE+Calibri 163 247 False False BCDHEE+Calibri 247 248 False False BCDFEE+Calibri 248 251 False False BCDFEE+Calibri 252 300 False False SymbolMT 302 303 False False BCDFEE+Calibri 304 386 False False BCDFEE+Calibri 387 445 False False SymbolMT 447 448 False False BCDFEE+Calibri 449 523 False False BCDHEE+Calibri 523 524 False False BCDFEE+Calibri 524 530 False False BCDFEE+Calibri 531 540 False False BCDFEE+Calibri 542 560","title":"Rule-based extraction"},{"location":"recipes/rule-based/#rule-based-extraction","text":"Let's create a rule-based extractor for PDF documents. Note This pipeline will likely perform poorly as soon as your PDF documents come in varied forms. In that case, even a very simple trained pipeline may give you a substantial performance boost (see next section ). First, download this example PDF . We will use the following configuration: config.cfg [pipeline] components = [\"extractor\", \"classifier\", \"aggregator\"] components_config = ${components} [components.extractor] @factory = \"pdfminer-extractor\" # (2) extract_style = true [components.classifier] @factory = \"mask-classifier\" # (3) x0 = 0.2 x1 = 0.9 y0 = 0.3 y1 = 0.6 threshold = 0.1 [components.aggregator] @factory = \"styled-aggregator\" # (4) This is the top-level object, which organises the entire extraction process. Here we use the provided text-based extractor, based on the PDFMiner library This is where we define the rule-based classifier. Here, we use a \"mask\", meaning that every text bloc that falls within the boundaries will be assigned the body label, everything else will be tagged as pollution. This aggregator returns a tuple of dictionaries. The first contains compiled text for each label, the second exports their style. Save the configuration as config.cfg and run the following snippet: from edspdf import Config from pathlib import Path model = Config . from_disk ( \"config.cfg\" ) . resolve ()[ \"pipeline\" ] # (1) # Get a PDF pdf = Path ( \"letter.pdf\" ) . read_bytes () texts , styles = model ( pdf ) This code will output the following results: Visualisation Extracted Text Extracted Style Cher Pr ABC, Cher DEF, Nous souhaitons remercier le CSE pour son avis favorable quant \u00e0 l\u2019acc\u00e8s aux donn\u00e9es de l\u2019Entrep\u00f4t de Donn\u00e9es de Sant\u00e9 du projet n\u00b0 XXXX. Nous avons bien pris connaissance des conditions requises pour cet avis favorable, c\u2019est pourquoi nous nous engageons par la pr\u00e9sente \u00e0 : \u2022 Informer individuellement les patients concern\u00e9s par la recherche, admis \u00e0 l'AP-HP avant juillet 2017, sortis vivants, et non r\u00e9admis depuis. \u2022 Effectuer une demande d'autorisation \u00e0 la CNIL en cas d'appariement avec d\u2019autres cohortes. Bien cordialement, The start and end columns refer to the character indices within the extracted text. italic bold fontname start end False False BCDFEE+Calibri 0 22 False False BCDFEE+Calibri 24 90 False False BCDHEE+Calibri 90 91 False False BCDFEE+Calibri 91 111 False False BCDFEE+Calibri 112 113 False False BCDHEE+Calibri 113 114 False False BCDFEE+Calibri 114 161 False False BCDFEE+Calibri 163 247 False False BCDHEE+Calibri 247 248 False False BCDFEE+Calibri 248 251 False False BCDFEE+Calibri 252 300 False False SymbolMT 302 303 False False BCDFEE+Calibri 304 386 False False BCDFEE+Calibri 387 445 False False SymbolMT 447 448 False False BCDFEE+Calibri 449 523 False False BCDHEE+Calibri 523 524 False False BCDFEE+Calibri 524 530 False False BCDFEE+Calibri 531 540 False False BCDFEE+Calibri 542 560","title":"Rule-based extraction"},{"location":"recipes/training/","text":"Training a Pipeline In this chapter, we'll see how we can train a deep-learning based classifier to better classify the lines of the document and extract texts from the document. The architecture of the trainable classifier of this recipe is described in the following figure: Step-by-step walkthrough Training supervised models consists in feeding batches of samples taken from a training corpus to a model instantiated from a given architecture and optimizing the learnable weights of the model to decrease a given loss. The process of training a pipeline with EDS-PDF is as follows: We first start by seeding the random states and instantiating a new trainable pipeline from edspdf import Pipeline from edspdf.utils.random import set_seed set_seed ( 42 ) model = Pipeline () model . add_pipe ( \"pdfminer-extractor\" , name = \"extractor\" ) model . add_pipe ( \"deep-classifier\" , name = \"classifier\" , config = { \"embedding\" : { \"@factory\" : \"box-embedding\" , \"size\" : 72 , \"dropout_p\" : 0.1 , \"text_encoder\" : { \"@factory\" : \"box-text-embedding\" , \"pooler\" : { \"@factory\" : \"cnn-pooler\" , \"out_channels\" : 64 , \"kernel_sizes\" : ( 3 , 4 , 5 ), }, }, \"layout_encoder\" : { \"@factory\" : \"box-layout-embedding\" , \"n_positions\" : 64 , \"x_mode\" : \"sin\" , \"y_mode\" : \"sin\" , \"w_mode\" : \"sin\" , \"h_mode\" : \"sin\" , }, }, }, ) model . add_pipe ( \"simple-aggregator\" , name = \"aggregator\" ) We then load and adapt (i.e., convert into PDFDoc) the training and validation dataset, which is often a combination of JSON and PDF files. The recommended way of doing this is to make a Python generator of PDFDoc objects. train_docs = list ( segmentation_adapter ( train_path )( model )) val_docs = list ( segmentation_adapter ( val_path )( model )) We initialize the missing or incomplete components attributes (such as vocabularies) with the training dataset model . initialize ( train_docs ) The training dataset is then preprocessed into features. The resulting preprocessed dataset is then wrapped into a pytorch DataLoader to be fed to the model during the training loop with the model's own collate method. preprocessed = model . preprocess_many ( train_docs , supervision = True ) dataloader = DataLoader ( preprocessed , batch_size = 4 , shuffle = True , collate_fn = model . collate ) We instantiate an optimizer and start the training loop from itertools import chain , repeat optimizer = torch . optim . AdamW ( params = model . parameters (), lr = 3e-4 , ) # We will loop over the dataloader iterator = chain . from_iterable ( repeat ( dataloader )) for step in tqdm ( range ( max_steps ), \"Training model\" , leave = True ): batch = next ( iterator ) optimizer . zero_grad () The trainable components are fed the collated batches from the dataloader with the TrainableComponent.module_forward methods to compute the losses. Since outputs of shared subcomponents are cached, we must not forget to empty the cache at every step. The training loop is otherwise carried in a similar fashion to a standard pytorch training loop model . reset_cache () loss = torch . zeros ((), device = \"cpu\" ) for component in model . trainable_components : output = component . module_forward ( batch [ component . name ], supervision = True , ) loss += output [ \"loss\" ] loss . backward () optimizer . step () Finally, the model is evaluated on the validation dataset at regular intervals and saved at the end of the training if ( step % 100 ) == 0 : print ( model . score ( val_docs )) torch . save ( model , \"model.pt\" ) Adapting a dataset The first step of training a pipeline is to adapt the dataset to the pipeline. This is done by converting the dataset into a list of PDFDoc objects. from edspdf.utils.alignment import align_box_labels @registry . adapter . register ( \"my-segmentation-adapter\" ) def segmentation_adapter ( path : DirectoryPath , ) -> Callable [[ Pipeline ], Generator [ PDFDoc , Any , None ]]: def adapt_to ( model ): for anns_filepath in sorted ( Path ( path ) . glob ( \"*.json\" )): pdf_filepath = str ( anns_filepath ) . replace ( \".json\" , \".pdf\" ) with open ( anns_filepath ) as f : sample = json . load ( f ) pdf = Path ( pdf_filepath ) . read_bytes () if len ( sample [ \"annotations\" ]) == 0 : continue doc = model . components . extractor ( pdf ) doc . id = pdf_filepath . split ( \".\" )[ 0 ] . split ( \"/\" )[ - 1 ] doc . lines = [ line for page in sorted ( set ( b . page for b in doc . lines )) for line in align_box_labels ( src_boxes = [ Box ( page = b [ \"page\" ], x0 = b [ \"x0\" ], x1 = b [ \"x1\" ], y0 = b [ \"y0\" ], y1 = b [ \"y1\" ], label = b [ \"label\" ], ) for b in sample [ \"annotations\" ] if b [ \"page\" ] == page ], dst_boxes = doc . lines , pollution_label = None , ) if line . text == \"\" or line . label is not None ] yield doc return adapt_to Full example Let's wrap the training code in a function, and make it callable from the command line ! train.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 import itertools from pydantic import DirectoryPath from pathlib import Path import json import torch from torch.utils.data import DataLoader from tqdm import tqdm from edspdf import Pipeline , Cli , registry from edspdf.models import Box from edspdf.utils.random import set_seed from edspdf.utils.alignment import align_box_labels app = Cli ( pretty_exceptions_show_locals = False ) @app . command ( name = \"train\" ) def train_my_model ( seed : int = 42 , train_path : DirectoryPath = \"data/train\" , val_path : DirectoryPath = \"data/val\" , max_steps : int = 1000 , batch_size : int = 4 , lr : float = 3e-4 , ): set_seed ( seed ) # We define the model model = Pipeline () model . add_pipe ( \"pdfminer-extractor\" , name = \"extractor\" ) model . add_pipe ( \"deep-classifier\" , name = \"classifier\" , config = { \"embedding\" : { \"@factory\" : \"box-embedding\" , \"size\" : 72 , \"dropout_p\" : 0.1 , \"text_encoder\" : { \"@factory\" : \"box-text-embedding\" , \"pooler\" : { \"@factory\" : \"cnn-pooler\" , \"out_channels\" : 64 , \"kernel_sizes\" : ( 3 , 4 , 5 )}, }, \"layout_encoder\" : { \"@factory\" : \"box-layout-embedding\" , \"n_positions\" : 64 , \"x_mode\" : \"sin\" , \"y_mode\" : \"sin\" , \"w_mode\" : \"sin\" , \"h_mode\" : \"sin\" , }, }, }) model . add_pipe ( \"simple-aggregator\" , name = \"aggregator\" ) # Loading and adapting the training and validation data train_docs = list ( segmentation_adapter ( train_path )( model )) val_docs = list ( segmentation_adapter ( val_path )( model )) # Taking the first `initialization_subset` samples to initialize the model model . initialize ( train_docs ) # Preprocessing the training dataset into a dataloader preprocessed = model . preprocess_many ( train_docs , supervision = True ), dataloader = DataLoader ( preprocessed , batch_size = batch_size , collate_fn = model . collate , shuffle = True , ) optimizer = torch . optim . AdamW ( params = model . parameters (), lr = lr , ) # We will loop over the dataloader iterator = itertools . chain . from_iterable ( itertools . repeat ( dataloader )) for step in tqdm ( range ( max_steps ), \"Training model\" , leave = True ): batch = next ( iterator ) optimizer . zero_grad () model . reset_cache () loss = torch . zeros ((), device = 'cpu' ) for component in model . trainable_components : output = component . module_forward ( batch [ component . name ], supervision = True , ) loss += output [ \"loss\" ] loss . backward () optimizer . step () if ( step % 100 ) == 0 : print ( model . score ( val_docs )) torch . save ( model , \"model.pt\" ) return model @registry . adapter . register ( \"my-segmentation-adapter\" ) def segmentation_adapter ( path : str , ): def adapt_to ( model ): for anns_filepath in sorted ( Path ( path ) . glob ( \"*.json\" )): pdf_filepath = str ( anns_filepath ) . replace ( \".json\" , \".pdf\" ) with open ( anns_filepath ) as f : sample = json . load ( f ) pdf = Path ( pdf_filepath ) . read_bytes () if len ( sample [ \"annotations\" ]) == 0 : continue doc = model . components . extractor ( pdf ) doc . id = pdf_filepath . split ( \".\" )[ 0 ] . split ( \"/\" )[ - 1 ] doc . lines = [ line for page in sorted ( set ( b . page for b in doc . lines )) for line in align_box_labels ( src_boxes = [ Box ( page = b [ \"page\" ], x0 = b [ \"x0\" ], x1 = b [ \"x1\" ], y0 = b [ \"y0\" ], y1 = b [ \"y1\" ], label = b [ \"label\" ]) for b in sample [ \"annotations\" ] if b [ \"page\" ] == page ], dst_boxes = doc . lines , pollution_label = None , ) if line . text == \"\" or line . label is not None ] yield doc return adapt_to if __name__ == \"__main__\" : app () python train.py --seed 42 At the end of the training, the pipeline is ready to use (with the .pipe method) since every trained component of the pipeline is self-sufficient, ie contains the preprocessing, inference and postprocessing code required to run it. Configuration To decouple the configuration and the code of our training script, let's define a configuration file where we will describe both our training parameters and the pipeline. config.cfg # This is this equivalent of the API-based declaration at the beginning of the tutorial [pipeline] components = [\"extractor\",\"classifier\"] components_config = ${components} [components] [components.extractor] @factory = pdfminer-extractor [components.classifier] @factory = deep-classifier [components.classifier.embedding] @factory = box-embedding size = 72 dropout_p = 0.1 [components.classifier.embedding.text_encoder] @factory = \"box-text-embedding\" [components.classifier.embedding.text_encoder.pooler] @factory = \"cnn-pooler\" out_channels = 64 kernel_sizes = [3,4,5] [components.classifier.embedding.layout_encoder] @factory = \"box-layout-embedding\" n_positions = 64 x_mode = sin y_mode = sin w_mode = sin h_mode = sin # This is were we define the training script parameters # the \"train\" section refers to the name of the command in the training script [train] pipeline = ${pipeline} train_data = {\"@adapter\": \"my-segmentation-adapter\", \"path\": \"data/train\"} val_data = {\"@adapter\": \"my-segmentation-adapter\", \"path\": \"data/val\"} max_steps = 1000 seed = 42 lr = 3e-4 batch_size = 4 and update our training script to use the pipeline and the data adapters defined in the configuration file instead of the Python declaration : @app.command(name=\"train\") def train_my_model( + pipeline: Pipeline, + train_path: DirectoryPath = \"data/train\", - train_data: Callable = segmentation_adapter(\"data/train\"), + val_path: DirectoryPath = \"data/val\", - val_data: Callable = segmentation_adapter(\"data/val\"), seed: int = 42, max_steps: int = 1000, batch_size: int = 4, lr: float = 3e-4, ): set_seed(seed) - # We define the model - model = Pipeline() - model.add_pipe(\"pdfminer-extractor\", name=\"extractor\") - model.add_pipe(\"deep-classifier\", name=\"classifier\", config={ - \"embedding\": { - \"@factory\": \"box-embedding\", - \"size\": 72, - \"dropout_p\": 0.1, - \"text_encoder\": { - \"@factory\": \"box-text-embedding\", - \"pooler\": {\"@factory\": \"cnn-pooler\", \"out_channels\": 64, \"kernel_sizes\": (3, 4, 5)} - }, - \"layout_encoder\": { - \"@factory\": \"box-layout-embedding\", - \"n_positions\": 64, \"x_mode\": \"sin\", \"y_mode\": \"sin\", \"w_mode\": \"sin\", \"h_mode\": \"sin\", - }, - }, - }) - model.add_pipe(\"simple-aggregator\", name=\"aggregator\") # Loading and adapting the training and validation data - train_docs = list(segmentation_adapter(train_path)(model)) + train_docs = list(train_data(model)) - val_docs = list(segmentation_adapter(val_path)(model)) + val_docs = list(val_data(model)) # Taking the first `initialization_subset` samples to initialize the model ... That's it ! We can now call the training script with the configuration file as a parameter, and override some of its defaults values: python train.py --config config.cfg --seed 43","title":"Training a Pipeline"},{"location":"recipes/training/#training-a-pipeline","text":"In this chapter, we'll see how we can train a deep-learning based classifier to better classify the lines of the document and extract texts from the document. The architecture of the trainable classifier of this recipe is described in the following figure:","title":"Training a Pipeline"},{"location":"recipes/training/#step-by-step-walkthrough","text":"Training supervised models consists in feeding batches of samples taken from a training corpus to a model instantiated from a given architecture and optimizing the learnable weights of the model to decrease a given loss. The process of training a pipeline with EDS-PDF is as follows: We first start by seeding the random states and instantiating a new trainable pipeline from edspdf import Pipeline from edspdf.utils.random import set_seed set_seed ( 42 ) model = Pipeline () model . add_pipe ( \"pdfminer-extractor\" , name = \"extractor\" ) model . add_pipe ( \"deep-classifier\" , name = \"classifier\" , config = { \"embedding\" : { \"@factory\" : \"box-embedding\" , \"size\" : 72 , \"dropout_p\" : 0.1 , \"text_encoder\" : { \"@factory\" : \"box-text-embedding\" , \"pooler\" : { \"@factory\" : \"cnn-pooler\" , \"out_channels\" : 64 , \"kernel_sizes\" : ( 3 , 4 , 5 ), }, }, \"layout_encoder\" : { \"@factory\" : \"box-layout-embedding\" , \"n_positions\" : 64 , \"x_mode\" : \"sin\" , \"y_mode\" : \"sin\" , \"w_mode\" : \"sin\" , \"h_mode\" : \"sin\" , }, }, }, ) model . add_pipe ( \"simple-aggregator\" , name = \"aggregator\" ) We then load and adapt (i.e., convert into PDFDoc) the training and validation dataset, which is often a combination of JSON and PDF files. The recommended way of doing this is to make a Python generator of PDFDoc objects. train_docs = list ( segmentation_adapter ( train_path )( model )) val_docs = list ( segmentation_adapter ( val_path )( model )) We initialize the missing or incomplete components attributes (such as vocabularies) with the training dataset model . initialize ( train_docs ) The training dataset is then preprocessed into features. The resulting preprocessed dataset is then wrapped into a pytorch DataLoader to be fed to the model during the training loop with the model's own collate method. preprocessed = model . preprocess_many ( train_docs , supervision = True ) dataloader = DataLoader ( preprocessed , batch_size = 4 , shuffle = True , collate_fn = model . collate ) We instantiate an optimizer and start the training loop from itertools import chain , repeat optimizer = torch . optim . AdamW ( params = model . parameters (), lr = 3e-4 , ) # We will loop over the dataloader iterator = chain . from_iterable ( repeat ( dataloader )) for step in tqdm ( range ( max_steps ), \"Training model\" , leave = True ): batch = next ( iterator ) optimizer . zero_grad () The trainable components are fed the collated batches from the dataloader with the TrainableComponent.module_forward methods to compute the losses. Since outputs of shared subcomponents are cached, we must not forget to empty the cache at every step. The training loop is otherwise carried in a similar fashion to a standard pytorch training loop model . reset_cache () loss = torch . zeros ((), device = \"cpu\" ) for component in model . trainable_components : output = component . module_forward ( batch [ component . name ], supervision = True , ) loss += output [ \"loss\" ] loss . backward () optimizer . step () Finally, the model is evaluated on the validation dataset at regular intervals and saved at the end of the training if ( step % 100 ) == 0 : print ( model . score ( val_docs )) torch . save ( model , \"model.pt\" )","title":"Step-by-step walkthrough"},{"location":"recipes/training/#adapting-a-dataset","text":"The first step of training a pipeline is to adapt the dataset to the pipeline. This is done by converting the dataset into a list of PDFDoc objects. from edspdf.utils.alignment import align_box_labels @registry . adapter . register ( \"my-segmentation-adapter\" ) def segmentation_adapter ( path : DirectoryPath , ) -> Callable [[ Pipeline ], Generator [ PDFDoc , Any , None ]]: def adapt_to ( model ): for anns_filepath in sorted ( Path ( path ) . glob ( \"*.json\" )): pdf_filepath = str ( anns_filepath ) . replace ( \".json\" , \".pdf\" ) with open ( anns_filepath ) as f : sample = json . load ( f ) pdf = Path ( pdf_filepath ) . read_bytes () if len ( sample [ \"annotations\" ]) == 0 : continue doc = model . components . extractor ( pdf ) doc . id = pdf_filepath . split ( \".\" )[ 0 ] . split ( \"/\" )[ - 1 ] doc . lines = [ line for page in sorted ( set ( b . page for b in doc . lines )) for line in align_box_labels ( src_boxes = [ Box ( page = b [ \"page\" ], x0 = b [ \"x0\" ], x1 = b [ \"x1\" ], y0 = b [ \"y0\" ], y1 = b [ \"y1\" ], label = b [ \"label\" ], ) for b in sample [ \"annotations\" ] if b [ \"page\" ] == page ], dst_boxes = doc . lines , pollution_label = None , ) if line . text == \"\" or line . label is not None ] yield doc return adapt_to","title":"Adapting a dataset"},{"location":"recipes/training/#full-example","text":"Let's wrap the training code in a function, and make it callable from the command line ! train.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 import itertools from pydantic import DirectoryPath from pathlib import Path import json import torch from torch.utils.data import DataLoader from tqdm import tqdm from edspdf import Pipeline , Cli , registry from edspdf.models import Box from edspdf.utils.random import set_seed from edspdf.utils.alignment import align_box_labels app = Cli ( pretty_exceptions_show_locals = False ) @app . command ( name = \"train\" ) def train_my_model ( seed : int = 42 , train_path : DirectoryPath = \"data/train\" , val_path : DirectoryPath = \"data/val\" , max_steps : int = 1000 , batch_size : int = 4 , lr : float = 3e-4 , ): set_seed ( seed ) # We define the model model = Pipeline () model . add_pipe ( \"pdfminer-extractor\" , name = \"extractor\" ) model . add_pipe ( \"deep-classifier\" , name = \"classifier\" , config = { \"embedding\" : { \"@factory\" : \"box-embedding\" , \"size\" : 72 , \"dropout_p\" : 0.1 , \"text_encoder\" : { \"@factory\" : \"box-text-embedding\" , \"pooler\" : { \"@factory\" : \"cnn-pooler\" , \"out_channels\" : 64 , \"kernel_sizes\" : ( 3 , 4 , 5 )}, }, \"layout_encoder\" : { \"@factory\" : \"box-layout-embedding\" , \"n_positions\" : 64 , \"x_mode\" : \"sin\" , \"y_mode\" : \"sin\" , \"w_mode\" : \"sin\" , \"h_mode\" : \"sin\" , }, }, }) model . add_pipe ( \"simple-aggregator\" , name = \"aggregator\" ) # Loading and adapting the training and validation data train_docs = list ( segmentation_adapter ( train_path )( model )) val_docs = list ( segmentation_adapter ( val_path )( model )) # Taking the first `initialization_subset` samples to initialize the model model . initialize ( train_docs ) # Preprocessing the training dataset into a dataloader preprocessed = model . preprocess_many ( train_docs , supervision = True ), dataloader = DataLoader ( preprocessed , batch_size = batch_size , collate_fn = model . collate , shuffle = True , ) optimizer = torch . optim . AdamW ( params = model . parameters (), lr = lr , ) # We will loop over the dataloader iterator = itertools . chain . from_iterable ( itertools . repeat ( dataloader )) for step in tqdm ( range ( max_steps ), \"Training model\" , leave = True ): batch = next ( iterator ) optimizer . zero_grad () model . reset_cache () loss = torch . zeros ((), device = 'cpu' ) for component in model . trainable_components : output = component . module_forward ( batch [ component . name ], supervision = True , ) loss += output [ \"loss\" ] loss . backward () optimizer . step () if ( step % 100 ) == 0 : print ( model . score ( val_docs )) torch . save ( model , \"model.pt\" ) return model @registry . adapter . register ( \"my-segmentation-adapter\" ) def segmentation_adapter ( path : str , ): def adapt_to ( model ): for anns_filepath in sorted ( Path ( path ) . glob ( \"*.json\" )): pdf_filepath = str ( anns_filepath ) . replace ( \".json\" , \".pdf\" ) with open ( anns_filepath ) as f : sample = json . load ( f ) pdf = Path ( pdf_filepath ) . read_bytes () if len ( sample [ \"annotations\" ]) == 0 : continue doc = model . components . extractor ( pdf ) doc . id = pdf_filepath . split ( \".\" )[ 0 ] . split ( \"/\" )[ - 1 ] doc . lines = [ line for page in sorted ( set ( b . page for b in doc . lines )) for line in align_box_labels ( src_boxes = [ Box ( page = b [ \"page\" ], x0 = b [ \"x0\" ], x1 = b [ \"x1\" ], y0 = b [ \"y0\" ], y1 = b [ \"y1\" ], label = b [ \"label\" ]) for b in sample [ \"annotations\" ] if b [ \"page\" ] == page ], dst_boxes = doc . lines , pollution_label = None , ) if line . text == \"\" or line . label is not None ] yield doc return adapt_to if __name__ == \"__main__\" : app () python train.py --seed 42 At the end of the training, the pipeline is ready to use (with the .pipe method) since every trained component of the pipeline is self-sufficient, ie contains the preprocessing, inference and postprocessing code required to run it.","title":"Full example"},{"location":"recipes/training/#configuration","text":"To decouple the configuration and the code of our training script, let's define a configuration file where we will describe both our training parameters and the pipeline. config.cfg # This is this equivalent of the API-based declaration at the beginning of the tutorial [pipeline] components = [\"extractor\",\"classifier\"] components_config = ${components} [components] [components.extractor] @factory = pdfminer-extractor [components.classifier] @factory = deep-classifier [components.classifier.embedding] @factory = box-embedding size = 72 dropout_p = 0.1 [components.classifier.embedding.text_encoder] @factory = \"box-text-embedding\" [components.classifier.embedding.text_encoder.pooler] @factory = \"cnn-pooler\" out_channels = 64 kernel_sizes = [3,4,5] [components.classifier.embedding.layout_encoder] @factory = \"box-layout-embedding\" n_positions = 64 x_mode = sin y_mode = sin w_mode = sin h_mode = sin # This is were we define the training script parameters # the \"train\" section refers to the name of the command in the training script [train] pipeline = ${pipeline} train_data = {\"@adapter\": \"my-segmentation-adapter\", \"path\": \"data/train\"} val_data = {\"@adapter\": \"my-segmentation-adapter\", \"path\": \"data/val\"} max_steps = 1000 seed = 42 lr = 3e-4 batch_size = 4 and update our training script to use the pipeline and the data adapters defined in the configuration file instead of the Python declaration : @app.command(name=\"train\") def train_my_model( + pipeline: Pipeline, + train_path: DirectoryPath = \"data/train\", - train_data: Callable = segmentation_adapter(\"data/train\"), + val_path: DirectoryPath = \"data/val\", - val_data: Callable = segmentation_adapter(\"data/val\"), seed: int = 42, max_steps: int = 1000, batch_size: int = 4, lr: float = 3e-4, ): set_seed(seed) - # We define the model - model = Pipeline() - model.add_pipe(\"pdfminer-extractor\", name=\"extractor\") - model.add_pipe(\"deep-classifier\", name=\"classifier\", config={ - \"embedding\": { - \"@factory\": \"box-embedding\", - \"size\": 72, - \"dropout_p\": 0.1, - \"text_encoder\": { - \"@factory\": \"box-text-embedding\", - \"pooler\": {\"@factory\": \"cnn-pooler\", \"out_channels\": 64, \"kernel_sizes\": (3, 4, 5)} - }, - \"layout_encoder\": { - \"@factory\": \"box-layout-embedding\", - \"n_positions\": 64, \"x_mode\": \"sin\", \"y_mode\": \"sin\", \"w_mode\": \"sin\", \"h_mode\": \"sin\", - }, - }, - }) - model.add_pipe(\"simple-aggregator\", name=\"aggregator\") # Loading and adapting the training and validation data - train_docs = list(segmentation_adapter(train_path)(model)) + train_docs = list(train_data(model)) - val_docs = list(segmentation_adapter(val_path)(model)) + val_docs = list(val_data(model)) # Taking the first `initialization_subset` samples to initialize the model ... That's it ! We can now call the training script with the configuration file as a parameter, and override some of its defaults values: python train.py --config config.cfg --seed 43","title":"Configuration"},{"location":"reference/","text":"edspdf","title":"`edspdf`"},{"location":"reference/#edspdf","text":"","title":"edspdf"},{"location":"reference/SUMMARY/","text":"edspdf cli component components aggregators simple styled classifiers deep_classifier dummy harmonized_classifier mask random extractors pdfminer config layers box_embedding box_layout_embedding box_layout_preprocessor box_text_embedding box_transformer cnn_pooler relative_attention sinusoidal_embedding vocabulary model models box doc style text_box pipeline recipes train registry utils alignment collections optimization random torch visualization annotations merge","title":"SUMMARY"},{"location":"reference/cli/","text":"edspdf.cli Cli Bases: Typer Custom Typer object that: - validates a command parameters before executing it - accepts a configuration file describing the parameters - automatically instantiates parameters given a dictionary when type hinted Source code in edspdf/cli.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Cli ( Typer ): \"\"\" Custom Typer object that: - validates a command parameters before executing it - accepts a configuration file describing the parameters - automatically instantiates parameters given a dictionary when type hinted \"\"\" def command ( # noqa self , name , * , cls : Optional [ Type [ TyperCommand ]] = None , context_settings : Optional [ Dict [ Any , Any ]] = None , help : Optional [ str ] = None , epilog : Optional [ str ] = None , short_help : Optional [ str ] = None , options_metavar : str = \"[OPTIONS]\" , add_help_option : bool = True , no_args_is_help : bool = False , hidden : bool = False , deprecated : bool = False , # Rich settings rich_help_panel : Union [ str , None ] = Default ( None ), ) -> Callable [[ CommandFunctionType ], CommandFunctionType ]: typer_command = super () . command ( name = name , cls = cls , help = help , epilog = epilog , short_help = short_help , options_metavar = options_metavar , add_help_option = add_help_option , no_args_is_help = no_args_is_help , hidden = hidden , deprecated = deprecated , rich_help_panel = rich_help_panel , context_settings = { ** ( context_settings or {}), \"ignore_unknown_options\" : True , \"allow_extra_args\" : True , }, ) def wrapper ( fn ): validated = validate_arguments ( fn ) @typer_command def command ( ctx : Context , config : Optional [ Path ] = None ): if config is not None : config = Config . from_str ( Path ( config ) . read_text (), resolve = False ) else : config = Config ({ name : {}}) for k , v in parse_overrides ( ctx . args ) . items (): if \".\" not in k : parts = ( name , k ) else : parts = k . split ( \".\" ) if ( parts [ 0 ] in validated . model . __fields__ and not parts [ 0 ] in config ): parts = ( name , * parts ) current = config if parts [ 0 ] not in current : raise Exception ( f \" { k } does not match any existing section in config\" ) for part in parts [: - 1 ]: current = current . setdefault ( part , Config ()) current [ parts [ - 1 ]] = v try : default_seed = validated . model . __fields__ . get ( \"seed\" ) seed = config . get ( name , {}) . get ( \"seed\" , default_seed ) if seed is not None : set_seed ( seed ) config = config . resolve () return validated ( ** config . get ( name , {})) except ValidationError as e : print ( \" \\x1b [ {} m {} \\x1b [0m\" . format ( \"38;5;1\" , \"Validation error\" )) print ( str ( e )) sys . exit ( 1 ) return validated return wrapper","title":"cli"},{"location":"reference/cli/#edspdfcli","text":"","title":"edspdf.cli"},{"location":"reference/cli/#edspdf.cli.Cli","text":"Bases: Typer Custom Typer object that: - validates a command parameters before executing it - accepts a configuration file describing the parameters - automatically instantiates parameters given a dictionary when type hinted Source code in edspdf/cli.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 class Cli ( Typer ): \"\"\" Custom Typer object that: - validates a command parameters before executing it - accepts a configuration file describing the parameters - automatically instantiates parameters given a dictionary when type hinted \"\"\" def command ( # noqa self , name , * , cls : Optional [ Type [ TyperCommand ]] = None , context_settings : Optional [ Dict [ Any , Any ]] = None , help : Optional [ str ] = None , epilog : Optional [ str ] = None , short_help : Optional [ str ] = None , options_metavar : str = \"[OPTIONS]\" , add_help_option : bool = True , no_args_is_help : bool = False , hidden : bool = False , deprecated : bool = False , # Rich settings rich_help_panel : Union [ str , None ] = Default ( None ), ) -> Callable [[ CommandFunctionType ], CommandFunctionType ]: typer_command = super () . command ( name = name , cls = cls , help = help , epilog = epilog , short_help = short_help , options_metavar = options_metavar , add_help_option = add_help_option , no_args_is_help = no_args_is_help , hidden = hidden , deprecated = deprecated , rich_help_panel = rich_help_panel , context_settings = { ** ( context_settings or {}), \"ignore_unknown_options\" : True , \"allow_extra_args\" : True , }, ) def wrapper ( fn ): validated = validate_arguments ( fn ) @typer_command def command ( ctx : Context , config : Optional [ Path ] = None ): if config is not None : config = Config . from_str ( Path ( config ) . read_text (), resolve = False ) else : config = Config ({ name : {}}) for k , v in parse_overrides ( ctx . args ) . items (): if \".\" not in k : parts = ( name , k ) else : parts = k . split ( \".\" ) if ( parts [ 0 ] in validated . model . __fields__ and not parts [ 0 ] in config ): parts = ( name , * parts ) current = config if parts [ 0 ] not in current : raise Exception ( f \" { k } does not match any existing section in config\" ) for part in parts [: - 1 ]: current = current . setdefault ( part , Config ()) current [ parts [ - 1 ]] = v try : default_seed = validated . model . __fields__ . get ( \"seed\" ) seed = config . get ( name , {}) . get ( \"seed\" , default_seed ) if seed is not None : set_seed ( seed ) config = config . resolve () return validated ( ** config . get ( name , {})) except ValidationError as e : print ( \" \\x1b [ {} m {} \\x1b [0m\" . format ( \"38;5;1\" , \"Validation error\" )) print ( str ( e )) sys . exit ( 1 ) return validated return wrapper","title":"Cli"},{"location":"reference/component/","text":"edspdf.component Component Bases: Generic [ InT , OutT ] Source code in edspdf/component.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class Component ( Generic [ InT , OutT ]): def __init__ ( self , scorer : Optional [ Scorer [ OutT ]] = None , ): self . name : Optional [ str ] = None self . scorer = scorer self . needs_training = False self . initialized = False def reset_cache ( self , cache : Optional [ CacheEnum ] = None ): pass def initialize ( self , gold_data : Iterable [ OutT ]): \"\"\" Initialize the missing properties of the component, such as its vocabulary, using the gold data. Parameters ---------- gold_data: Iterable[OutT] Gold data to use for initialization \"\"\" @abstractmethod def __call__ ( self , doc : InT ) -> OutT : \"\"\" Processes a single document Parameters ---------- doc: InT Document to process Returns ------- OutT Processed document \"\"\" def batch_process ( self , docs : Sequence [ InT ], refs = None ) -> Sequence [ OutT ]: return [ self ( doc ) for doc in docs ] def score ( self , pairs ): if self . scorer is None : return {} return self . scorer ( pairs ) def __repr__ ( self ): return \" {} ()\" . format ( self . __class__ . __name__ ) def pipe ( self , docs : Iterable [ InT ], batch_size = 1 ) -> Iterable [ OutT ]: \"\"\" Applies the component on a collection of documents. It is recommended to use the [`Pipeline.pipe`][edspdf.pipeline.Pipeline.pipe] method instead of this one to apply a pipeline on a collection of documents, to benefit from the caching of intermediate results and avoiding loading too many documents in memory at once. Parameters ---------- docs: Iterable[InT] Input docs batch_size: int Batch size to use when making batched to be process at once \"\"\" for batch in batchify ( docs , batch_size = batch_size ): self . reset_cache () yield from self . batch_process ( batch ) initialize ( gold_data ) Initialize the missing properties of the component, such as its vocabulary, using the gold data. PARAMETER DESCRIPTION gold_data Gold data to use for initialization TYPE: Iterable [ OutT ] Source code in edspdf/component.py 109 110 111 112 113 114 115 116 117 118 def initialize ( self , gold_data : Iterable [ OutT ]): \"\"\" Initialize the missing properties of the component, such as its vocabulary, using the gold data. Parameters ---------- gold_data: Iterable[OutT] Gold data to use for initialization \"\"\" __call__ ( doc ) abstractmethod Processes a single document PARAMETER DESCRIPTION doc Document to process TYPE: InT RETURNS DESCRIPTION OutT Processed document Source code in edspdf/component.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @abstractmethod def __call__ ( self , doc : InT ) -> OutT : \"\"\" Processes a single document Parameters ---------- doc: InT Document to process Returns ------- OutT Processed document \"\"\" pipe ( docs , batch_size = 1 ) Applies the component on a collection of documents. It is recommended to use the Pipeline.pipe method instead of this one to apply a pipeline on a collection of documents, to benefit from the caching of intermediate results and avoiding loading too many documents in memory at once. PARAMETER DESCRIPTION docs Input docs TYPE: Iterable [ InT ] batch_size Batch size to use when making batched to be process at once DEFAULT: 1 Source code in edspdf/component.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def pipe ( self , docs : Iterable [ InT ], batch_size = 1 ) -> Iterable [ OutT ]: \"\"\" Applies the component on a collection of documents. It is recommended to use the [`Pipeline.pipe`][edspdf.pipeline.Pipeline.pipe] method instead of this one to apply a pipeline on a collection of documents, to benefit from the caching of intermediate results and avoiding loading too many documents in memory at once. Parameters ---------- docs: Iterable[InT] Input docs batch_size: int Batch size to use when making batched to be process at once \"\"\" for batch in batchify ( docs , batch_size = batch_size ): self . reset_cache () yield from self . batch_process ( batch ) Module Bases: torch . nn . Module , Generic [ InT , BatchT ] Base class for all EDS-PDF modules. This class is an extension of Pytorch's torch.nn.Module class. It adds a few methods to handle preprocessing and collating features, as well as caching intermediate results for components that share a common subcomponent. Source code in edspdf/component.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 class Module ( torch . nn . Module , Generic [ InT , BatchT ], metaclass = ModuleMeta ): \"\"\" Base class for all EDS-PDF modules. This class is an extension of Pytorch's `torch.nn.Module` class. It adds a few methods to handle preprocessing and collating features, as well as caching intermediate results for components that share a common subcomponent. \"\"\" IS_MODULE = True def __init__ ( self ): super () . __init__ () self . _preprocess_cache = {} self . _collate_cache = {} self . _forward_cache = {} self . _do_cache = True @contextlib . contextmanager def no_cache ( self ): saved = self . enable_cache ( False ) yield self . enable_cache ( saved ) def initialize ( self , gold_data : Iterable [ InT ], ** kwargs ): \"\"\" Initialize the missing properties of the module, such as its vocabulary, using the gold data and the provided keyword arguments. Parameters ---------- gold_data: Iterable[InT] Gold data to use for initialization kwargs: Any Additional keyword arguments to use for initialization \"\"\" for name , value in kwargs . items (): if value is None : continue current_value = getattr ( self , name ) if current_value is not None and current_value != value : raise ValueError ( \"Cannot initialize module with different values for \" \"attribute ' {} ': {} != {} \" . format ( name , current_value , value ) ) setattr ( self , name , value ) def enable_cache ( self , do_cache ): saved = self . _do_cache self . _do_cache = do_cache for module in self . modules (): if isinstance ( module , Module ) and module is not self : module . enable_cache ( do_cache ) return saved @property def device ( self ): return next ( iter ( self . parameters ())) . device def reset_cache ( self , cache : Optional [ CacheEnum ] = None ): def clear ( module ): try : assert cache is None or cache in ( CacheEnum . preprocess , CacheEnum . collate , CacheEnum . forward , ) if cache is None or cache == CacheEnum . preprocess : module . _preprocess_cache . clear () if cache is None or cache == CacheEnum . collate : module . _collate_cache . clear () if cache is None or cache == CacheEnum . forward : module . _forward_cache . clear () except AttributeError : pass self . apply ( clear ) def preprocess ( self , doc : InT , supervision : bool = False ) -> Dict [ str , Any ]: \"\"\" Parameters ---------- doc: InT supervision: bool Returns ------- Dict[str, Any] \"\"\" return {} def collate ( self , batch : Dict [ str , Sequence [ Any ]], device : torch . device ) -> BatchT : \"\"\"Collate operation : should return some tensors\"\"\" return {} def forward ( self , * args , ** kwargs ) -> Dict [ str , Any ]: \"\"\"Forward pass of the torch module\"\"\" raise NotImplementedError () def module_forward ( self , * args , ** kwargs ): \"\"\" This is a wrapper around `torch.nn.Module.__call__` to avoid conflict with the [`Component.__call__`][edspdf.component.Component.__call__] method. \"\"\" return torch . nn . Module . __call__ ( self , * args , ** kwargs ) initialize ( gold_data , ** kwargs ) Initialize the missing properties of the module, such as its vocabulary, using the gold data and the provided keyword arguments. PARAMETER DESCRIPTION gold_data Gold data to use for initialization TYPE: Iterable [ InT ] kwargs Additional keyword arguments to use for initialization DEFAULT: {} Source code in edspdf/component.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def initialize ( self , gold_data : Iterable [ InT ], ** kwargs ): \"\"\" Initialize the missing properties of the module, such as its vocabulary, using the gold data and the provided keyword arguments. Parameters ---------- gold_data: Iterable[InT] Gold data to use for initialization kwargs: Any Additional keyword arguments to use for initialization \"\"\" for name , value in kwargs . items (): if value is None : continue current_value = getattr ( self , name ) if current_value is not None and current_value != value : raise ValueError ( \"Cannot initialize module with different values for \" \"attribute ' {} ': {} != {} \" . format ( name , current_value , value ) ) setattr ( self , name , value ) preprocess ( doc , supervision = False ) PARAMETER DESCRIPTION doc TYPE: InT supervision TYPE: bool DEFAULT: False RETURNS DESCRIPTION Dict [ str , Any ] Source code in edspdf/component.py 258 259 260 261 262 263 264 265 266 267 268 269 def preprocess ( self , doc : InT , supervision : bool = False ) -> Dict [ str , Any ]: \"\"\" Parameters ---------- doc: InT supervision: bool Returns ------- Dict[str, Any] \"\"\" return {} collate ( batch , device ) Collate operation : should return some tensors Source code in edspdf/component.py 271 272 273 def collate ( self , batch : Dict [ str , Sequence [ Any ]], device : torch . device ) -> BatchT : \"\"\"Collate operation : should return some tensors\"\"\" return {} forward ( * args , ** kwargs ) Forward pass of the torch module Source code in edspdf/component.py 275 276 277 def forward ( self , * args , ** kwargs ) -> Dict [ str , Any ]: \"\"\"Forward pass of the torch module\"\"\" raise NotImplementedError () module_forward ( * args , ** kwargs ) This is a wrapper around torch.nn.Module.__call__ to avoid conflict with the Component.__call__ method. Source code in edspdf/component.py 279 280 281 282 283 284 285 def module_forward ( self , * args , ** kwargs ): \"\"\" This is a wrapper around `torch.nn.Module.__call__` to avoid conflict with the [`Component.__call__`][edspdf.component.Component.__call__] method. \"\"\" return torch . nn . Module . __call__ ( self , * args , ** kwargs ) TrainableComponent Bases: Module [ InT , BatchT ] , Component [ InT , OutT ] A TrainableComponent is a Component that can be trained and inherits from both Module and Component . You can therefore use it either as a torch module inside a more complex neural network, or as a standalone component in a Pipeline . Source code in edspdf/component.py 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 class TrainableComponent ( Module [ InT , BatchT ], Component [ InT , OutT ]): \"\"\" A TrainableComponent is a Component that can be trained and inherits from both `Module` and `Component`. You can therefore use it either as a torch module inside a more complex neural network, or as a standalone component in a [Pipeline][edspdf.pipeline.Pipeline]. \"\"\" def __init__ ( self ): Module . __init__ ( self ) Component . __init__ ( self ) self . needs_training = True def __call__ ( self , doc : InT ) -> OutT : return next ( iter ( self . batch_process ([ doc ]))) def make_batch ( self , docs : Sequence [ InT ], supervision : bool = False ): batch = decompress_dict ( list ( batch_compress_dict ( [{ self . name : self . preprocess ( doc , supervision )} for doc in docs ] ) ) ) return batch def batch_process ( self , docs : Sequence [ InT ], refs = None ) -> Sequence [ OutT ]: with torch . no_grad (): batch = self . make_batch ( docs ) inputs = self . collate ( batch [ self . name ], device = self . device ) res = self . forward ( inputs ) docs = self . postprocess ( docs , res ) return docs def forward ( self , batch : BatchT , supervision = False ) -> Dict [ str , Any ]: return batch def postprocess ( self , docs : Sequence [ InT ], batch : Dict [ str , Any ]) -> Sequence [ OutT ]: return docs","title":"component"},{"location":"reference/component/#edspdfcomponent","text":"","title":"edspdf.component"},{"location":"reference/component/#edspdf.component.Component","text":"Bases: Generic [ InT , OutT ] Source code in edspdf/component.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 class Component ( Generic [ InT , OutT ]): def __init__ ( self , scorer : Optional [ Scorer [ OutT ]] = None , ): self . name : Optional [ str ] = None self . scorer = scorer self . needs_training = False self . initialized = False def reset_cache ( self , cache : Optional [ CacheEnum ] = None ): pass def initialize ( self , gold_data : Iterable [ OutT ]): \"\"\" Initialize the missing properties of the component, such as its vocabulary, using the gold data. Parameters ---------- gold_data: Iterable[OutT] Gold data to use for initialization \"\"\" @abstractmethod def __call__ ( self , doc : InT ) -> OutT : \"\"\" Processes a single document Parameters ---------- doc: InT Document to process Returns ------- OutT Processed document \"\"\" def batch_process ( self , docs : Sequence [ InT ], refs = None ) -> Sequence [ OutT ]: return [ self ( doc ) for doc in docs ] def score ( self , pairs ): if self . scorer is None : return {} return self . scorer ( pairs ) def __repr__ ( self ): return \" {} ()\" . format ( self . __class__ . __name__ ) def pipe ( self , docs : Iterable [ InT ], batch_size = 1 ) -> Iterable [ OutT ]: \"\"\" Applies the component on a collection of documents. It is recommended to use the [`Pipeline.pipe`][edspdf.pipeline.Pipeline.pipe] method instead of this one to apply a pipeline on a collection of documents, to benefit from the caching of intermediate results and avoiding loading too many documents in memory at once. Parameters ---------- docs: Iterable[InT] Input docs batch_size: int Batch size to use when making batched to be process at once \"\"\" for batch in batchify ( docs , batch_size = batch_size ): self . reset_cache () yield from self . batch_process ( batch )","title":"Component"},{"location":"reference/component/#edspdf.component.Component.initialize","text":"Initialize the missing properties of the component, such as its vocabulary, using the gold data. PARAMETER DESCRIPTION gold_data Gold data to use for initialization TYPE: Iterable [ OutT ] Source code in edspdf/component.py 109 110 111 112 113 114 115 116 117 118 def initialize ( self , gold_data : Iterable [ OutT ]): \"\"\" Initialize the missing properties of the component, such as its vocabulary, using the gold data. Parameters ---------- gold_data: Iterable[OutT] Gold data to use for initialization \"\"\"","title":"initialize()"},{"location":"reference/component/#edspdf.component.Component.__call__","text":"Processes a single document PARAMETER DESCRIPTION doc Document to process TYPE: InT RETURNS DESCRIPTION OutT Processed document Source code in edspdf/component.py 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 @abstractmethod def __call__ ( self , doc : InT ) -> OutT : \"\"\" Processes a single document Parameters ---------- doc: InT Document to process Returns ------- OutT Processed document \"\"\"","title":"__call__()"},{"location":"reference/component/#edspdf.component.Component.pipe","text":"Applies the component on a collection of documents. It is recommended to use the Pipeline.pipe method instead of this one to apply a pipeline on a collection of documents, to benefit from the caching of intermediate results and avoiding loading too many documents in memory at once. PARAMETER DESCRIPTION docs Input docs TYPE: Iterable [ InT ] batch_size Batch size to use when making batched to be process at once DEFAULT: 1 Source code in edspdf/component.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def pipe ( self , docs : Iterable [ InT ], batch_size = 1 ) -> Iterable [ OutT ]: \"\"\" Applies the component on a collection of documents. It is recommended to use the [`Pipeline.pipe`][edspdf.pipeline.Pipeline.pipe] method instead of this one to apply a pipeline on a collection of documents, to benefit from the caching of intermediate results and avoiding loading too many documents in memory at once. Parameters ---------- docs: Iterable[InT] Input docs batch_size: int Batch size to use when making batched to be process at once \"\"\" for batch in batchify ( docs , batch_size = batch_size ): self . reset_cache () yield from self . batch_process ( batch )","title":"pipe()"},{"location":"reference/component/#edspdf.component.Module","text":"Bases: torch . nn . Module , Generic [ InT , BatchT ] Base class for all EDS-PDF modules. This class is an extension of Pytorch's torch.nn.Module class. It adds a few methods to handle preprocessing and collating features, as well as caching intermediate results for components that share a common subcomponent. Source code in edspdf/component.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 class Module ( torch . nn . Module , Generic [ InT , BatchT ], metaclass = ModuleMeta ): \"\"\" Base class for all EDS-PDF modules. This class is an extension of Pytorch's `torch.nn.Module` class. It adds a few methods to handle preprocessing and collating features, as well as caching intermediate results for components that share a common subcomponent. \"\"\" IS_MODULE = True def __init__ ( self ): super () . __init__ () self . _preprocess_cache = {} self . _collate_cache = {} self . _forward_cache = {} self . _do_cache = True @contextlib . contextmanager def no_cache ( self ): saved = self . enable_cache ( False ) yield self . enable_cache ( saved ) def initialize ( self , gold_data : Iterable [ InT ], ** kwargs ): \"\"\" Initialize the missing properties of the module, such as its vocabulary, using the gold data and the provided keyword arguments. Parameters ---------- gold_data: Iterable[InT] Gold data to use for initialization kwargs: Any Additional keyword arguments to use for initialization \"\"\" for name , value in kwargs . items (): if value is None : continue current_value = getattr ( self , name ) if current_value is not None and current_value != value : raise ValueError ( \"Cannot initialize module with different values for \" \"attribute ' {} ': {} != {} \" . format ( name , current_value , value ) ) setattr ( self , name , value ) def enable_cache ( self , do_cache ): saved = self . _do_cache self . _do_cache = do_cache for module in self . modules (): if isinstance ( module , Module ) and module is not self : module . enable_cache ( do_cache ) return saved @property def device ( self ): return next ( iter ( self . parameters ())) . device def reset_cache ( self , cache : Optional [ CacheEnum ] = None ): def clear ( module ): try : assert cache is None or cache in ( CacheEnum . preprocess , CacheEnum . collate , CacheEnum . forward , ) if cache is None or cache == CacheEnum . preprocess : module . _preprocess_cache . clear () if cache is None or cache == CacheEnum . collate : module . _collate_cache . clear () if cache is None or cache == CacheEnum . forward : module . _forward_cache . clear () except AttributeError : pass self . apply ( clear ) def preprocess ( self , doc : InT , supervision : bool = False ) -> Dict [ str , Any ]: \"\"\" Parameters ---------- doc: InT supervision: bool Returns ------- Dict[str, Any] \"\"\" return {} def collate ( self , batch : Dict [ str , Sequence [ Any ]], device : torch . device ) -> BatchT : \"\"\"Collate operation : should return some tensors\"\"\" return {} def forward ( self , * args , ** kwargs ) -> Dict [ str , Any ]: \"\"\"Forward pass of the torch module\"\"\" raise NotImplementedError () def module_forward ( self , * args , ** kwargs ): \"\"\" This is a wrapper around `torch.nn.Module.__call__` to avoid conflict with the [`Component.__call__`][edspdf.component.Component.__call__] method. \"\"\" return torch . nn . Module . __call__ ( self , * args , ** kwargs )","title":"Module"},{"location":"reference/component/#edspdf.component.Module.initialize","text":"Initialize the missing properties of the module, such as its vocabulary, using the gold data and the provided keyword arguments. PARAMETER DESCRIPTION gold_data Gold data to use for initialization TYPE: Iterable [ InT ] kwargs Additional keyword arguments to use for initialization DEFAULT: {} Source code in edspdf/component.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 def initialize ( self , gold_data : Iterable [ InT ], ** kwargs ): \"\"\" Initialize the missing properties of the module, such as its vocabulary, using the gold data and the provided keyword arguments. Parameters ---------- gold_data: Iterable[InT] Gold data to use for initialization kwargs: Any Additional keyword arguments to use for initialization \"\"\" for name , value in kwargs . items (): if value is None : continue current_value = getattr ( self , name ) if current_value is not None and current_value != value : raise ValueError ( \"Cannot initialize module with different values for \" \"attribute ' {} ': {} != {} \" . format ( name , current_value , value ) ) setattr ( self , name , value )","title":"initialize()"},{"location":"reference/component/#edspdf.component.Module.preprocess","text":"PARAMETER DESCRIPTION doc TYPE: InT supervision TYPE: bool DEFAULT: False RETURNS DESCRIPTION Dict [ str , Any ] Source code in edspdf/component.py 258 259 260 261 262 263 264 265 266 267 268 269 def preprocess ( self , doc : InT , supervision : bool = False ) -> Dict [ str , Any ]: \"\"\" Parameters ---------- doc: InT supervision: bool Returns ------- Dict[str, Any] \"\"\" return {}","title":"preprocess()"},{"location":"reference/component/#edspdf.component.Module.collate","text":"Collate operation : should return some tensors Source code in edspdf/component.py 271 272 273 def collate ( self , batch : Dict [ str , Sequence [ Any ]], device : torch . device ) -> BatchT : \"\"\"Collate operation : should return some tensors\"\"\" return {}","title":"collate()"},{"location":"reference/component/#edspdf.component.Module.forward","text":"Forward pass of the torch module Source code in edspdf/component.py 275 276 277 def forward ( self , * args , ** kwargs ) -> Dict [ str , Any ]: \"\"\"Forward pass of the torch module\"\"\" raise NotImplementedError ()","title":"forward()"},{"location":"reference/component/#edspdf.component.Module.module_forward","text":"This is a wrapper around torch.nn.Module.__call__ to avoid conflict with the Component.__call__ method. Source code in edspdf/component.py 279 280 281 282 283 284 285 def module_forward ( self , * args , ** kwargs ): \"\"\" This is a wrapper around `torch.nn.Module.__call__` to avoid conflict with the [`Component.__call__`][edspdf.component.Component.__call__] method. \"\"\" return torch . nn . Module . __call__ ( self , * args , ** kwargs )","title":"module_forward()"},{"location":"reference/component/#edspdf.component.TrainableComponent","text":"Bases: Module [ InT , BatchT ] , Component [ InT , OutT ] A TrainableComponent is a Component that can be trained and inherits from both Module and Component . You can therefore use it either as a torch module inside a more complex neural network, or as a standalone component in a Pipeline . Source code in edspdf/component.py 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 class TrainableComponent ( Module [ InT , BatchT ], Component [ InT , OutT ]): \"\"\" A TrainableComponent is a Component that can be trained and inherits from both `Module` and `Component`. You can therefore use it either as a torch module inside a more complex neural network, or as a standalone component in a [Pipeline][edspdf.pipeline.Pipeline]. \"\"\" def __init__ ( self ): Module . __init__ ( self ) Component . __init__ ( self ) self . needs_training = True def __call__ ( self , doc : InT ) -> OutT : return next ( iter ( self . batch_process ([ doc ]))) def make_batch ( self , docs : Sequence [ InT ], supervision : bool = False ): batch = decompress_dict ( list ( batch_compress_dict ( [{ self . name : self . preprocess ( doc , supervision )} for doc in docs ] ) ) ) return batch def batch_process ( self , docs : Sequence [ InT ], refs = None ) -> Sequence [ OutT ]: with torch . no_grad (): batch = self . make_batch ( docs ) inputs = self . collate ( batch [ self . name ], device = self . device ) res = self . forward ( inputs ) docs = self . postprocess ( docs , res ) return docs def forward ( self , batch : BatchT , supervision = False ) -> Dict [ str , Any ]: return batch def postprocess ( self , docs : Sequence [ InT ], batch : Dict [ str , Any ]) -> Sequence [ OutT ]: return docs","title":"TrainableComponent"},{"location":"reference/config/","text":"edspdf.config Config Bases: dict Source code in edspdf/config.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 class Config ( dict ): def __init__ ( self , * args , ** kwargs ): if len ( args ) == 1 and isinstance ( args [ 0 ], dict ): assert len ( kwargs ) == 0 kwargs = args [ 0 ] path = kwargs . pop ( \"__path__\" , None ) kwargs = { key : Config ( value ) if isinstance ( value , dict ) and not isinstance ( value , Config ) else value for key , value in kwargs . items () } super () . __init__ ( ** kwargs ) self . __path__ = path @classmethod def from_str ( cls , s : str , resolve : bool = False ) -> \"Config\" : parser = ConfigParser () parser . optionxform = str parser . read_string ( s ) config = Config () for section in parser . sections (): parts = split_path ( section ) current = config for part in parts : if part not in current : current [ part ] = current = Config () else : current = current [ part ] current . clear () current . update ( { k : config_literal_eval ( v ) for k , v in parser . items ( section )} ) if resolve : return config . resolve () return config @classmethod def from_disk ( cls , path : Union [ str , Path ], resolve : bool = False ) -> \"Config\" : s = Path ( path ) . read_text () return cls . from_str ( s , resolve = resolve ) def to_disk ( self , path : Union [ str , Path ]): s = self . to_str () Path ( path ) . write_text ( s ) def serialize ( self ): \"\"\" Try to convert non-serializable objects using the RESOLVED object back to their original catalogue + params form Returns ------- Config \"\"\" refs = {} def rec ( o , path = ()): if o is None or isinstance ( o , ( str , int , float , bool , tuple , list , Reference ) ): return o if isinstance ( o , collections . Mapping ): serialized = { k : rec ( v , ( * path , k )) for k , v in o . items ()} if isinstance ( o , Config ): serialized = Config ( serialized ) serialized . __path__ = o . __path__ return serialized cfg = None try : cfg = o . cfg except AttributeError : try : cfg = RESOLVED [ o ] except KeyError : pass if cfg is not None : if id ( o ) in refs : return refs [ id ( o )] else : refs [ id ( o )] = Reference ( join_path ( path )) return rec ( cfg , path ) raise TypeError ( f \"Cannot dump { o !r} \" ) result = rec ( self ) return result def to_str ( self ): additional_sections = {} def rec ( o , path = ()): if isinstance ( o , collections . Mapping ): if isinstance ( o , Config ) and o . __path__ is not None : res = { k : rec ( v , ( * o . __path__ , k )) for k , v in o . items ()} current = additional_sections for part in o . __path__ [: - 1 ]: current = current . setdefault ( part , Config ()) current [ o . __path__ [ - 1 ]] = res return Reference ( join_path ( o . __path__ )) else : return { k : rec ( v , ( * path , k )) for k , v in o . items ()} return o prepared = flatten_sections ( rec ( self . serialize ())) prepared . update ( flatten_sections ( additional_sections )) parser = ConfigParser () parser . optionxform = str for section_name , section in prepared . items (): parser . add_section ( section_name ) parser [ section_name ] . update ( { k : config_literal_dump ( v ) for k , v in section . items ()} ) s = StringIO () parser . write ( s ) return s . getvalue () def resolve ( self , _path = (), leaves = None , deep = True ): from .registry import registry # local import because circular deps copy = Config ( ** self ) if leaves is None : leaves = {} missing = [] items = [( k , v ) for k , v in copy . items ()] if deep else [] last_count = len ( leaves ) while len ( items ): traced_missing_values = [] for key , value in items : try : if isinstance ( value , Config ): if ( * _path , key ) not in leaves : leaves [( * _path , key )] = value . resolve (( * _path , key ), leaves ) copy [ key ] = leaves [( * _path , key )] elif isinstance ( value , Reference ): try : leaves [( * _path , key )] = leaves [ tuple ( split_path ( value ))] except KeyError : raise MissingReference ([ value ]) else : copy [ key ] = leaves [( * _path , key )] except MissingReference as e : traced_missing_values . extend ( e . references ) missing . append (( key , value )) if len ( missing ) > 0 and len ( leaves ) <= last_count : raise MissingReference ( dedup ( traced_missing_values )) items = list ( missing ) last_count = len ( leaves ) missing = [] registries = [ ( key , value , registry . _catalogue [ key [ 1 :]]) for key , value in copy . items () if key . startswith ( \"@\" ) ] assert len ( registries ) <= 1 , ( f \"Cannot resolve using multiple \" f \"registries at { '.' . join ( _path ) } \" ) def patch_errors ( errors : Union [ Sequence [ ErrorWrapper ], ErrorWrapper ]): if isinstance ( errors , list ): res = [] for error in errors : res . append ( patch_errors ( error )) return res return ErrorWrapper ( errors . exc , ( * _path , * errors . loc_tuple ())) if len ( registries ) == 1 : params = dict ( copy ) params . pop ( registries [ 0 ][ 0 ]) fn = registries [ 0 ][ 2 ] . get ( registries [ 0 ][ 1 ]) try : resolved = fn ( ** params ) try : resolved . cfg except Exception : try : RESOLVED [ resolved ] = self except Exception : print ( f \"Could not store original config for { resolved } \" ) pass return resolved except ValidationError as e : raise ValidationError ( patch_errors ( e . raw_errors ), e . model ) return copy def merge ( self , * updates : Union [ Dict [ str , Any ], \"Config\" ], remove_extra : bool = False , ) -> \"Config\" : \"\"\" Deep merge two configs. Largely inspired from spaCy config merge function. Parameters ---------- updates: Union[Config, Dict] Configs to update the original config remove_extra: If true, restricts update to keys that existed in the original config Returns ------- The new config \"\"\" def deep_set ( current , path , val ): try : path = split_path ( path ) for part in path [: - 1 ]: current = ( current [ part ] if remove_extra else current . setdefault ( part , {}) ) except KeyError : return if path [ - 1 ] not in current and remove_extra : return current [ path [ - 1 ]] = val def rec ( old , new ): for key , new_val in list ( new . items ()): if \".\" in key : deep_set ( old , key , new_val ) continue if key not in old : if remove_extra : continue else : old [ key ] = new_val continue old_val = old [ key ] if isinstance ( old_val , dict ) and isinstance ( new_val , dict ): old_resolver = next (( k for k in old_val if k . startswith ( \"@\" )), None ) new_resolver = next (( k for k in new_val if k . startswith ( \"@\" )), None ) if ( new_resolver is not None and old_resolver is not None and ( old_resolver != new_resolver or old_val . get ( old_resolver ) != new_val . get ( new_resolver ) ) ): old [ key ] = new_val else : rec ( old [ key ], new_val ) else : old [ key ] = new_val return old config = deepcopy ( self ) for u in updates : u = deepcopy ( u ) rec ( config , u ) return Config ( ** config ) serialize () Try to convert non-serializable objects using the RESOLVED object back to their original catalogue + params form RETURNS DESCRIPTION Config Source code in edspdf/config.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 def serialize ( self ): \"\"\" Try to convert non-serializable objects using the RESOLVED object back to their original catalogue + params form Returns ------- Config \"\"\" refs = {} def rec ( o , path = ()): if o is None or isinstance ( o , ( str , int , float , bool , tuple , list , Reference ) ): return o if isinstance ( o , collections . Mapping ): serialized = { k : rec ( v , ( * path , k )) for k , v in o . items ()} if isinstance ( o , Config ): serialized = Config ( serialized ) serialized . __path__ = o . __path__ return serialized cfg = None try : cfg = o . cfg except AttributeError : try : cfg = RESOLVED [ o ] except KeyError : pass if cfg is not None : if id ( o ) in refs : return refs [ id ( o )] else : refs [ id ( o )] = Reference ( join_path ( path )) return rec ( cfg , path ) raise TypeError ( f \"Cannot dump { o !r} \" ) result = rec ( self ) return result merge ( * updates , remove_extra = False ) Deep merge two configs. Largely inspired from spaCy config merge function. PARAMETER DESCRIPTION updates Configs to update the original config TYPE: Union [ Dict [ str , Any ], Config ] DEFAULT: () remove_extra If true, restricts update to keys that existed in the original config TYPE: bool DEFAULT: False RETURNS DESCRIPTION The new config Source code in edspdf/config.py 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 def merge ( self , * updates : Union [ Dict [ str , Any ], \"Config\" ], remove_extra : bool = False , ) -> \"Config\" : \"\"\" Deep merge two configs. Largely inspired from spaCy config merge function. Parameters ---------- updates: Union[Config, Dict] Configs to update the original config remove_extra: If true, restricts update to keys that existed in the original config Returns ------- The new config \"\"\" def deep_set ( current , path , val ): try : path = split_path ( path ) for part in path [: - 1 ]: current = ( current [ part ] if remove_extra else current . setdefault ( part , {}) ) except KeyError : return if path [ - 1 ] not in current and remove_extra : return current [ path [ - 1 ]] = val def rec ( old , new ): for key , new_val in list ( new . items ()): if \".\" in key : deep_set ( old , key , new_val ) continue if key not in old : if remove_extra : continue else : old [ key ] = new_val continue old_val = old [ key ] if isinstance ( old_val , dict ) and isinstance ( new_val , dict ): old_resolver = next (( k for k in old_val if k . startswith ( \"@\" )), None ) new_resolver = next (( k for k in new_val if k . startswith ( \"@\" )), None ) if ( new_resolver is not None and old_resolver is not None and ( old_resolver != new_resolver or old_val . get ( old_resolver ) != new_val . get ( new_resolver ) ) ): old [ key ] = new_val else : rec ( old [ key ], new_val ) else : old [ key ] = new_val return old config = deepcopy ( self ) for u in updates : u = deepcopy ( u ) rec ( config , u ) return Config ( ** config ) resolve_non_dict ( model , values ) Iterates over the model fields and try to resolve the matching values if they are not type hinted as dictionaries. Source code in edspdf/config.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def resolve_non_dict ( model : pydantic . BaseModel , values : Dict [ str , Any ]): \"\"\" Iterates over the model fields and try to resolve the matching values if they are not type hinted as dictionaries. \"\"\" values = dict ( values ) for field in model . __fields__ . values (): if field . name not in values : continue if field . shape not in pydantic . fields . MAPPING_LIKE_SHAPES and isinstance ( values [ field . name ], dict ): values [ field . name ] = Config ( values [ field . name ]) . resolve ( deep = False ) return values validate_arguments ( func = None , * , config = None , save_params = None ) Decorator to validate the arguments passed to a function. PARAMETER DESCRIPTION func The function or class to call TYPE: Optional [ Callable ] DEFAULT: None config The validation configuration object TYPE: Dict DEFAULT: None save_params Should we save the function parameters TYPE: Optional [ Dict ] DEFAULT: None RETURNS DESCRIPTION Any Source code in edspdf/config.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def validate_arguments ( func : Optional [ Callable ] = None , * , config : Dict = None , save_params : Optional [ Dict ] = None , ) -> Any : \"\"\" Decorator to validate the arguments passed to a function. Parameters ---------- func: Callable The function or class to call config: Dict The validation configuration object save_params: bool Should we save the function parameters Returns ------- Any \"\"\" if config is None : config = {} config = { ** config , \"arbitrary_types_allowed\" : True } def validate ( _func : Callable ) -> Callable : if isinstance ( _func , type ): if hasattr ( _func , \"raw_function\" ): vd = pydantic . decorator . ValidatedFunction ( _func . raw_function , config ) else : vd = pydantic . decorator . ValidatedFunction ( _func . __init__ , config ) vd . model . __name__ = _func . __name__ vd . model . __fields__ [ \"self\" ] . required = False def __get_validators__ (): def validate ( value ): params = value if isinstance ( value , dict ): value = Config ( value ) . resolve ( deep = False ) if not isinstance ( value , dict ): return value m = vd . init_model_instance ( ** value ) d = { k : v for k , v in m . _iter () if k in m . __fields_set__ or m . __fields__ [ k ] . default_factory } var_kwargs = d . pop ( vd . v_kwargs_name , {}) resolved = _func ( ** d , ** var_kwargs ) if save_params is not None : RESOLVED [ resolved ] = { ** save_params , ** params } return resolved yield validate @wraps ( vd . raw_function ) def wrapper_function ( * args : Any , ** kwargs : Any ) -> Any : values = vd . build_values ( args , kwargs ) if save_params is not None : if set ( values . keys ()) & { ALT_V_ARGS , ALT_V_KWARGS , V_POSITIONAL_ONLY_NAME , V_DUPLICATE_KWARGS , \"args\" , \"kwargs\" , }: print ( \"VALUES\" , values . keys (), values [ \"kwargs\" ]) raise Exception ( f \" { func } must not have positional only args, \" f \"kwargs or duplicated kwargs\" ) params = dict ( values ) resolved = params . pop ( \"self\" ) RESOLVED [ resolved ] = { ** save_params , ** params } return vd . execute ( vd . model ( ** resolve_non_dict ( vd . model , values ))) _func . vd = vd # type: ignore # _func.validate = vd.init_model_instance # type: ignore _func . __get_validators__ = __get_validators__ # type: ignore _func . raw_function = vd . raw_function # type: ignore _func . model = vd . model # type: ignore _func . __init__ = wrapper_function return _func else : vd = pydantic . decorator . ValidatedFunction ( _func , config ) @wraps ( _func ) def wrapper_function ( * args : Any , ** kwargs : Any ) -> Any : values = vd . build_values ( args , kwargs ) resolved = vd . execute ( vd . model ( ** resolve_non_dict ( vd . model , values ))) if save_params is not None : if set ( values . keys ()) & { ALT_V_ARGS , ALT_V_KWARGS , V_POSITIONAL_ONLY_NAME , V_DUPLICATE_KWARGS , \"args\" , \"kwargs\" , }: raise Exception ( f \" { func } must not have positional only args, \" f \"kwargs or duplicated kwargs\" ) RESOLVED [ resolved ] = { ** save_params , ** values } return resolved wrapper_function . vd = vd # type: ignore wrapper_function . validate = vd . init_model_instance # type: ignore wrapper_function . raw_function = vd . raw_function # type: ignore wrapper_function . model = vd . model # type: ignore return wrapper_function if func : return validate ( func ) else : return validate","title":"config"},{"location":"reference/config/#edspdfconfig","text":"","title":"edspdf.config"},{"location":"reference/config/#edspdf.config.Config","text":"Bases: dict Source code in edspdf/config.py 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 class Config ( dict ): def __init__ ( self , * args , ** kwargs ): if len ( args ) == 1 and isinstance ( args [ 0 ], dict ): assert len ( kwargs ) == 0 kwargs = args [ 0 ] path = kwargs . pop ( \"__path__\" , None ) kwargs = { key : Config ( value ) if isinstance ( value , dict ) and not isinstance ( value , Config ) else value for key , value in kwargs . items () } super () . __init__ ( ** kwargs ) self . __path__ = path @classmethod def from_str ( cls , s : str , resolve : bool = False ) -> \"Config\" : parser = ConfigParser () parser . optionxform = str parser . read_string ( s ) config = Config () for section in parser . sections (): parts = split_path ( section ) current = config for part in parts : if part not in current : current [ part ] = current = Config () else : current = current [ part ] current . clear () current . update ( { k : config_literal_eval ( v ) for k , v in parser . items ( section )} ) if resolve : return config . resolve () return config @classmethod def from_disk ( cls , path : Union [ str , Path ], resolve : bool = False ) -> \"Config\" : s = Path ( path ) . read_text () return cls . from_str ( s , resolve = resolve ) def to_disk ( self , path : Union [ str , Path ]): s = self . to_str () Path ( path ) . write_text ( s ) def serialize ( self ): \"\"\" Try to convert non-serializable objects using the RESOLVED object back to their original catalogue + params form Returns ------- Config \"\"\" refs = {} def rec ( o , path = ()): if o is None or isinstance ( o , ( str , int , float , bool , tuple , list , Reference ) ): return o if isinstance ( o , collections . Mapping ): serialized = { k : rec ( v , ( * path , k )) for k , v in o . items ()} if isinstance ( o , Config ): serialized = Config ( serialized ) serialized . __path__ = o . __path__ return serialized cfg = None try : cfg = o . cfg except AttributeError : try : cfg = RESOLVED [ o ] except KeyError : pass if cfg is not None : if id ( o ) in refs : return refs [ id ( o )] else : refs [ id ( o )] = Reference ( join_path ( path )) return rec ( cfg , path ) raise TypeError ( f \"Cannot dump { o !r} \" ) result = rec ( self ) return result def to_str ( self ): additional_sections = {} def rec ( o , path = ()): if isinstance ( o , collections . Mapping ): if isinstance ( o , Config ) and o . __path__ is not None : res = { k : rec ( v , ( * o . __path__ , k )) for k , v in o . items ()} current = additional_sections for part in o . __path__ [: - 1 ]: current = current . setdefault ( part , Config ()) current [ o . __path__ [ - 1 ]] = res return Reference ( join_path ( o . __path__ )) else : return { k : rec ( v , ( * path , k )) for k , v in o . items ()} return o prepared = flatten_sections ( rec ( self . serialize ())) prepared . update ( flatten_sections ( additional_sections )) parser = ConfigParser () parser . optionxform = str for section_name , section in prepared . items (): parser . add_section ( section_name ) parser [ section_name ] . update ( { k : config_literal_dump ( v ) for k , v in section . items ()} ) s = StringIO () parser . write ( s ) return s . getvalue () def resolve ( self , _path = (), leaves = None , deep = True ): from .registry import registry # local import because circular deps copy = Config ( ** self ) if leaves is None : leaves = {} missing = [] items = [( k , v ) for k , v in copy . items ()] if deep else [] last_count = len ( leaves ) while len ( items ): traced_missing_values = [] for key , value in items : try : if isinstance ( value , Config ): if ( * _path , key ) not in leaves : leaves [( * _path , key )] = value . resolve (( * _path , key ), leaves ) copy [ key ] = leaves [( * _path , key )] elif isinstance ( value , Reference ): try : leaves [( * _path , key )] = leaves [ tuple ( split_path ( value ))] except KeyError : raise MissingReference ([ value ]) else : copy [ key ] = leaves [( * _path , key )] except MissingReference as e : traced_missing_values . extend ( e . references ) missing . append (( key , value )) if len ( missing ) > 0 and len ( leaves ) <= last_count : raise MissingReference ( dedup ( traced_missing_values )) items = list ( missing ) last_count = len ( leaves ) missing = [] registries = [ ( key , value , registry . _catalogue [ key [ 1 :]]) for key , value in copy . items () if key . startswith ( \"@\" ) ] assert len ( registries ) <= 1 , ( f \"Cannot resolve using multiple \" f \"registries at { '.' . join ( _path ) } \" ) def patch_errors ( errors : Union [ Sequence [ ErrorWrapper ], ErrorWrapper ]): if isinstance ( errors , list ): res = [] for error in errors : res . append ( patch_errors ( error )) return res return ErrorWrapper ( errors . exc , ( * _path , * errors . loc_tuple ())) if len ( registries ) == 1 : params = dict ( copy ) params . pop ( registries [ 0 ][ 0 ]) fn = registries [ 0 ][ 2 ] . get ( registries [ 0 ][ 1 ]) try : resolved = fn ( ** params ) try : resolved . cfg except Exception : try : RESOLVED [ resolved ] = self except Exception : print ( f \"Could not store original config for { resolved } \" ) pass return resolved except ValidationError as e : raise ValidationError ( patch_errors ( e . raw_errors ), e . model ) return copy def merge ( self , * updates : Union [ Dict [ str , Any ], \"Config\" ], remove_extra : bool = False , ) -> \"Config\" : \"\"\" Deep merge two configs. Largely inspired from spaCy config merge function. Parameters ---------- updates: Union[Config, Dict] Configs to update the original config remove_extra: If true, restricts update to keys that existed in the original config Returns ------- The new config \"\"\" def deep_set ( current , path , val ): try : path = split_path ( path ) for part in path [: - 1 ]: current = ( current [ part ] if remove_extra else current . setdefault ( part , {}) ) except KeyError : return if path [ - 1 ] not in current and remove_extra : return current [ path [ - 1 ]] = val def rec ( old , new ): for key , new_val in list ( new . items ()): if \".\" in key : deep_set ( old , key , new_val ) continue if key not in old : if remove_extra : continue else : old [ key ] = new_val continue old_val = old [ key ] if isinstance ( old_val , dict ) and isinstance ( new_val , dict ): old_resolver = next (( k for k in old_val if k . startswith ( \"@\" )), None ) new_resolver = next (( k for k in new_val if k . startswith ( \"@\" )), None ) if ( new_resolver is not None and old_resolver is not None and ( old_resolver != new_resolver or old_val . get ( old_resolver ) != new_val . get ( new_resolver ) ) ): old [ key ] = new_val else : rec ( old [ key ], new_val ) else : old [ key ] = new_val return old config = deepcopy ( self ) for u in updates : u = deepcopy ( u ) rec ( config , u ) return Config ( ** config )","title":"Config"},{"location":"reference/config/#edspdf.config.Config.serialize","text":"Try to convert non-serializable objects using the RESOLVED object back to their original catalogue + params form RETURNS DESCRIPTION Config Source code in edspdf/config.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 def serialize ( self ): \"\"\" Try to convert non-serializable objects using the RESOLVED object back to their original catalogue + params form Returns ------- Config \"\"\" refs = {} def rec ( o , path = ()): if o is None or isinstance ( o , ( str , int , float , bool , tuple , list , Reference ) ): return o if isinstance ( o , collections . Mapping ): serialized = { k : rec ( v , ( * path , k )) for k , v in o . items ()} if isinstance ( o , Config ): serialized = Config ( serialized ) serialized . __path__ = o . __path__ return serialized cfg = None try : cfg = o . cfg except AttributeError : try : cfg = RESOLVED [ o ] except KeyError : pass if cfg is not None : if id ( o ) in refs : return refs [ id ( o )] else : refs [ id ( o )] = Reference ( join_path ( path )) return rec ( cfg , path ) raise TypeError ( f \"Cannot dump { o !r} \" ) result = rec ( self ) return result","title":"serialize()"},{"location":"reference/config/#edspdf.config.Config.merge","text":"Deep merge two configs. Largely inspired from spaCy config merge function. PARAMETER DESCRIPTION updates Configs to update the original config TYPE: Union [ Dict [ str , Any ], Config ] DEFAULT: () remove_extra If true, restricts update to keys that existed in the original config TYPE: bool DEFAULT: False RETURNS DESCRIPTION The new config Source code in edspdf/config.py 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 def merge ( self , * updates : Union [ Dict [ str , Any ], \"Config\" ], remove_extra : bool = False , ) -> \"Config\" : \"\"\" Deep merge two configs. Largely inspired from spaCy config merge function. Parameters ---------- updates: Union[Config, Dict] Configs to update the original config remove_extra: If true, restricts update to keys that existed in the original config Returns ------- The new config \"\"\" def deep_set ( current , path , val ): try : path = split_path ( path ) for part in path [: - 1 ]: current = ( current [ part ] if remove_extra else current . setdefault ( part , {}) ) except KeyError : return if path [ - 1 ] not in current and remove_extra : return current [ path [ - 1 ]] = val def rec ( old , new ): for key , new_val in list ( new . items ()): if \".\" in key : deep_set ( old , key , new_val ) continue if key not in old : if remove_extra : continue else : old [ key ] = new_val continue old_val = old [ key ] if isinstance ( old_val , dict ) and isinstance ( new_val , dict ): old_resolver = next (( k for k in old_val if k . startswith ( \"@\" )), None ) new_resolver = next (( k for k in new_val if k . startswith ( \"@\" )), None ) if ( new_resolver is not None and old_resolver is not None and ( old_resolver != new_resolver or old_val . get ( old_resolver ) != new_val . get ( new_resolver ) ) ): old [ key ] = new_val else : rec ( old [ key ], new_val ) else : old [ key ] = new_val return old config = deepcopy ( self ) for u in updates : u = deepcopy ( u ) rec ( config , u ) return Config ( ** config )","title":"merge()"},{"location":"reference/config/#edspdf.config.resolve_non_dict","text":"Iterates over the model fields and try to resolve the matching values if they are not type hinted as dictionaries. Source code in edspdf/config.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def resolve_non_dict ( model : pydantic . BaseModel , values : Dict [ str , Any ]): \"\"\" Iterates over the model fields and try to resolve the matching values if they are not type hinted as dictionaries. \"\"\" values = dict ( values ) for field in model . __fields__ . values (): if field . name not in values : continue if field . shape not in pydantic . fields . MAPPING_LIKE_SHAPES and isinstance ( values [ field . name ], dict ): values [ field . name ] = Config ( values [ field . name ]) . resolve ( deep = False ) return values","title":"resolve_non_dict()"},{"location":"reference/config/#edspdf.config.validate_arguments","text":"Decorator to validate the arguments passed to a function. PARAMETER DESCRIPTION func The function or class to call TYPE: Optional [ Callable ] DEFAULT: None config The validation configuration object TYPE: Dict DEFAULT: None save_params Should we save the function parameters TYPE: Optional [ Dict ] DEFAULT: None RETURNS DESCRIPTION Any Source code in edspdf/config.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def validate_arguments ( func : Optional [ Callable ] = None , * , config : Dict = None , save_params : Optional [ Dict ] = None , ) -> Any : \"\"\" Decorator to validate the arguments passed to a function. Parameters ---------- func: Callable The function or class to call config: Dict The validation configuration object save_params: bool Should we save the function parameters Returns ------- Any \"\"\" if config is None : config = {} config = { ** config , \"arbitrary_types_allowed\" : True } def validate ( _func : Callable ) -> Callable : if isinstance ( _func , type ): if hasattr ( _func , \"raw_function\" ): vd = pydantic . decorator . ValidatedFunction ( _func . raw_function , config ) else : vd = pydantic . decorator . ValidatedFunction ( _func . __init__ , config ) vd . model . __name__ = _func . __name__ vd . model . __fields__ [ \"self\" ] . required = False def __get_validators__ (): def validate ( value ): params = value if isinstance ( value , dict ): value = Config ( value ) . resolve ( deep = False ) if not isinstance ( value , dict ): return value m = vd . init_model_instance ( ** value ) d = { k : v for k , v in m . _iter () if k in m . __fields_set__ or m . __fields__ [ k ] . default_factory } var_kwargs = d . pop ( vd . v_kwargs_name , {}) resolved = _func ( ** d , ** var_kwargs ) if save_params is not None : RESOLVED [ resolved ] = { ** save_params , ** params } return resolved yield validate @wraps ( vd . raw_function ) def wrapper_function ( * args : Any , ** kwargs : Any ) -> Any : values = vd . build_values ( args , kwargs ) if save_params is not None : if set ( values . keys ()) & { ALT_V_ARGS , ALT_V_KWARGS , V_POSITIONAL_ONLY_NAME , V_DUPLICATE_KWARGS , \"args\" , \"kwargs\" , }: print ( \"VALUES\" , values . keys (), values [ \"kwargs\" ]) raise Exception ( f \" { func } must not have positional only args, \" f \"kwargs or duplicated kwargs\" ) params = dict ( values ) resolved = params . pop ( \"self\" ) RESOLVED [ resolved ] = { ** save_params , ** params } return vd . execute ( vd . model ( ** resolve_non_dict ( vd . model , values ))) _func . vd = vd # type: ignore # _func.validate = vd.init_model_instance # type: ignore _func . __get_validators__ = __get_validators__ # type: ignore _func . raw_function = vd . raw_function # type: ignore _func . model = vd . model # type: ignore _func . __init__ = wrapper_function return _func else : vd = pydantic . decorator . ValidatedFunction ( _func , config ) @wraps ( _func ) def wrapper_function ( * args : Any , ** kwargs : Any ) -> Any : values = vd . build_values ( args , kwargs ) resolved = vd . execute ( vd . model ( ** resolve_non_dict ( vd . model , values ))) if save_params is not None : if set ( values . keys ()) & { ALT_V_ARGS , ALT_V_KWARGS , V_POSITIONAL_ONLY_NAME , V_DUPLICATE_KWARGS , \"args\" , \"kwargs\" , }: raise Exception ( f \" { func } must not have positional only args, \" f \"kwargs or duplicated kwargs\" ) RESOLVED [ resolved ] = { ** save_params , ** values } return resolved wrapper_function . vd = vd # type: ignore wrapper_function . validate = vd . init_model_instance # type: ignore wrapper_function . raw_function = vd . raw_function # type: ignore wrapper_function . model = vd . model # type: ignore return wrapper_function if func : return validate ( func ) else : return validate","title":"validate_arguments()"},{"location":"reference/model/","text":"edspdf.model","title":"model"},{"location":"reference/model/#edspdfmodel","text":"","title":"edspdf.model"},{"location":"reference/pipeline/","text":"edspdf.pipeline Pipeline The Pipeline is the core object of EDS-PDF. It is responsible for the orchestration of the components and processing PDF documents end-to-end. A pipeline is usually created empty and then populated with components via the add_pipe method. Here is an example : pipeline = Pipeline () pipeline . add_pipe ( \"pdfminer-extractor\" ) pipeline . add_pipe ( \"mask-classifier\" ) pipeline . add_pipe ( \"simple-aggregator\" ) Source code in edspdf/pipeline.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 @validate_arguments class Pipeline : \"\"\" The Pipeline is the core object of EDS-PDF. It is responsible for the orchestration of the components and processing PDF documents end-to-end. A pipeline is usually created empty and then populated with components via the `add_pipe` method. Here is an example : ```python pipeline = Pipeline() pipeline.add_pipe(\"pdfminer-extractor\") pipeline.add_pipe(\"mask-classifier\") pipeline.add_pipe(\"simple-aggregator\") ``` \"\"\" def __init__ ( self , components : Optional [ List [ str ]] = None , components_config : Optional [ Dict [ str , Component ]] = None , batch_size : int = 4 , ): \"\"\" Initializes the pipeline. The pipeline is empty by default and can be populated with components via the `add_pipe` method. Parameters ---------- components: Optional[List[str]] List of component names components_config: Optional[Dict[str, Component]] Dictionary of component configurations. The keys of the dictionary must match the component names. The values are the component configurations, which can contain unresolved configuration for nested components or instances of components. batch_size: int The default number of documents to process in parallel when running the pipeline \"\"\" super () . __init__ () if components is None : components = ComponentsMap ({}) if components_config is None : components_config = {} self . components = ComponentsMap ({}) for name in components : component = components_config [ name ] component . name = name self . components [ name ] = component self . batch_size = batch_size self . meta = {} def add_pipe ( self , factory_name : Union [ str , Callable ], name = None , config = None ): \"\"\" Adds a component to the pipeline. The component can be either a factory name or an instantiated component. If a factory name is provided, the component will be instantiated with the class from the registry matching the factory name and using the provided config as arguments. Parameters ---------- factory_name: Union[str, Callable] Either a factory name or an instantiated component name: str Name of the component config: Dict Configuration of the component. The configuration can contain unresolved configuration for nested components such as `{\"@factory_name\": \"my-sub-component\", ...}` Returns ------- Component The added component \"\"\" if isinstance ( factory_name , str ): if config is None : config = {} # config = Config(config).resolve() cls = registry . factory . get ( factory_name ) component = cls ( ** config ) elif hasattr ( factory_name , \"__call__\" ) or hasattr ( factory_name , \"process\" ): if config is not None : raise TypeError ( \"Cannot provide both an instantiated component and its config\" ) component = factory_name factory_name = next ( k for k , v in registry . factory . get_all () . items () if component . __class__ == v ) else : raise TypeError ( \"`add_pipe` first argument must either be a factory name \" f \"or an instantiated component. You passed a { type ( factory_name ) } \" ) if name is None : if factory_name is None : raise TypeError ( \"Could not automatically assign a name for component {} : either\" \"provide a name explicitly to the add_pipe function, or define a \" \"factory_name field on this component.\" . format ( component ) ) name = factory_name self . components [ name ] = component component . name = name if not ( hasattr ( component , \"process\" ) or hasattr ( component , \"__call__\" )): raise TypeError ( \"Component must have a process method or be callable\" ) return component def reset_cache ( self , cache : Optional [ CacheEnum ] = None ): \"\"\" Reset the caches of the components in this pipeline Parameters ---------- cache: Optional[CacheEnum] The cache to reset (either `preprocess`, `collate` or `forward`) If None, all caches are reset \"\"\" for component in self . components . values (): try : component . reset_cache ( cache ) except AttributeError : pass def __call__ ( self , doc : InputT ) -> OutputT : \"\"\" Applies the pipeline on a sample Parameters ---------- doc: InputT The document to process Returns ------- OutputT \"\"\" self . reset_cache () for name , component in self . components . items (): doc = component ( doc ) return doc def pipe ( self , docs : Iterable [ InputT ]) -> Iterable : \"\"\" Apply the pipeline on a collection of documents Parameters ---------- docs: Iterable[InputT] The documents to process Returns ------- Iterable An iterable collection of processed documents \"\"\" for batch in batchify ( docs , batch_size = self . batch_size ): self . reset_cache () for component in self . components . values (): batch = component . batch_process ( batch ) yield from batch def initialize ( self , data : Iterable [ InputT ]): \"\"\" Initialize the components of the pipeline Each component must be initialized before the next components are run. Since a component might need the full training data to be initialized, all data may be fed to the component, making it impossible to enable batch caching. Therefore, we disable cache during the entire operation, so heavy computation (such as embeddings) that is usually shared will be repeated for each initialized component. Parameters ---------- data: SupervisedData \"\"\" # Component initialization print ( \"Initializing components\" ) data = multi_tee ( data ) with self . no_cache (): for name , component in self . components . items (): if not component . initialized : component . initialize ( data ) print ( f \"Component { repr ( name ) } initialized\" ) def score ( self , docs : Sequence [ InputT ]): \"\"\" Scores a pipeline against a sequence of annotated documents Parameters ---------- docs: Sequence[InputT] The documents to score Returns ------- Dict[str, Any] A dictionary containing the metrics of the pipeline, as well as the speed of the pipeline. Each component that has a scorer will also be scored and its metrics will be included in the returned dictionary under a key named after each component. \"\"\" self . train ( False ) inputs : Sequence [ InputT ] = copy . deepcopy ( docs ) golds : Iterable [ Dict [ str , InputT ]] = docs scored_components = {} # Predicting intermediate steps preds = defaultdict ( lambda : []) for batch in batchify ( tqdm ( inputs , \"Scoring components\" ), batch_size = self . batch_size ): self . reset_cache () for name , component in self . components . items (): if component . scorer is not None : scored_components [ name ] = component batch = component . batch_process ( batch ) preds [ name ] . extend ( copy . deepcopy ( batch )) t0 = time . time () for _ in tqdm ( self . pipe ( inputs ), \"Scoring pipeline\" , total = len ( inputs )): pass duration = time . time () - t0 # Scoring metrics : Dict [ str , Any ] = { \"speed\" : len ( inputs ) / duration , } for name , component in scored_components . items (): metrics [ name ] = component . score ( list ( zip ( preds [ name ], golds ))) return metrics def preprocess ( self , doc : InputT , supervision : bool = False ): \"\"\" Runs the preprocessing methods of each component in the pipeline on a document and returns a dictionary containing the results, with the component names as keys. Parameters ---------- doc: InputT The document to preprocess supervision: bool Whether to include supervision information in the preprocessing Returns ------- Dict[str, Any] \"\"\" prep = {} for name , component in self . components . items (): if isinstance ( component , TrainableComponent ): prep [ name ] = component . preprocess ( doc , supervision = supervision ) return prep def preprocess_many ( self , docs : Iterable [ InputT ], compress = True , supervision = True ): \"\"\" Runs the preprocessing methods of each component in the pipeline on a collection of documents and returns an iterable of dictionaries containing the results, with the component names as keys. Parameters ---------- docs: Iterable[InputT] compress: bool Whether to deduplicate identical preprocessing outputs of the results if multiple documents share identical subcomponents. This step is required to enable the cache mechanism when training or running the pipeline over a tabular datasets such as pyarrow tables that do not store referential equality information. supervision: bool Whether to include supervision information in the preprocessing Returns ------- Iterable[OutputT] \"\"\" preprocessed = map ( partial ( self . preprocess , supervision = supervision ), docs ) if compress : return batch_compress_dict ( preprocessed ) return preprocessed def collate ( self , batch : Dict [ str , Any ], device : Optional [ torch . device ] = None ): \"\"\" Collates a batch of preprocessed samples into a single (maybe nested) dictionary of tensors by calling the collate method of each component. Parameters ---------- batch: Dict[str, Any] The batch of preprocessed samples device: Optional[torch.device] The device to move the tensors to before returning them Returns ------- Dict[str, Any] The collated batch \"\"\" batch = decompress_dict ( batch ) if device is None : device = next ( p . device for p in self . parameters ()) for name , component in self . components . items (): if name in batch : component : TrainableComponent component_inputs = batch [ name ] batch [ name ] = component . collate ( component_inputs , device ) return batch def train ( self , mode = True ): \"\"\" Enables training mode on pytorch modules Parameters ---------- mode: bool Whether to enable training or not \"\"\" for component in self . components . values (): if hasattr ( component , \"train\" ): component . train ( mode ) @property def cfg ( self ): \"\"\"Returns the initial configuration of the pipeline\"\"\" return Config ( components = list ( self . components . keys ()), components_config = Config ( ** self . components , __path__ = ( \"components\" ,)), ) . serialize () @contextmanager def no_cache ( self ): \"\"\"Disable caching for all (trainable) components in the pipeline\"\"\" saved = [] for component in self . components . values (): if isinstance ( component , TrainableComponent ): saved . append (( component , component . enable_cache ( False ))) yield for component , do_cache in saved : component . enable_cache ( do_cache ) def parameters ( self ): \"\"\"Returns an iterator over the Pytorch parameters of the components in the pipeline\"\"\" seen = set () for component in self . components . values (): if isinstance ( component , torch . nn . Module ): for param in component . parameters (): if param in seen : continue seen . add ( param ) yield param def __repr__ ( self ): return \"Pipeline( {} )\" . format ( \" \\n {} \\n \" . format ( \" \\n \" . join ( indent ( f \"( { name } ): \" + repr ( component ), prefix = \" \" ) for name , component in self . components . items () ) ) if len ( self . components ) else \"\" ) @property def trainable_components ( self ) -> List [ TrainableComponent ]: \"\"\"Returns the list of trainable components in the pipeline.\"\"\" return [ c for c in self . components . values () if isinstance ( c , TrainableComponent ) and c . needs_training ] def __iter__ ( self ): \"\"\"Returns an iterator over the components in the pipeline.\"\"\" return iter ( self . components . values ()) def __len__ ( self ): \"\"\"Returns the number of components in the pipeline.\"\"\" return len ( self . components ) cfg property Returns the initial configuration of the pipeline trainable_components : List [ TrainableComponent ] property Returns the list of trainable components in the pipeline. __init__ ( components = None , components_config = None , batch_size = 4 ) Initializes the pipeline. The pipeline is empty by default and can be populated with components via the add_pipe method. PARAMETER DESCRIPTION components List of component names TYPE: Optional [ List [ str ]] DEFAULT: None components_config Dictionary of component configurations. The keys of the dictionary must match the component names. The values are the component configurations, which can contain unresolved configuration for nested components or instances of components. TYPE: Optional [ Dict [ str , Component ]] DEFAULT: None batch_size The default number of documents to process in parallel when running the pipeline TYPE: int DEFAULT: 4 Source code in edspdf/pipeline.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , components : Optional [ List [ str ]] = None , components_config : Optional [ Dict [ str , Component ]] = None , batch_size : int = 4 , ): \"\"\" Initializes the pipeline. The pipeline is empty by default and can be populated with components via the `add_pipe` method. Parameters ---------- components: Optional[List[str]] List of component names components_config: Optional[Dict[str, Component]] Dictionary of component configurations. The keys of the dictionary must match the component names. The values are the component configurations, which can contain unresolved configuration for nested components or instances of components. batch_size: int The default number of documents to process in parallel when running the pipeline \"\"\" super () . __init__ () if components is None : components = ComponentsMap ({}) if components_config is None : components_config = {} self . components = ComponentsMap ({}) for name in components : component = components_config [ name ] component . name = name self . components [ name ] = component self . batch_size = batch_size self . meta = {} add_pipe ( factory_name , name = None , config = None ) Adds a component to the pipeline. The component can be either a factory name or an instantiated component. If a factory name is provided, the component will be instantiated with the class from the registry matching the factory name and using the provided config as arguments. PARAMETER DESCRIPTION factory_name Either a factory name or an instantiated component TYPE: Union [ str , Callable ] name Name of the component DEFAULT: None config Configuration of the component. The configuration can contain unresolved configuration for nested components such as {\"@factory_name\": \"my-sub-component\", ...} DEFAULT: None RETURNS DESCRIPTION Component The added component Source code in edspdf/pipeline.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def add_pipe ( self , factory_name : Union [ str , Callable ], name = None , config = None ): \"\"\" Adds a component to the pipeline. The component can be either a factory name or an instantiated component. If a factory name is provided, the component will be instantiated with the class from the registry matching the factory name and using the provided config as arguments. Parameters ---------- factory_name: Union[str, Callable] Either a factory name or an instantiated component name: str Name of the component config: Dict Configuration of the component. The configuration can contain unresolved configuration for nested components such as `{\"@factory_name\": \"my-sub-component\", ...}` Returns ------- Component The added component \"\"\" if isinstance ( factory_name , str ): if config is None : config = {} # config = Config(config).resolve() cls = registry . factory . get ( factory_name ) component = cls ( ** config ) elif hasattr ( factory_name , \"__call__\" ) or hasattr ( factory_name , \"process\" ): if config is not None : raise TypeError ( \"Cannot provide both an instantiated component and its config\" ) component = factory_name factory_name = next ( k for k , v in registry . factory . get_all () . items () if component . __class__ == v ) else : raise TypeError ( \"`add_pipe` first argument must either be a factory name \" f \"or an instantiated component. You passed a { type ( factory_name ) } \" ) if name is None : if factory_name is None : raise TypeError ( \"Could not automatically assign a name for component {} : either\" \"provide a name explicitly to the add_pipe function, or define a \" \"factory_name field on this component.\" . format ( component ) ) name = factory_name self . components [ name ] = component component . name = name if not ( hasattr ( component , \"process\" ) or hasattr ( component , \"__call__\" )): raise TypeError ( \"Component must have a process method or be callable\" ) return component reset_cache ( cache = None ) Reset the caches of the components in this pipeline PARAMETER DESCRIPTION cache The cache to reset (either preprocess , collate or forward ) If None, all caches are reset TYPE: Optional [ CacheEnum ] DEFAULT: None Source code in edspdf/pipeline.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def reset_cache ( self , cache : Optional [ CacheEnum ] = None ): \"\"\" Reset the caches of the components in this pipeline Parameters ---------- cache: Optional[CacheEnum] The cache to reset (either `preprocess`, `collate` or `forward`) If None, all caches are reset \"\"\" for component in self . components . values (): try : component . reset_cache ( cache ) except AttributeError : pass __call__ ( doc ) Applies the pipeline on a sample PARAMETER DESCRIPTION doc The document to process TYPE: InputT RETURNS DESCRIPTION OutputT Source code in edspdf/pipeline.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def __call__ ( self , doc : InputT ) -> OutputT : \"\"\" Applies the pipeline on a sample Parameters ---------- doc: InputT The document to process Returns ------- OutputT \"\"\" self . reset_cache () for name , component in self . components . items (): doc = component ( doc ) return doc pipe ( docs ) Apply the pipeline on a collection of documents PARAMETER DESCRIPTION docs The documents to process TYPE: Iterable [ InputT ] RETURNS DESCRIPTION Iterable An iterable collection of processed documents Source code in edspdf/pipeline.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def pipe ( self , docs : Iterable [ InputT ]) -> Iterable : \"\"\" Apply the pipeline on a collection of documents Parameters ---------- docs: Iterable[InputT] The documents to process Returns ------- Iterable An iterable collection of processed documents \"\"\" for batch in batchify ( docs , batch_size = self . batch_size ): self . reset_cache () for component in self . components . values (): batch = component . batch_process ( batch ) yield from batch initialize ( data ) Initialize the components of the pipeline Each component must be initialized before the next components are run. Since a component might need the full training data to be initialized, all data may be fed to the component, making it impossible to enable batch caching. Therefore, we disable cache during the entire operation, so heavy computation (such as embeddings) that is usually shared will be repeated for each initialized component. PARAMETER DESCRIPTION data TYPE: Iterable [ InputT ] Source code in edspdf/pipeline.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def initialize ( self , data : Iterable [ InputT ]): \"\"\" Initialize the components of the pipeline Each component must be initialized before the next components are run. Since a component might need the full training data to be initialized, all data may be fed to the component, making it impossible to enable batch caching. Therefore, we disable cache during the entire operation, so heavy computation (such as embeddings) that is usually shared will be repeated for each initialized component. Parameters ---------- data: SupervisedData \"\"\" # Component initialization print ( \"Initializing components\" ) data = multi_tee ( data ) with self . no_cache (): for name , component in self . components . items (): if not component . initialized : component . initialize ( data ) print ( f \"Component { repr ( name ) } initialized\" ) score ( docs ) Scores a pipeline against a sequence of annotated documents PARAMETER DESCRIPTION docs The documents to score TYPE: Sequence [ InputT ] RETURNS DESCRIPTION Dict [ str , Any ] A dictionary containing the metrics of the pipeline, as well as the speed of the pipeline. Each component that has a scorer will also be scored and its metrics will be included in the returned dictionary under a key named after each component. Source code in edspdf/pipeline.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def score ( self , docs : Sequence [ InputT ]): \"\"\" Scores a pipeline against a sequence of annotated documents Parameters ---------- docs: Sequence[InputT] The documents to score Returns ------- Dict[str, Any] A dictionary containing the metrics of the pipeline, as well as the speed of the pipeline. Each component that has a scorer will also be scored and its metrics will be included in the returned dictionary under a key named after each component. \"\"\" self . train ( False ) inputs : Sequence [ InputT ] = copy . deepcopy ( docs ) golds : Iterable [ Dict [ str , InputT ]] = docs scored_components = {} # Predicting intermediate steps preds = defaultdict ( lambda : []) for batch in batchify ( tqdm ( inputs , \"Scoring components\" ), batch_size = self . batch_size ): self . reset_cache () for name , component in self . components . items (): if component . scorer is not None : scored_components [ name ] = component batch = component . batch_process ( batch ) preds [ name ] . extend ( copy . deepcopy ( batch )) t0 = time . time () for _ in tqdm ( self . pipe ( inputs ), \"Scoring pipeline\" , total = len ( inputs )): pass duration = time . time () - t0 # Scoring metrics : Dict [ str , Any ] = { \"speed\" : len ( inputs ) / duration , } for name , component in scored_components . items (): metrics [ name ] = component . score ( list ( zip ( preds [ name ], golds ))) return metrics preprocess ( doc , supervision = False ) Runs the preprocessing methods of each component in the pipeline on a document and returns a dictionary containing the results, with the component names as keys. PARAMETER DESCRIPTION doc The document to preprocess TYPE: InputT supervision Whether to include supervision information in the preprocessing TYPE: bool DEFAULT: False RETURNS DESCRIPTION Dict [ str , Any ] Source code in edspdf/pipeline.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def preprocess ( self , doc : InputT , supervision : bool = False ): \"\"\" Runs the preprocessing methods of each component in the pipeline on a document and returns a dictionary containing the results, with the component names as keys. Parameters ---------- doc: InputT The document to preprocess supervision: bool Whether to include supervision information in the preprocessing Returns ------- Dict[str, Any] \"\"\" prep = {} for name , component in self . components . items (): if isinstance ( component , TrainableComponent ): prep [ name ] = component . preprocess ( doc , supervision = supervision ) return prep preprocess_many ( docs , compress = True , supervision = True ) Runs the preprocessing methods of each component in the pipeline on a collection of documents and returns an iterable of dictionaries containing the results, with the component names as keys. PARAMETER DESCRIPTION docs TYPE: Iterable [ InputT ] compress Whether to deduplicate identical preprocessing outputs of the results if multiple documents share identical subcomponents. This step is required to enable the cache mechanism when training or running the pipeline over a tabular datasets such as pyarrow tables that do not store referential equality information. DEFAULT: True supervision Whether to include supervision information in the preprocessing DEFAULT: True RETURNS DESCRIPTION Iterable [ OutputT ] Source code in edspdf/pipeline.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def preprocess_many ( self , docs : Iterable [ InputT ], compress = True , supervision = True ): \"\"\" Runs the preprocessing methods of each component in the pipeline on a collection of documents and returns an iterable of dictionaries containing the results, with the component names as keys. Parameters ---------- docs: Iterable[InputT] compress: bool Whether to deduplicate identical preprocessing outputs of the results if multiple documents share identical subcomponents. This step is required to enable the cache mechanism when training or running the pipeline over a tabular datasets such as pyarrow tables that do not store referential equality information. supervision: bool Whether to include supervision information in the preprocessing Returns ------- Iterable[OutputT] \"\"\" preprocessed = map ( partial ( self . preprocess , supervision = supervision ), docs ) if compress : return batch_compress_dict ( preprocessed ) return preprocessed collate ( batch , device = None ) Collates a batch of preprocessed samples into a single (maybe nested) dictionary of tensors by calling the collate method of each component. PARAMETER DESCRIPTION batch The batch of preprocessed samples TYPE: Dict [ str , Any ] device The device to move the tensors to before returning them TYPE: Optional [ torch . device ] DEFAULT: None RETURNS DESCRIPTION Dict [ str , Any ] The collated batch Source code in edspdf/pipeline.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 def collate ( self , batch : Dict [ str , Any ], device : Optional [ torch . device ] = None ): \"\"\" Collates a batch of preprocessed samples into a single (maybe nested) dictionary of tensors by calling the collate method of each component. Parameters ---------- batch: Dict[str, Any] The batch of preprocessed samples device: Optional[torch.device] The device to move the tensors to before returning them Returns ------- Dict[str, Any] The collated batch \"\"\" batch = decompress_dict ( batch ) if device is None : device = next ( p . device for p in self . parameters ()) for name , component in self . components . items (): if name in batch : component : TrainableComponent component_inputs = batch [ name ] batch [ name ] = component . collate ( component_inputs , device ) return batch train ( mode = True ) Enables training mode on pytorch modules PARAMETER DESCRIPTION mode Whether to enable training or not DEFAULT: True Source code in edspdf/pipeline.py 366 367 368 369 370 371 372 373 374 375 376 377 def train ( self , mode = True ): \"\"\" Enables training mode on pytorch modules Parameters ---------- mode: bool Whether to enable training or not \"\"\" for component in self . components . values (): if hasattr ( component , \"train\" ): component . train ( mode ) no_cache () Disable caching for all (trainable) components in the pipeline Source code in edspdf/pipeline.py 387 388 389 390 391 392 393 394 395 396 @contextmanager def no_cache ( self ): \"\"\"Disable caching for all (trainable) components in the pipeline\"\"\" saved = [] for component in self . components . values (): if isinstance ( component , TrainableComponent ): saved . append (( component , component . enable_cache ( False ))) yield for component , do_cache in saved : component . enable_cache ( do_cache ) parameters () Returns an iterator over the Pytorch parameters of the components in the pipeline Source code in edspdf/pipeline.py 398 399 400 401 402 403 404 405 406 407 408 def parameters ( self ): \"\"\"Returns an iterator over the Pytorch parameters of the components in the pipeline\"\"\" seen = set () for component in self . components . values (): if isinstance ( component , torch . nn . Module ): for param in component . parameters (): if param in seen : continue seen . add ( param ) yield param __iter__ () Returns an iterator over the components in the pipeline. Source code in edspdf/pipeline.py 431 432 433 def __iter__ ( self ): \"\"\"Returns an iterator over the components in the pipeline.\"\"\" return iter ( self . components . values ()) __len__ () Returns the number of components in the pipeline. Source code in edspdf/pipeline.py 435 436 437 def __len__ ( self ): \"\"\"Returns the number of components in the pipeline.\"\"\" return len ( self . components )","title":"pipeline"},{"location":"reference/pipeline/#edspdfpipeline","text":"","title":"edspdf.pipeline"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline","text":"The Pipeline is the core object of EDS-PDF. It is responsible for the orchestration of the components and processing PDF documents end-to-end. A pipeline is usually created empty and then populated with components via the add_pipe method. Here is an example : pipeline = Pipeline () pipeline . add_pipe ( \"pdfminer-extractor\" ) pipeline . add_pipe ( \"mask-classifier\" ) pipeline . add_pipe ( \"simple-aggregator\" ) Source code in edspdf/pipeline.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 @validate_arguments class Pipeline : \"\"\" The Pipeline is the core object of EDS-PDF. It is responsible for the orchestration of the components and processing PDF documents end-to-end. A pipeline is usually created empty and then populated with components via the `add_pipe` method. Here is an example : ```python pipeline = Pipeline() pipeline.add_pipe(\"pdfminer-extractor\") pipeline.add_pipe(\"mask-classifier\") pipeline.add_pipe(\"simple-aggregator\") ``` \"\"\" def __init__ ( self , components : Optional [ List [ str ]] = None , components_config : Optional [ Dict [ str , Component ]] = None , batch_size : int = 4 , ): \"\"\" Initializes the pipeline. The pipeline is empty by default and can be populated with components via the `add_pipe` method. Parameters ---------- components: Optional[List[str]] List of component names components_config: Optional[Dict[str, Component]] Dictionary of component configurations. The keys of the dictionary must match the component names. The values are the component configurations, which can contain unresolved configuration for nested components or instances of components. batch_size: int The default number of documents to process in parallel when running the pipeline \"\"\" super () . __init__ () if components is None : components = ComponentsMap ({}) if components_config is None : components_config = {} self . components = ComponentsMap ({}) for name in components : component = components_config [ name ] component . name = name self . components [ name ] = component self . batch_size = batch_size self . meta = {} def add_pipe ( self , factory_name : Union [ str , Callable ], name = None , config = None ): \"\"\" Adds a component to the pipeline. The component can be either a factory name or an instantiated component. If a factory name is provided, the component will be instantiated with the class from the registry matching the factory name and using the provided config as arguments. Parameters ---------- factory_name: Union[str, Callable] Either a factory name or an instantiated component name: str Name of the component config: Dict Configuration of the component. The configuration can contain unresolved configuration for nested components such as `{\"@factory_name\": \"my-sub-component\", ...}` Returns ------- Component The added component \"\"\" if isinstance ( factory_name , str ): if config is None : config = {} # config = Config(config).resolve() cls = registry . factory . get ( factory_name ) component = cls ( ** config ) elif hasattr ( factory_name , \"__call__\" ) or hasattr ( factory_name , \"process\" ): if config is not None : raise TypeError ( \"Cannot provide both an instantiated component and its config\" ) component = factory_name factory_name = next ( k for k , v in registry . factory . get_all () . items () if component . __class__ == v ) else : raise TypeError ( \"`add_pipe` first argument must either be a factory name \" f \"or an instantiated component. You passed a { type ( factory_name ) } \" ) if name is None : if factory_name is None : raise TypeError ( \"Could not automatically assign a name for component {} : either\" \"provide a name explicitly to the add_pipe function, or define a \" \"factory_name field on this component.\" . format ( component ) ) name = factory_name self . components [ name ] = component component . name = name if not ( hasattr ( component , \"process\" ) or hasattr ( component , \"__call__\" )): raise TypeError ( \"Component must have a process method or be callable\" ) return component def reset_cache ( self , cache : Optional [ CacheEnum ] = None ): \"\"\" Reset the caches of the components in this pipeline Parameters ---------- cache: Optional[CacheEnum] The cache to reset (either `preprocess`, `collate` or `forward`) If None, all caches are reset \"\"\" for component in self . components . values (): try : component . reset_cache ( cache ) except AttributeError : pass def __call__ ( self , doc : InputT ) -> OutputT : \"\"\" Applies the pipeline on a sample Parameters ---------- doc: InputT The document to process Returns ------- OutputT \"\"\" self . reset_cache () for name , component in self . components . items (): doc = component ( doc ) return doc def pipe ( self , docs : Iterable [ InputT ]) -> Iterable : \"\"\" Apply the pipeline on a collection of documents Parameters ---------- docs: Iterable[InputT] The documents to process Returns ------- Iterable An iterable collection of processed documents \"\"\" for batch in batchify ( docs , batch_size = self . batch_size ): self . reset_cache () for component in self . components . values (): batch = component . batch_process ( batch ) yield from batch def initialize ( self , data : Iterable [ InputT ]): \"\"\" Initialize the components of the pipeline Each component must be initialized before the next components are run. Since a component might need the full training data to be initialized, all data may be fed to the component, making it impossible to enable batch caching. Therefore, we disable cache during the entire operation, so heavy computation (such as embeddings) that is usually shared will be repeated for each initialized component. Parameters ---------- data: SupervisedData \"\"\" # Component initialization print ( \"Initializing components\" ) data = multi_tee ( data ) with self . no_cache (): for name , component in self . components . items (): if not component . initialized : component . initialize ( data ) print ( f \"Component { repr ( name ) } initialized\" ) def score ( self , docs : Sequence [ InputT ]): \"\"\" Scores a pipeline against a sequence of annotated documents Parameters ---------- docs: Sequence[InputT] The documents to score Returns ------- Dict[str, Any] A dictionary containing the metrics of the pipeline, as well as the speed of the pipeline. Each component that has a scorer will also be scored and its metrics will be included in the returned dictionary under a key named after each component. \"\"\" self . train ( False ) inputs : Sequence [ InputT ] = copy . deepcopy ( docs ) golds : Iterable [ Dict [ str , InputT ]] = docs scored_components = {} # Predicting intermediate steps preds = defaultdict ( lambda : []) for batch in batchify ( tqdm ( inputs , \"Scoring components\" ), batch_size = self . batch_size ): self . reset_cache () for name , component in self . components . items (): if component . scorer is not None : scored_components [ name ] = component batch = component . batch_process ( batch ) preds [ name ] . extend ( copy . deepcopy ( batch )) t0 = time . time () for _ in tqdm ( self . pipe ( inputs ), \"Scoring pipeline\" , total = len ( inputs )): pass duration = time . time () - t0 # Scoring metrics : Dict [ str , Any ] = { \"speed\" : len ( inputs ) / duration , } for name , component in scored_components . items (): metrics [ name ] = component . score ( list ( zip ( preds [ name ], golds ))) return metrics def preprocess ( self , doc : InputT , supervision : bool = False ): \"\"\" Runs the preprocessing methods of each component in the pipeline on a document and returns a dictionary containing the results, with the component names as keys. Parameters ---------- doc: InputT The document to preprocess supervision: bool Whether to include supervision information in the preprocessing Returns ------- Dict[str, Any] \"\"\" prep = {} for name , component in self . components . items (): if isinstance ( component , TrainableComponent ): prep [ name ] = component . preprocess ( doc , supervision = supervision ) return prep def preprocess_many ( self , docs : Iterable [ InputT ], compress = True , supervision = True ): \"\"\" Runs the preprocessing methods of each component in the pipeline on a collection of documents and returns an iterable of dictionaries containing the results, with the component names as keys. Parameters ---------- docs: Iterable[InputT] compress: bool Whether to deduplicate identical preprocessing outputs of the results if multiple documents share identical subcomponents. This step is required to enable the cache mechanism when training or running the pipeline over a tabular datasets such as pyarrow tables that do not store referential equality information. supervision: bool Whether to include supervision information in the preprocessing Returns ------- Iterable[OutputT] \"\"\" preprocessed = map ( partial ( self . preprocess , supervision = supervision ), docs ) if compress : return batch_compress_dict ( preprocessed ) return preprocessed def collate ( self , batch : Dict [ str , Any ], device : Optional [ torch . device ] = None ): \"\"\" Collates a batch of preprocessed samples into a single (maybe nested) dictionary of tensors by calling the collate method of each component. Parameters ---------- batch: Dict[str, Any] The batch of preprocessed samples device: Optional[torch.device] The device to move the tensors to before returning them Returns ------- Dict[str, Any] The collated batch \"\"\" batch = decompress_dict ( batch ) if device is None : device = next ( p . device for p in self . parameters ()) for name , component in self . components . items (): if name in batch : component : TrainableComponent component_inputs = batch [ name ] batch [ name ] = component . collate ( component_inputs , device ) return batch def train ( self , mode = True ): \"\"\" Enables training mode on pytorch modules Parameters ---------- mode: bool Whether to enable training or not \"\"\" for component in self . components . values (): if hasattr ( component , \"train\" ): component . train ( mode ) @property def cfg ( self ): \"\"\"Returns the initial configuration of the pipeline\"\"\" return Config ( components = list ( self . components . keys ()), components_config = Config ( ** self . components , __path__ = ( \"components\" ,)), ) . serialize () @contextmanager def no_cache ( self ): \"\"\"Disable caching for all (trainable) components in the pipeline\"\"\" saved = [] for component in self . components . values (): if isinstance ( component , TrainableComponent ): saved . append (( component , component . enable_cache ( False ))) yield for component , do_cache in saved : component . enable_cache ( do_cache ) def parameters ( self ): \"\"\"Returns an iterator over the Pytorch parameters of the components in the pipeline\"\"\" seen = set () for component in self . components . values (): if isinstance ( component , torch . nn . Module ): for param in component . parameters (): if param in seen : continue seen . add ( param ) yield param def __repr__ ( self ): return \"Pipeline( {} )\" . format ( \" \\n {} \\n \" . format ( \" \\n \" . join ( indent ( f \"( { name } ): \" + repr ( component ), prefix = \" \" ) for name , component in self . components . items () ) ) if len ( self . components ) else \"\" ) @property def trainable_components ( self ) -> List [ TrainableComponent ]: \"\"\"Returns the list of trainable components in the pipeline.\"\"\" return [ c for c in self . components . values () if isinstance ( c , TrainableComponent ) and c . needs_training ] def __iter__ ( self ): \"\"\"Returns an iterator over the components in the pipeline.\"\"\" return iter ( self . components . values ()) def __len__ ( self ): \"\"\"Returns the number of components in the pipeline.\"\"\" return len ( self . components )","title":"Pipeline"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.cfg","text":"Returns the initial configuration of the pipeline","title":"cfg"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.trainable_components","text":"Returns the list of trainable components in the pipeline.","title":"trainable_components"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.__init__","text":"Initializes the pipeline. The pipeline is empty by default and can be populated with components via the add_pipe method. PARAMETER DESCRIPTION components List of component names TYPE: Optional [ List [ str ]] DEFAULT: None components_config Dictionary of component configurations. The keys of the dictionary must match the component names. The values are the component configurations, which can contain unresolved configuration for nested components or instances of components. TYPE: Optional [ Dict [ str , Component ]] DEFAULT: None batch_size The default number of documents to process in parallel when running the pipeline TYPE: int DEFAULT: 4 Source code in edspdf/pipeline.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __init__ ( self , components : Optional [ List [ str ]] = None , components_config : Optional [ Dict [ str , Component ]] = None , batch_size : int = 4 , ): \"\"\" Initializes the pipeline. The pipeline is empty by default and can be populated with components via the `add_pipe` method. Parameters ---------- components: Optional[List[str]] List of component names components_config: Optional[Dict[str, Component]] Dictionary of component configurations. The keys of the dictionary must match the component names. The values are the component configurations, which can contain unresolved configuration for nested components or instances of components. batch_size: int The default number of documents to process in parallel when running the pipeline \"\"\" super () . __init__ () if components is None : components = ComponentsMap ({}) if components_config is None : components_config = {} self . components = ComponentsMap ({}) for name in components : component = components_config [ name ] component . name = name self . components [ name ] = component self . batch_size = batch_size self . meta = {}","title":"__init__()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.add_pipe","text":"Adds a component to the pipeline. The component can be either a factory name or an instantiated component. If a factory name is provided, the component will be instantiated with the class from the registry matching the factory name and using the provided config as arguments. PARAMETER DESCRIPTION factory_name Either a factory name or an instantiated component TYPE: Union [ str , Callable ] name Name of the component DEFAULT: None config Configuration of the component. The configuration can contain unresolved configuration for nested components such as {\"@factory_name\": \"my-sub-component\", ...} DEFAULT: None RETURNS DESCRIPTION Component The added component Source code in edspdf/pipeline.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def add_pipe ( self , factory_name : Union [ str , Callable ], name = None , config = None ): \"\"\" Adds a component to the pipeline. The component can be either a factory name or an instantiated component. If a factory name is provided, the component will be instantiated with the class from the registry matching the factory name and using the provided config as arguments. Parameters ---------- factory_name: Union[str, Callable] Either a factory name or an instantiated component name: str Name of the component config: Dict Configuration of the component. The configuration can contain unresolved configuration for nested components such as `{\"@factory_name\": \"my-sub-component\", ...}` Returns ------- Component The added component \"\"\" if isinstance ( factory_name , str ): if config is None : config = {} # config = Config(config).resolve() cls = registry . factory . get ( factory_name ) component = cls ( ** config ) elif hasattr ( factory_name , \"__call__\" ) or hasattr ( factory_name , \"process\" ): if config is not None : raise TypeError ( \"Cannot provide both an instantiated component and its config\" ) component = factory_name factory_name = next ( k for k , v in registry . factory . get_all () . items () if component . __class__ == v ) else : raise TypeError ( \"`add_pipe` first argument must either be a factory name \" f \"or an instantiated component. You passed a { type ( factory_name ) } \" ) if name is None : if factory_name is None : raise TypeError ( \"Could not automatically assign a name for component {} : either\" \"provide a name explicitly to the add_pipe function, or define a \" \"factory_name field on this component.\" . format ( component ) ) name = factory_name self . components [ name ] = component component . name = name if not ( hasattr ( component , \"process\" ) or hasattr ( component , \"__call__\" )): raise TypeError ( \"Component must have a process method or be callable\" ) return component","title":"add_pipe()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.reset_cache","text":"Reset the caches of the components in this pipeline PARAMETER DESCRIPTION cache The cache to reset (either preprocess , collate or forward ) If None, all caches are reset TYPE: Optional [ CacheEnum ] DEFAULT: None Source code in edspdf/pipeline.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def reset_cache ( self , cache : Optional [ CacheEnum ] = None ): \"\"\" Reset the caches of the components in this pipeline Parameters ---------- cache: Optional[CacheEnum] The cache to reset (either `preprocess`, `collate` or `forward`) If None, all caches are reset \"\"\" for component in self . components . values (): try : component . reset_cache ( cache ) except AttributeError : pass","title":"reset_cache()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.__call__","text":"Applies the pipeline on a sample PARAMETER DESCRIPTION doc The document to process TYPE: InputT RETURNS DESCRIPTION OutputT Source code in edspdf/pipeline.py 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def __call__ ( self , doc : InputT ) -> OutputT : \"\"\" Applies the pipeline on a sample Parameters ---------- doc: InputT The document to process Returns ------- OutputT \"\"\" self . reset_cache () for name , component in self . components . items (): doc = component ( doc ) return doc","title":"__call__()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.pipe","text":"Apply the pipeline on a collection of documents PARAMETER DESCRIPTION docs The documents to process TYPE: Iterable [ InputT ] RETURNS DESCRIPTION Iterable An iterable collection of processed documents Source code in edspdf/pipeline.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def pipe ( self , docs : Iterable [ InputT ]) -> Iterable : \"\"\" Apply the pipeline on a collection of documents Parameters ---------- docs: Iterable[InputT] The documents to process Returns ------- Iterable An iterable collection of processed documents \"\"\" for batch in batchify ( docs , batch_size = self . batch_size ): self . reset_cache () for component in self . components . values (): batch = component . batch_process ( batch ) yield from batch","title":"pipe()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.initialize","text":"Initialize the components of the pipeline Each component must be initialized before the next components are run. Since a component might need the full training data to be initialized, all data may be fed to the component, making it impossible to enable batch caching. Therefore, we disable cache during the entire operation, so heavy computation (such as embeddings) that is usually shared will be repeated for each initialized component. PARAMETER DESCRIPTION data TYPE: Iterable [ InputT ] Source code in edspdf/pipeline.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def initialize ( self , data : Iterable [ InputT ]): \"\"\" Initialize the components of the pipeline Each component must be initialized before the next components are run. Since a component might need the full training data to be initialized, all data may be fed to the component, making it impossible to enable batch caching. Therefore, we disable cache during the entire operation, so heavy computation (such as embeddings) that is usually shared will be repeated for each initialized component. Parameters ---------- data: SupervisedData \"\"\" # Component initialization print ( \"Initializing components\" ) data = multi_tee ( data ) with self . no_cache (): for name , component in self . components . items (): if not component . initialized : component . initialize ( data ) print ( f \"Component { repr ( name ) } initialized\" )","title":"initialize()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.score","text":"Scores a pipeline against a sequence of annotated documents PARAMETER DESCRIPTION docs The documents to score TYPE: Sequence [ InputT ] RETURNS DESCRIPTION Dict [ str , Any ] A dictionary containing the metrics of the pipeline, as well as the speed of the pipeline. Each component that has a scorer will also be scored and its metrics will be included in the returned dictionary under a key named after each component. Source code in edspdf/pipeline.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def score ( self , docs : Sequence [ InputT ]): \"\"\" Scores a pipeline against a sequence of annotated documents Parameters ---------- docs: Sequence[InputT] The documents to score Returns ------- Dict[str, Any] A dictionary containing the metrics of the pipeline, as well as the speed of the pipeline. Each component that has a scorer will also be scored and its metrics will be included in the returned dictionary under a key named after each component. \"\"\" self . train ( False ) inputs : Sequence [ InputT ] = copy . deepcopy ( docs ) golds : Iterable [ Dict [ str , InputT ]] = docs scored_components = {} # Predicting intermediate steps preds = defaultdict ( lambda : []) for batch in batchify ( tqdm ( inputs , \"Scoring components\" ), batch_size = self . batch_size ): self . reset_cache () for name , component in self . components . items (): if component . scorer is not None : scored_components [ name ] = component batch = component . batch_process ( batch ) preds [ name ] . extend ( copy . deepcopy ( batch )) t0 = time . time () for _ in tqdm ( self . pipe ( inputs ), \"Scoring pipeline\" , total = len ( inputs )): pass duration = time . time () - t0 # Scoring metrics : Dict [ str , Any ] = { \"speed\" : len ( inputs ) / duration , } for name , component in scored_components . items (): metrics [ name ] = component . score ( list ( zip ( preds [ name ], golds ))) return metrics","title":"score()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.preprocess","text":"Runs the preprocessing methods of each component in the pipeline on a document and returns a dictionary containing the results, with the component names as keys. PARAMETER DESCRIPTION doc The document to preprocess TYPE: InputT supervision Whether to include supervision information in the preprocessing TYPE: bool DEFAULT: False RETURNS DESCRIPTION Dict [ str , Any ] Source code in edspdf/pipeline.py 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 def preprocess ( self , doc : InputT , supervision : bool = False ): \"\"\" Runs the preprocessing methods of each component in the pipeline on a document and returns a dictionary containing the results, with the component names as keys. Parameters ---------- doc: InputT The document to preprocess supervision: bool Whether to include supervision information in the preprocessing Returns ------- Dict[str, Any] \"\"\" prep = {} for name , component in self . components . items (): if isinstance ( component , TrainableComponent ): prep [ name ] = component . preprocess ( doc , supervision = supervision ) return prep","title":"preprocess()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.preprocess_many","text":"Runs the preprocessing methods of each component in the pipeline on a collection of documents and returns an iterable of dictionaries containing the results, with the component names as keys. PARAMETER DESCRIPTION docs TYPE: Iterable [ InputT ] compress Whether to deduplicate identical preprocessing outputs of the results if multiple documents share identical subcomponents. This step is required to enable the cache mechanism when training or running the pipeline over a tabular datasets such as pyarrow tables that do not store referential equality information. DEFAULT: True supervision Whether to include supervision information in the preprocessing DEFAULT: True RETURNS DESCRIPTION Iterable [ OutputT ] Source code in edspdf/pipeline.py 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def preprocess_many ( self , docs : Iterable [ InputT ], compress = True , supervision = True ): \"\"\" Runs the preprocessing methods of each component in the pipeline on a collection of documents and returns an iterable of dictionaries containing the results, with the component names as keys. Parameters ---------- docs: Iterable[InputT] compress: bool Whether to deduplicate identical preprocessing outputs of the results if multiple documents share identical subcomponents. This step is required to enable the cache mechanism when training or running the pipeline over a tabular datasets such as pyarrow tables that do not store referential equality information. supervision: bool Whether to include supervision information in the preprocessing Returns ------- Iterable[OutputT] \"\"\" preprocessed = map ( partial ( self . preprocess , supervision = supervision ), docs ) if compress : return batch_compress_dict ( preprocessed ) return preprocessed","title":"preprocess_many()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.collate","text":"Collates a batch of preprocessed samples into a single (maybe nested) dictionary of tensors by calling the collate method of each component. PARAMETER DESCRIPTION batch The batch of preprocessed samples TYPE: Dict [ str , Any ] device The device to move the tensors to before returning them TYPE: Optional [ torch . device ] DEFAULT: None RETURNS DESCRIPTION Dict [ str , Any ] The collated batch Source code in edspdf/pipeline.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 def collate ( self , batch : Dict [ str , Any ], device : Optional [ torch . device ] = None ): \"\"\" Collates a batch of preprocessed samples into a single (maybe nested) dictionary of tensors by calling the collate method of each component. Parameters ---------- batch: Dict[str, Any] The batch of preprocessed samples device: Optional[torch.device] The device to move the tensors to before returning them Returns ------- Dict[str, Any] The collated batch \"\"\" batch = decompress_dict ( batch ) if device is None : device = next ( p . device for p in self . parameters ()) for name , component in self . components . items (): if name in batch : component : TrainableComponent component_inputs = batch [ name ] batch [ name ] = component . collate ( component_inputs , device ) return batch","title":"collate()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.train","text":"Enables training mode on pytorch modules PARAMETER DESCRIPTION mode Whether to enable training or not DEFAULT: True Source code in edspdf/pipeline.py 366 367 368 369 370 371 372 373 374 375 376 377 def train ( self , mode = True ): \"\"\" Enables training mode on pytorch modules Parameters ---------- mode: bool Whether to enable training or not \"\"\" for component in self . components . values (): if hasattr ( component , \"train\" ): component . train ( mode )","title":"train()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.no_cache","text":"Disable caching for all (trainable) components in the pipeline Source code in edspdf/pipeline.py 387 388 389 390 391 392 393 394 395 396 @contextmanager def no_cache ( self ): \"\"\"Disable caching for all (trainable) components in the pipeline\"\"\" saved = [] for component in self . components . values (): if isinstance ( component , TrainableComponent ): saved . append (( component , component . enable_cache ( False ))) yield for component , do_cache in saved : component . enable_cache ( do_cache )","title":"no_cache()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.parameters","text":"Returns an iterator over the Pytorch parameters of the components in the pipeline Source code in edspdf/pipeline.py 398 399 400 401 402 403 404 405 406 407 408 def parameters ( self ): \"\"\"Returns an iterator over the Pytorch parameters of the components in the pipeline\"\"\" seen = set () for component in self . components . values (): if isinstance ( component , torch . nn . Module ): for param in component . parameters (): if param in seen : continue seen . add ( param ) yield param","title":"parameters()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.__iter__","text":"Returns an iterator over the components in the pipeline. Source code in edspdf/pipeline.py 431 432 433 def __iter__ ( self ): \"\"\"Returns an iterator over the components in the pipeline.\"\"\" return iter ( self . components . values ())","title":"__iter__()"},{"location":"reference/pipeline/#edspdf.pipeline.Pipeline.__len__","text":"Returns the number of components in the pipeline. Source code in edspdf/pipeline.py 435 436 437 def __len__ ( self ): \"\"\"Returns the number of components in the pipeline.\"\"\" return len ( self . components )","title":"__len__()"},{"location":"reference/registry/","text":"edspdf.registry","title":"registry"},{"location":"reference/registry/#edspdfregistry","text":"","title":"edspdf.registry"},{"location":"reference/components/","text":"edspdf.components","title":"`edspdf.components`"},{"location":"reference/components/#edspdfcomponents","text":"","title":"edspdf.components"},{"location":"reference/components/aggregators/","text":"edspdf.components.aggregators","title":"`edspdf.components.aggregators`"},{"location":"reference/components/aggregators/#edspdfcomponentsaggregators","text":"","title":"edspdf.components.aggregators"},{"location":"reference/components/aggregators/simple/","text":"edspdf.components.aggregators.simple","title":"simple"},{"location":"reference/components/aggregators/simple/#edspdfcomponentsaggregatorssimple","text":"","title":"edspdf.components.aggregators.simple"},{"location":"reference/components/aggregators/styled/","text":"edspdf.components.aggregators.styled StyledAggregator Bases: SimpleAggregator Aggregator that returns text and styles. Source code in edspdf/components/aggregators/styled.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 @registry . factory . register ( \"styled-aggregator\" ) class StyledAggregator ( SimpleAggregator ): \"\"\" Aggregator that returns text and styles. \"\"\" def __call__ ( self , doc : PDFDoc ) -> Tuple [ Dict [ str , str ], Dict [ str , List [ Dict ]]]: row_height = sum ( b . y1 - b . y0 for b in doc . lines ) / max ( 1 , len ( doc . lines )) all_lines = sorted ( [ line for line in doc . lines if len ( line . text ) > 0 and line . label is not None ], key = lambda b : ( b . label , b . page , b . y1 // row_height , b . x0 ), ) texts = {} styles = {} for label , lines in groupby ( all_lines , key = lambda b : b . label ): styles [ label ] = [] text = \"\" lines : List [ TextBox ] = list ( lines ) pairs = list ( zip ( lines , [ * lines [ 1 :], None ])) dys = [ next_box . y1 - line . y1 if next_box is not None and line . page == next_box . page else None for line , next_box in pairs ] height = np . median ( np . asarray ([ line . y1 - line . y0 for line in lines ])) for ( line , next_box ), dy in zip ( pairs , dys ): for style in line . styles : style_dict = style . dict () style_dict [ \"begin\" ] += len ( text ) style_dict [ \"end\" ] += len ( text ) styles [ label ] . append ( style_dict ) text = text + line . text if next_box is None : continue if line . page != next_box . page : text = text + \" \\n\\n \" elif dy / height > self . new_paragraph_threshold : text = text + \" \\n\\n \" elif dy / height > self . new_line_threshold : text = text + \" \\n \" else : text = text + \" \" texts [ label ] = \"\" . join ( text ) return texts , styles","title":"styled"},{"location":"reference/components/aggregators/styled/#edspdfcomponentsaggregatorsstyled","text":"","title":"edspdf.components.aggregators.styled"},{"location":"reference/components/aggregators/styled/#edspdf.components.aggregators.styled.StyledAggregator","text":"Bases: SimpleAggregator Aggregator that returns text and styles. Source code in edspdf/components/aggregators/styled.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 @registry . factory . register ( \"styled-aggregator\" ) class StyledAggregator ( SimpleAggregator ): \"\"\" Aggregator that returns text and styles. \"\"\" def __call__ ( self , doc : PDFDoc ) -> Tuple [ Dict [ str , str ], Dict [ str , List [ Dict ]]]: row_height = sum ( b . y1 - b . y0 for b in doc . lines ) / max ( 1 , len ( doc . lines )) all_lines = sorted ( [ line for line in doc . lines if len ( line . text ) > 0 and line . label is not None ], key = lambda b : ( b . label , b . page , b . y1 // row_height , b . x0 ), ) texts = {} styles = {} for label , lines in groupby ( all_lines , key = lambda b : b . label ): styles [ label ] = [] text = \"\" lines : List [ TextBox ] = list ( lines ) pairs = list ( zip ( lines , [ * lines [ 1 :], None ])) dys = [ next_box . y1 - line . y1 if next_box is not None and line . page == next_box . page else None for line , next_box in pairs ] height = np . median ( np . asarray ([ line . y1 - line . y0 for line in lines ])) for ( line , next_box ), dy in zip ( pairs , dys ): for style in line . styles : style_dict = style . dict () style_dict [ \"begin\" ] += len ( text ) style_dict [ \"end\" ] += len ( text ) styles [ label ] . append ( style_dict ) text = text + line . text if next_box is None : continue if line . page != next_box . page : text = text + \" \\n\\n \" elif dy / height > self . new_paragraph_threshold : text = text + \" \\n\\n \" elif dy / height > self . new_line_threshold : text = text + \" \\n \" else : text = text + \" \" texts [ label ] = \"\" . join ( text ) return texts , styles","title":"StyledAggregator"},{"location":"reference/components/classifiers/","text":"edspdf.components.classifiers","title":"`edspdf.components.classifiers`"},{"location":"reference/components/classifiers/#edspdfcomponentsclassifiers","text":"","title":"edspdf.components.classifiers"},{"location":"reference/components/classifiers/deep_classifier/","text":"edspdf.components.classifiers.deep_classifier DeepClassifier Bases: TrainableComponent [ PDFDoc , Dict [ str , Any ], PDFDoc ] Source code in edspdf/components/classifiers/deep_classifier.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @registry . factory . register ( \"deep-classifier\" ) class DeepClassifier ( TrainableComponent [ PDFDoc , Dict [ str , Any ], PDFDoc ]): def __init__ ( self , embedding : Module , labels : Sequence [ str ] = (), activation : ActivationFunction = \"gelu\" , dropout_p : float = 0.15 , scorer : Scorer = classifier_scorer , ): \"\"\" Runs a deep learning classifier model on the boxes. Parameters ---------- labels: Sequence[str] Initial labels of the classifier (will be completed during initialization) embedding: Module Embedding module to encode the PDF boxes activation: ActivationFunction Name of the activation function dropout_p: float Dropout probability used on the output of the box and textual encoders scorer: Scorer Scoring function \"\"\" super () . __init__ () self . label_vocabulary : Vocabulary = Vocabulary ( list ( dict . fromkeys ([ \"pollution\" , * labels ])) ) self . embedding : Module = embedding size = self . embedding . output_size self . linear = torch . nn . Linear ( size , size ) self . classifier : torch . nn . Linear = None # noqa self . activation = get_activation_function ( activation ) self . dropout = torch . nn . Dropout ( dropout_p ) # Scoring function self . scorer = scorer def initialize ( self , gold_data : Iterable [ PDFDoc ]): self . embedding . initialize ( gold_data ) with self . label_vocabulary . initialization (): for doc in tqdm ( gold_data , desc = \"Initializing classifier\" ): with self . no_cache (): self . preprocess ( doc , supervision = True ) self . classifier = torch . nn . Linear ( in_features = self . embedding . output_size , out_features = len ( self . label_vocabulary ), ) def preprocess ( self , doc : PDFDoc , supervision : bool = False ) -> Dict [ str , Any ]: result = { \"embedding\" : self . embedding . preprocess ( doc , supervision = supervision ), \"doc_id\" : doc . id , } if supervision : text_boxes = doc . lines result [ \"labels\" ] = [ self . label_vocabulary . encode ( b . label ) if b . label is not None else - 100 for b in text_boxes ] return result def collate ( self , batch , device : torch . device ) -> Dict : collated = { \"embedding\" : self . embedding . collate ( batch [ \"embedding\" ], device ), \"doc_id\" : batch [ \"doc_id\" ], } if \"labels\" in batch : collated . update ( { \"labels\" : torch . as_tensor ( flatten ( batch [ \"labels\" ]), device = device ), } ) return collated def forward ( self , batch : Dict , supervision = False ) -> Dict : embeds = self . embedding ( batch [ \"embedding\" ]) output = { \"loss\" : 0 } # Label prediction / learning logits = self . classifier ( embeds ) if supervision : targets = batch [ \"labels\" ] output [ \"label_loss\" ] = F . cross_entropy ( logits , targets , reduction = \"sum\" , ) output [ \"loss\" ] = output [ \"loss\" ] + output [ \"label_loss\" ] else : output [ \"logits\" ] = logits output [ \"labels\" ] = logits . argmax ( - 1 ) return output def postprocess ( self , docs : Sequence [ PDFDoc ], output : Dict ) -> Sequence [ PDFDoc ]: for b , label in zip ( ( b for doc in docs for b in doc . lines ), output [ \"labels\" ] . cpu () . tolist (), ): if b . text == \"\" : b . label = None else : b . label = self . label_vocabulary . decode ( label ) return docs __init__ ( embedding , labels = (), activation = 'gelu' , dropout_p = 0.15 , scorer = classifier_scorer ) Runs a deep learning classifier model on the boxes. PARAMETER DESCRIPTION labels Initial labels of the classifier (will be completed during initialization) TYPE: Sequence [ str ] DEFAULT: () embedding Embedding module to encode the PDF boxes TYPE: Module activation Name of the activation function TYPE: ActivationFunction DEFAULT: 'gelu' dropout_p Dropout probability used on the output of the box and textual encoders TYPE: float DEFAULT: 0.15 scorer Scoring function TYPE: Scorer DEFAULT: classifier_scorer Source code in edspdf/components/classifiers/deep_classifier.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , embedding : Module , labels : Sequence [ str ] = (), activation : ActivationFunction = \"gelu\" , dropout_p : float = 0.15 , scorer : Scorer = classifier_scorer , ): \"\"\" Runs a deep learning classifier model on the boxes. Parameters ---------- labels: Sequence[str] Initial labels of the classifier (will be completed during initialization) embedding: Module Embedding module to encode the PDF boxes activation: ActivationFunction Name of the activation function dropout_p: float Dropout probability used on the output of the box and textual encoders scorer: Scorer Scoring function \"\"\" super () . __init__ () self . label_vocabulary : Vocabulary = Vocabulary ( list ( dict . fromkeys ([ \"pollution\" , * labels ])) ) self . embedding : Module = embedding size = self . embedding . output_size self . linear = torch . nn . Linear ( size , size ) self . classifier : torch . nn . Linear = None # noqa self . activation = get_activation_function ( activation ) self . dropout = torch . nn . Dropout ( dropout_p ) # Scoring function self . scorer = scorer","title":"deep_classifier"},{"location":"reference/components/classifiers/deep_classifier/#edspdfcomponentsclassifiersdeep_classifier","text":"","title":"edspdf.components.classifiers.deep_classifier"},{"location":"reference/components/classifiers/deep_classifier/#edspdf.components.classifiers.deep_classifier.DeepClassifier","text":"Bases: TrainableComponent [ PDFDoc , Dict [ str , Any ], PDFDoc ] Source code in edspdf/components/classifiers/deep_classifier.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 @registry . factory . register ( \"deep-classifier\" ) class DeepClassifier ( TrainableComponent [ PDFDoc , Dict [ str , Any ], PDFDoc ]): def __init__ ( self , embedding : Module , labels : Sequence [ str ] = (), activation : ActivationFunction = \"gelu\" , dropout_p : float = 0.15 , scorer : Scorer = classifier_scorer , ): \"\"\" Runs a deep learning classifier model on the boxes. Parameters ---------- labels: Sequence[str] Initial labels of the classifier (will be completed during initialization) embedding: Module Embedding module to encode the PDF boxes activation: ActivationFunction Name of the activation function dropout_p: float Dropout probability used on the output of the box and textual encoders scorer: Scorer Scoring function \"\"\" super () . __init__ () self . label_vocabulary : Vocabulary = Vocabulary ( list ( dict . fromkeys ([ \"pollution\" , * labels ])) ) self . embedding : Module = embedding size = self . embedding . output_size self . linear = torch . nn . Linear ( size , size ) self . classifier : torch . nn . Linear = None # noqa self . activation = get_activation_function ( activation ) self . dropout = torch . nn . Dropout ( dropout_p ) # Scoring function self . scorer = scorer def initialize ( self , gold_data : Iterable [ PDFDoc ]): self . embedding . initialize ( gold_data ) with self . label_vocabulary . initialization (): for doc in tqdm ( gold_data , desc = \"Initializing classifier\" ): with self . no_cache (): self . preprocess ( doc , supervision = True ) self . classifier = torch . nn . Linear ( in_features = self . embedding . output_size , out_features = len ( self . label_vocabulary ), ) def preprocess ( self , doc : PDFDoc , supervision : bool = False ) -> Dict [ str , Any ]: result = { \"embedding\" : self . embedding . preprocess ( doc , supervision = supervision ), \"doc_id\" : doc . id , } if supervision : text_boxes = doc . lines result [ \"labels\" ] = [ self . label_vocabulary . encode ( b . label ) if b . label is not None else - 100 for b in text_boxes ] return result def collate ( self , batch , device : torch . device ) -> Dict : collated = { \"embedding\" : self . embedding . collate ( batch [ \"embedding\" ], device ), \"doc_id\" : batch [ \"doc_id\" ], } if \"labels\" in batch : collated . update ( { \"labels\" : torch . as_tensor ( flatten ( batch [ \"labels\" ]), device = device ), } ) return collated def forward ( self , batch : Dict , supervision = False ) -> Dict : embeds = self . embedding ( batch [ \"embedding\" ]) output = { \"loss\" : 0 } # Label prediction / learning logits = self . classifier ( embeds ) if supervision : targets = batch [ \"labels\" ] output [ \"label_loss\" ] = F . cross_entropy ( logits , targets , reduction = \"sum\" , ) output [ \"loss\" ] = output [ \"loss\" ] + output [ \"label_loss\" ] else : output [ \"logits\" ] = logits output [ \"labels\" ] = logits . argmax ( - 1 ) return output def postprocess ( self , docs : Sequence [ PDFDoc ], output : Dict ) -> Sequence [ PDFDoc ]: for b , label in zip ( ( b for doc in docs for b in doc . lines ), output [ \"labels\" ] . cpu () . tolist (), ): if b . text == \"\" : b . label = None else : b . label = self . label_vocabulary . decode ( label ) return docs","title":"DeepClassifier"},{"location":"reference/components/classifiers/deep_classifier/#edspdf.components.classifiers.deep_classifier.DeepClassifier.__init__","text":"Runs a deep learning classifier model on the boxes. PARAMETER DESCRIPTION labels Initial labels of the classifier (will be completed during initialization) TYPE: Sequence [ str ] DEFAULT: () embedding Embedding module to encode the PDF boxes TYPE: Module activation Name of the activation function TYPE: ActivationFunction DEFAULT: 'gelu' dropout_p Dropout probability used on the output of the box and textual encoders TYPE: float DEFAULT: 0.15 scorer Scoring function TYPE: Scorer DEFAULT: classifier_scorer Source code in edspdf/components/classifiers/deep_classifier.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __init__ ( self , embedding : Module , labels : Sequence [ str ] = (), activation : ActivationFunction = \"gelu\" , dropout_p : float = 0.15 , scorer : Scorer = classifier_scorer , ): \"\"\" Runs a deep learning classifier model on the boxes. Parameters ---------- labels: Sequence[str] Initial labels of the classifier (will be completed during initialization) embedding: Module Embedding module to encode the PDF boxes activation: ActivationFunction Name of the activation function dropout_p: float Dropout probability used on the output of the box and textual encoders scorer: Scorer Scoring function \"\"\" super () . __init__ () self . label_vocabulary : Vocabulary = Vocabulary ( list ( dict . fromkeys ([ \"pollution\" , * labels ])) ) self . embedding : Module = embedding size = self . embedding . output_size self . linear = torch . nn . Linear ( size , size ) self . classifier : torch . nn . Linear = None # noqa self . activation = get_activation_function ( activation ) self . dropout = torch . nn . Dropout ( dropout_p ) # Scoring function self . scorer = scorer","title":"__init__()"},{"location":"reference/components/classifiers/dummy/","text":"edspdf.components.classifiers.dummy DummyClassifier Bases: Component Dummy classifier, for chaos purposes. Classifies each line to a random element. Source code in edspdf/components/classifiers/dummy.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @registry . factory . register ( \"dummy-classifier\" ) class DummyClassifier ( Component ): \"\"\" Dummy classifier, for chaos purposes. Classifies each line to a random element. \"\"\" def __init__ ( self , label : str , ) -> None : super () . __init__ () self . label = label def __call__ ( self , doc : PDFDoc ) -> PDFDoc : for b in doc . lines : b . label = self . label return doc","title":"dummy"},{"location":"reference/components/classifiers/dummy/#edspdfcomponentsclassifiersdummy","text":"","title":"edspdf.components.classifiers.dummy"},{"location":"reference/components/classifiers/dummy/#edspdf.components.classifiers.dummy.DummyClassifier","text":"Bases: Component Dummy classifier, for chaos purposes. Classifies each line to a random element. Source code in edspdf/components/classifiers/dummy.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 @registry . factory . register ( \"dummy-classifier\" ) class DummyClassifier ( Component ): \"\"\" Dummy classifier, for chaos purposes. Classifies each line to a random element. \"\"\" def __init__ ( self , label : str , ) -> None : super () . __init__ () self . label = label def __call__ ( self , doc : PDFDoc ) -> PDFDoc : for b in doc . lines : b . label = self . label return doc","title":"DummyClassifier"},{"location":"reference/components/classifiers/harmonized_classifier/","text":"edspdf.components.classifiers.harmonized_classifier HarmonizedClassifier Bases: TrainableComponent [ PDFDoc , Dict [ str , Any ], PDFDoc ] Source code in edspdf/components/classifiers/harmonized_classifier.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 @registry . factory . register ( \"harmonized-classifier\" ) class HarmonizedClassifier ( TrainableComponent [ PDFDoc , Dict [ str , Any ], PDFDoc ]): def __init__ ( self , labels : Sequence [ str ], embedding : Module , activation : ActivationFunction = \"gelu\" , do_harmonize : bool = True , n_relative_positions : int = 64 , dropout_p : float = 0.15 , scorer : Scorer = classifier_scorer , ): \"\"\" Runs a deep learning classifier model on the boxes. Parameters ---------- labels: Sequence[str] Initial labels of the classifier (will be completed during initialization) embedding: Module Embedding module to encode the PDF boxes activation: ActivationFunction Name of the activation function do_harmonize: bool Perform the harmonization process n_relative_positions: int Maximum range of embeddable relative positions between boxes during the harmonization process dropout_p: float Dropout probability used on the output of the box and textual encoders scorer: Scorer Scoring function \"\"\" super () . __init__ () self . label_vocabulary : Vocabulary = Vocabulary ( list ( dict . fromkeys ([ \"pollution\" , * labels ])) ) self . embedding : Module = embedding size = self . embedding . output_size self . do_harmonize = do_harmonize if self . do_harmonize : self . n_relative_positions = n_relative_positions self . box_preprocessor = BoxLayoutPreprocessor () self . adjacency = RelativeAttention ( size = self . embedding . output_size , n_heads = 2 , do_pooling = False , head_size = self . embedding . output_size , position_embedding = torch . nn . Parameter ( torch . randn (( n_relative_positions , self . embedding . output_size )) ), dropout_p = 0 , n_coordinates = 2 , mode = ( RelativeAttentionMode . c2c , RelativeAttentionMode . c2p , RelativeAttentionMode . p2c , ), ) self . linear = torch . nn . Linear ( size , size ) self . classifier : torch . nn . Linear = None # noqa self . activation = get_activation_function ( activation ) self . dropout = torch . nn . Dropout ( dropout_p ) # Scoring function self . scorer = scorer def initialize ( self , gold_data : Iterable [ PDFDoc ]): self . embedding . initialize ( gold_data ) with self . label_vocabulary . initialization (): for doc in tqdm ( gold_data , desc = \"Initializing classifier\" ): with self . no_cache (): self . preprocess ( doc , supervision = True ) self . classifier = torch . nn . Linear ( in_features = self . embedding . output_size , out_features = len ( self . label_vocabulary ), ) def preprocess ( self , doc : PDFDoc , supervision : bool = False ) -> Dict [ str , Any ]: result = { \"embedding\" : self . embedding . preprocess ( doc , supervision = supervision ), \"doc_id\" : doc . id , } if self . do_harmonize : result [ \"boxes\" ] = self . box_preprocessor . preprocess ( doc , supervision = supervision ) if supervision : text_boxes = doc . lines result [ \"labels\" ] = [ self . label_vocabulary . encode ( b . label ) if b . label is not None else - 100 for b in text_boxes ] if self . do_harmonize : next_indices = [ - 1 ] * len ( text_boxes ) prev_indices = [ - 1 ] * len ( text_boxes ) for i , b in enumerate ( doc . lines ): if b . next_box_idx is not None : next_i = b . next_box_idx next_indices [ i ] = next_i prev_indices [ next_i ] = i result [ \"next_indices\" ] = next_indices result [ \"prev_indices\" ] = prev_indices return result def collate ( self , batch , device : torch . device ) -> Dict : collated = { \"embedding\" : self . embedding . collate ( batch [ \"embedding\" ], device ), \"doc_id\" : batch [ \"doc_id\" ], } if \"labels\" in batch : collated . update ( { \"labels\" : torch . as_tensor ( flatten ( batch [ \"labels\" ]), device = device ), } ) if \"next_indices\" in batch : shifted_next_indices = [] shifted_prev_indices = [] for doc_next_indices in batch [ \"next_indices\" ]: offset = len ( shifted_next_indices ) shifted_next_indices . extend ( [ i + offset if i > 0 else - 1 for i in doc_next_indices ] ) for doc_prev_indices in batch [ \"prev_indices\" ]: offset = len ( shifted_prev_indices ) shifted_prev_indices . extend ( [ i + offset if i > 0 else - 1 for i in doc_prev_indices ] ) collated . update ( { \"next_indices\" : torch . as_tensor ( shifted_next_indices , device = device ), \"prev_indices\" : torch . as_tensor ( shifted_prev_indices , device = device ), } ) if \"boxes\" in batch : collated . update ( { \"boxes\" : self . box_preprocessor . collate ( batch [ \"boxes\" ], device ), } ) return collated def harmonize ( self , block_label_logits , next_logits , prev_logits ): n_pages , n_blocks , n_labels = block_label_logits . shape device = block_label_logits . device constraints = torch . eye ( n_labels , n_labels + 1 , device = device ) constraints [:, - 1 ] = 1 is_next_logits = next_logits . log_softmax ( - 1 ) is_prev_logits = prev_logits . log_softmax ( - 1 ) label_logits = torch . full ( ( n_pages , n_blocks + 1 , n_labels + 1 ), IMPOSSIBLE , device = device ) label_logits [: n_pages , : n_blocks , : n_labels ] = block_label_logits label_logits [:, - 1 , - 1 ] = 0 label_logits = updated_label_logits = label_logits . log_softmax ( - 1 ) label_update_logits = torch . zeros_like ( label_logits ) for _ in range ( 9 ): next_label_logits = log_einsum_exp ( \"pij,pjx->pix\" , is_next_logits , updated_label_logits , ) prev_label_logits = log_einsum_exp ( \"pij,pjx->pix\" , is_prev_logits , updated_label_logits , ) label_update_logits [ ... , : n_blocks , : n_labels ] = 0.05 * label_update_logits [ ... , : n_blocks , : n_labels ] + 0.95 * ( log_einsum_exp ( \"piy,xy->pix\" , next_label_logits , constraints . log (), ) + log_einsum_exp ( \"piy,xy->pix\" , prev_label_logits , constraints . log (), ) ) updated_label_logits = ( label_update_logits + label_logits ) . log_softmax ( - 1 ) return updated_label_logits [: n_pages , : n_blocks , : n_labels ] def forward ( self , batch : Dict , supervision = False ) -> Dict : embeds = self . embedding ( batch [ \"embedding\" ]) device = embeds . device # ex: [[0, 2, -1], [1, 3, 4]] output = { \"loss\" : 0 } next_scores = prev_scores = None if self . do_harmonize : page_boxes = batch [ \"boxes\" ][ \"page_ids\" ] page_boxes_mask = page_boxes != - 1 relative_positions = compute_pdf_relative_positions ( x0 = batch [ \"boxes\" ][ \"xmin\" ][ page_boxes ], x1 = batch [ \"boxes\" ][ \"xmax\" ][ page_boxes ], y0 = batch [ \"boxes\" ][ \"ymin\" ][ page_boxes ], y1 = batch [ \"boxes\" ][ \"ymax\" ][ page_boxes ], width = batch [ \"boxes\" ][ \"width\" ][ page_boxes ], height = batch [ \"boxes\" ][ \"height\" ][ page_boxes ], n_relative_positions = self . n_relative_positions , ) # n_pages * n_blocks * n_blocks * 2 # How to go from initial blocks ([0, 1, 2, 3, 4]) to per-page indice ? idx_in_page = page_boxes # [[0, 2, -1], [1, 3, 4]] idx_in_page = idx_in_page . view ( - 1 ) # [0, 2, -1, 1, 3, 4] idx_in_page = idx_in_page % len ( idx_in_page + 1 ) # [0, 2, 5, 1, 3, 4] idx_in_page = idx_in_page . argsort ()[ : page_boxes_mask . sum () ] # [0, 3, 1, 4, 5]... and removed [, 2] idx_in_page = idx_in_page % page_boxes . shape [ - 1 ] # [0, 0, 1, 1, 2] page_range = torch . arange ( page_boxes_mask . shape [ - 1 ], device = device ) # During the training, to learn adjacency, the two boxes must exist next_mask : torch . BoolTensor = page_boxes_mask [:, :, None ] & page_boxes_mask [:, None , :] # During the training, to learn adjacency, the two boxes must exist prev_mask : torch . BoolTensor = page_boxes_mask [:, :, None ] & page_boxes_mask [:, None , :] next_scores , prev_scores = self . adjacency ( embeds [ page_boxes ], relative_positions = relative_positions , ) . unbind ( - 1 ) next_scores = next_scores . masked_fill ( ~ next_mask , IMPOSSIBLE ) prev_scores = prev_scores . masked_fill ( ~ prev_mask , IMPOSSIBLE ) next_scores = torch . cat ( [ next_scores , torch . zeros_like ( next_scores [:, :, : 1 ])], dim = 2 ) prev_scores = torch . cat ( [ prev_scores , torch . zeros_like ( prev_scores [:, :, : 1 ])], dim = 2 ) if supervision : labels_per_page = batch [ \"labels\" ][ page_boxes ] # n_blocks -> indice in page next_target = idx_in_page [ batch [ \"next_indices\" ]] next_target [ batch [ \"next_indices\" ] == - 1 ] = next_scores . shape [ - 1 ] - 1 next_target = next_target [:, None ] == torch . arange ( next_scores . shape [ - 1 ], device = device ) next_target [ batch [ \"next_indices\" ] == - 1 , : - 1 ] = ( ( labels_per_page [:, :, None ] == labels_per_page [:, None , :]) & ( page_range [ None , :, None ] < page_range [ None , None , :]) & next_mask )[ page_boxes_mask ][ batch [ \"next_indices\" ] == - 1 ] # n_blocks -> indice in page prev_target = idx_in_page [ batch [ \"prev_indices\" ]] prev_target [ batch [ \"prev_indices\" ] == - 1 ] = prev_scores . shape [ - 1 ] - 1 prev_target = prev_target [:, None ] == torch . arange ( prev_scores . shape [ - 1 ], device = device ) prev_target [ batch [ \"prev_indices\" ] == - 1 , : - 1 ] = ( ( labels_per_page [:, :, None ] == labels_per_page [:, None , :]) & ( page_range [ None , :, None ] > page_range [ None , None , :]) & prev_mask )[ page_boxes_mask ][ batch [ \"prev_indices\" ] == - 1 ] output [ \"adj_loss\" ] = - ( torch . log_softmax ( next_scores [ page_boxes_mask ], dim =- 1 ) . masked_fill ( ~ next_target , - 10000 ) . logsumexp ( - 1 ) . sum () + torch . log_softmax ( prev_scores [ page_boxes_mask ], dim =- 1 ) . masked_fill ( ~ prev_target , - 10000 ) . logsumexp ( - 1 ) . sum () ) output [ \"loss\" ] = output [ \"loss\" ] + output [ \"adj_loss\" ] output [ \"next_target\" ] = next_target output [ \"prev_target\" ] = prev_target # Label prediction / learning logits = self . classifier ( embeds ) if supervision : targets = batch [ \"labels\" ] output [ \"label_loss\" ] = ( F . cross_entropy ( logits , targets , reduction = \"sum\" , ) / 100 ) ** 2 output [ \"loss\" ] = output [ \"loss\" ] + output [ \"label_loss\" ] else : if self . do_harmonize : logits = self . harmonize ( logits [ page_boxes ], next_scores , prev_scores , )[ page_boxes_mask ] output [ \"next_scores\" ] = next_scores output [ \"prev_scores\" ] = prev_scores output [ \"logits\" ] = logits output [ \"labels\" ] = logits . argmax ( - 1 ) return output def postprocess ( self , docs : Sequence [ PDFDoc ], output : Dict ) -> Sequence [ PDFDoc ]: for b , label in zip ( ( b for doc in docs for b in doc . lines ), output [ \"labels\" ] . cpu () . tolist (), ): if b . text == \"\" : b . label = None else : b . label = self . label_vocabulary . decode ( label ) return docs __init__ ( labels , embedding , activation = 'gelu' , do_harmonize = True , n_relative_positions = 64 , dropout_p = 0.15 , scorer = classifier_scorer ) Runs a deep learning classifier model on the boxes. PARAMETER DESCRIPTION labels Initial labels of the classifier (will be completed during initialization) TYPE: Sequence [ str ] embedding Embedding module to encode the PDF boxes TYPE: Module activation Name of the activation function TYPE: ActivationFunction DEFAULT: 'gelu' do_harmonize Perform the harmonization process TYPE: bool DEFAULT: True n_relative_positions Maximum range of embeddable relative positions between boxes during the harmonization process TYPE: int DEFAULT: 64 dropout_p Dropout probability used on the output of the box and textual encoders TYPE: float DEFAULT: 0.15 scorer Scoring function TYPE: Scorer DEFAULT: classifier_scorer Source code in edspdf/components/classifiers/harmonized_classifier.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , labels : Sequence [ str ], embedding : Module , activation : ActivationFunction = \"gelu\" , do_harmonize : bool = True , n_relative_positions : int = 64 , dropout_p : float = 0.15 , scorer : Scorer = classifier_scorer , ): \"\"\" Runs a deep learning classifier model on the boxes. Parameters ---------- labels: Sequence[str] Initial labels of the classifier (will be completed during initialization) embedding: Module Embedding module to encode the PDF boxes activation: ActivationFunction Name of the activation function do_harmonize: bool Perform the harmonization process n_relative_positions: int Maximum range of embeddable relative positions between boxes during the harmonization process dropout_p: float Dropout probability used on the output of the box and textual encoders scorer: Scorer Scoring function \"\"\" super () . __init__ () self . label_vocabulary : Vocabulary = Vocabulary ( list ( dict . fromkeys ([ \"pollution\" , * labels ])) ) self . embedding : Module = embedding size = self . embedding . output_size self . do_harmonize = do_harmonize if self . do_harmonize : self . n_relative_positions = n_relative_positions self . box_preprocessor = BoxLayoutPreprocessor () self . adjacency = RelativeAttention ( size = self . embedding . output_size , n_heads = 2 , do_pooling = False , head_size = self . embedding . output_size , position_embedding = torch . nn . Parameter ( torch . randn (( n_relative_positions , self . embedding . output_size )) ), dropout_p = 0 , n_coordinates = 2 , mode = ( RelativeAttentionMode . c2c , RelativeAttentionMode . c2p , RelativeAttentionMode . p2c , ), ) self . linear = torch . nn . Linear ( size , size ) self . classifier : torch . nn . Linear = None # noqa self . activation = get_activation_function ( activation ) self . dropout = torch . nn . Dropout ( dropout_p ) # Scoring function self . scorer = scorer","title":"harmonized_classifier"},{"location":"reference/components/classifiers/harmonized_classifier/#edspdfcomponentsclassifiersharmonized_classifier","text":"","title":"edspdf.components.classifiers.harmonized_classifier"},{"location":"reference/components/classifiers/harmonized_classifier/#edspdf.components.classifiers.harmonized_classifier.HarmonizedClassifier","text":"Bases: TrainableComponent [ PDFDoc , Dict [ str , Any ], PDFDoc ] Source code in edspdf/components/classifiers/harmonized_classifier.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 @registry . factory . register ( \"harmonized-classifier\" ) class HarmonizedClassifier ( TrainableComponent [ PDFDoc , Dict [ str , Any ], PDFDoc ]): def __init__ ( self , labels : Sequence [ str ], embedding : Module , activation : ActivationFunction = \"gelu\" , do_harmonize : bool = True , n_relative_positions : int = 64 , dropout_p : float = 0.15 , scorer : Scorer = classifier_scorer , ): \"\"\" Runs a deep learning classifier model on the boxes. Parameters ---------- labels: Sequence[str] Initial labels of the classifier (will be completed during initialization) embedding: Module Embedding module to encode the PDF boxes activation: ActivationFunction Name of the activation function do_harmonize: bool Perform the harmonization process n_relative_positions: int Maximum range of embeddable relative positions between boxes during the harmonization process dropout_p: float Dropout probability used on the output of the box and textual encoders scorer: Scorer Scoring function \"\"\" super () . __init__ () self . label_vocabulary : Vocabulary = Vocabulary ( list ( dict . fromkeys ([ \"pollution\" , * labels ])) ) self . embedding : Module = embedding size = self . embedding . output_size self . do_harmonize = do_harmonize if self . do_harmonize : self . n_relative_positions = n_relative_positions self . box_preprocessor = BoxLayoutPreprocessor () self . adjacency = RelativeAttention ( size = self . embedding . output_size , n_heads = 2 , do_pooling = False , head_size = self . embedding . output_size , position_embedding = torch . nn . Parameter ( torch . randn (( n_relative_positions , self . embedding . output_size )) ), dropout_p = 0 , n_coordinates = 2 , mode = ( RelativeAttentionMode . c2c , RelativeAttentionMode . c2p , RelativeAttentionMode . p2c , ), ) self . linear = torch . nn . Linear ( size , size ) self . classifier : torch . nn . Linear = None # noqa self . activation = get_activation_function ( activation ) self . dropout = torch . nn . Dropout ( dropout_p ) # Scoring function self . scorer = scorer def initialize ( self , gold_data : Iterable [ PDFDoc ]): self . embedding . initialize ( gold_data ) with self . label_vocabulary . initialization (): for doc in tqdm ( gold_data , desc = \"Initializing classifier\" ): with self . no_cache (): self . preprocess ( doc , supervision = True ) self . classifier = torch . nn . Linear ( in_features = self . embedding . output_size , out_features = len ( self . label_vocabulary ), ) def preprocess ( self , doc : PDFDoc , supervision : bool = False ) -> Dict [ str , Any ]: result = { \"embedding\" : self . embedding . preprocess ( doc , supervision = supervision ), \"doc_id\" : doc . id , } if self . do_harmonize : result [ \"boxes\" ] = self . box_preprocessor . preprocess ( doc , supervision = supervision ) if supervision : text_boxes = doc . lines result [ \"labels\" ] = [ self . label_vocabulary . encode ( b . label ) if b . label is not None else - 100 for b in text_boxes ] if self . do_harmonize : next_indices = [ - 1 ] * len ( text_boxes ) prev_indices = [ - 1 ] * len ( text_boxes ) for i , b in enumerate ( doc . lines ): if b . next_box_idx is not None : next_i = b . next_box_idx next_indices [ i ] = next_i prev_indices [ next_i ] = i result [ \"next_indices\" ] = next_indices result [ \"prev_indices\" ] = prev_indices return result def collate ( self , batch , device : torch . device ) -> Dict : collated = { \"embedding\" : self . embedding . collate ( batch [ \"embedding\" ], device ), \"doc_id\" : batch [ \"doc_id\" ], } if \"labels\" in batch : collated . update ( { \"labels\" : torch . as_tensor ( flatten ( batch [ \"labels\" ]), device = device ), } ) if \"next_indices\" in batch : shifted_next_indices = [] shifted_prev_indices = [] for doc_next_indices in batch [ \"next_indices\" ]: offset = len ( shifted_next_indices ) shifted_next_indices . extend ( [ i + offset if i > 0 else - 1 for i in doc_next_indices ] ) for doc_prev_indices in batch [ \"prev_indices\" ]: offset = len ( shifted_prev_indices ) shifted_prev_indices . extend ( [ i + offset if i > 0 else - 1 for i in doc_prev_indices ] ) collated . update ( { \"next_indices\" : torch . as_tensor ( shifted_next_indices , device = device ), \"prev_indices\" : torch . as_tensor ( shifted_prev_indices , device = device ), } ) if \"boxes\" in batch : collated . update ( { \"boxes\" : self . box_preprocessor . collate ( batch [ \"boxes\" ], device ), } ) return collated def harmonize ( self , block_label_logits , next_logits , prev_logits ): n_pages , n_blocks , n_labels = block_label_logits . shape device = block_label_logits . device constraints = torch . eye ( n_labels , n_labels + 1 , device = device ) constraints [:, - 1 ] = 1 is_next_logits = next_logits . log_softmax ( - 1 ) is_prev_logits = prev_logits . log_softmax ( - 1 ) label_logits = torch . full ( ( n_pages , n_blocks + 1 , n_labels + 1 ), IMPOSSIBLE , device = device ) label_logits [: n_pages , : n_blocks , : n_labels ] = block_label_logits label_logits [:, - 1 , - 1 ] = 0 label_logits = updated_label_logits = label_logits . log_softmax ( - 1 ) label_update_logits = torch . zeros_like ( label_logits ) for _ in range ( 9 ): next_label_logits = log_einsum_exp ( \"pij,pjx->pix\" , is_next_logits , updated_label_logits , ) prev_label_logits = log_einsum_exp ( \"pij,pjx->pix\" , is_prev_logits , updated_label_logits , ) label_update_logits [ ... , : n_blocks , : n_labels ] = 0.05 * label_update_logits [ ... , : n_blocks , : n_labels ] + 0.95 * ( log_einsum_exp ( \"piy,xy->pix\" , next_label_logits , constraints . log (), ) + log_einsum_exp ( \"piy,xy->pix\" , prev_label_logits , constraints . log (), ) ) updated_label_logits = ( label_update_logits + label_logits ) . log_softmax ( - 1 ) return updated_label_logits [: n_pages , : n_blocks , : n_labels ] def forward ( self , batch : Dict , supervision = False ) -> Dict : embeds = self . embedding ( batch [ \"embedding\" ]) device = embeds . device # ex: [[0, 2, -1], [1, 3, 4]] output = { \"loss\" : 0 } next_scores = prev_scores = None if self . do_harmonize : page_boxes = batch [ \"boxes\" ][ \"page_ids\" ] page_boxes_mask = page_boxes != - 1 relative_positions = compute_pdf_relative_positions ( x0 = batch [ \"boxes\" ][ \"xmin\" ][ page_boxes ], x1 = batch [ \"boxes\" ][ \"xmax\" ][ page_boxes ], y0 = batch [ \"boxes\" ][ \"ymin\" ][ page_boxes ], y1 = batch [ \"boxes\" ][ \"ymax\" ][ page_boxes ], width = batch [ \"boxes\" ][ \"width\" ][ page_boxes ], height = batch [ \"boxes\" ][ \"height\" ][ page_boxes ], n_relative_positions = self . n_relative_positions , ) # n_pages * n_blocks * n_blocks * 2 # How to go from initial blocks ([0, 1, 2, 3, 4]) to per-page indice ? idx_in_page = page_boxes # [[0, 2, -1], [1, 3, 4]] idx_in_page = idx_in_page . view ( - 1 ) # [0, 2, -1, 1, 3, 4] idx_in_page = idx_in_page % len ( idx_in_page + 1 ) # [0, 2, 5, 1, 3, 4] idx_in_page = idx_in_page . argsort ()[ : page_boxes_mask . sum () ] # [0, 3, 1, 4, 5]... and removed [, 2] idx_in_page = idx_in_page % page_boxes . shape [ - 1 ] # [0, 0, 1, 1, 2] page_range = torch . arange ( page_boxes_mask . shape [ - 1 ], device = device ) # During the training, to learn adjacency, the two boxes must exist next_mask : torch . BoolTensor = page_boxes_mask [:, :, None ] & page_boxes_mask [:, None , :] # During the training, to learn adjacency, the two boxes must exist prev_mask : torch . BoolTensor = page_boxes_mask [:, :, None ] & page_boxes_mask [:, None , :] next_scores , prev_scores = self . adjacency ( embeds [ page_boxes ], relative_positions = relative_positions , ) . unbind ( - 1 ) next_scores = next_scores . masked_fill ( ~ next_mask , IMPOSSIBLE ) prev_scores = prev_scores . masked_fill ( ~ prev_mask , IMPOSSIBLE ) next_scores = torch . cat ( [ next_scores , torch . zeros_like ( next_scores [:, :, : 1 ])], dim = 2 ) prev_scores = torch . cat ( [ prev_scores , torch . zeros_like ( prev_scores [:, :, : 1 ])], dim = 2 ) if supervision : labels_per_page = batch [ \"labels\" ][ page_boxes ] # n_blocks -> indice in page next_target = idx_in_page [ batch [ \"next_indices\" ]] next_target [ batch [ \"next_indices\" ] == - 1 ] = next_scores . shape [ - 1 ] - 1 next_target = next_target [:, None ] == torch . arange ( next_scores . shape [ - 1 ], device = device ) next_target [ batch [ \"next_indices\" ] == - 1 , : - 1 ] = ( ( labels_per_page [:, :, None ] == labels_per_page [:, None , :]) & ( page_range [ None , :, None ] < page_range [ None , None , :]) & next_mask )[ page_boxes_mask ][ batch [ \"next_indices\" ] == - 1 ] # n_blocks -> indice in page prev_target = idx_in_page [ batch [ \"prev_indices\" ]] prev_target [ batch [ \"prev_indices\" ] == - 1 ] = prev_scores . shape [ - 1 ] - 1 prev_target = prev_target [:, None ] == torch . arange ( prev_scores . shape [ - 1 ], device = device ) prev_target [ batch [ \"prev_indices\" ] == - 1 , : - 1 ] = ( ( labels_per_page [:, :, None ] == labels_per_page [:, None , :]) & ( page_range [ None , :, None ] > page_range [ None , None , :]) & prev_mask )[ page_boxes_mask ][ batch [ \"prev_indices\" ] == - 1 ] output [ \"adj_loss\" ] = - ( torch . log_softmax ( next_scores [ page_boxes_mask ], dim =- 1 ) . masked_fill ( ~ next_target , - 10000 ) . logsumexp ( - 1 ) . sum () + torch . log_softmax ( prev_scores [ page_boxes_mask ], dim =- 1 ) . masked_fill ( ~ prev_target , - 10000 ) . logsumexp ( - 1 ) . sum () ) output [ \"loss\" ] = output [ \"loss\" ] + output [ \"adj_loss\" ] output [ \"next_target\" ] = next_target output [ \"prev_target\" ] = prev_target # Label prediction / learning logits = self . classifier ( embeds ) if supervision : targets = batch [ \"labels\" ] output [ \"label_loss\" ] = ( F . cross_entropy ( logits , targets , reduction = \"sum\" , ) / 100 ) ** 2 output [ \"loss\" ] = output [ \"loss\" ] + output [ \"label_loss\" ] else : if self . do_harmonize : logits = self . harmonize ( logits [ page_boxes ], next_scores , prev_scores , )[ page_boxes_mask ] output [ \"next_scores\" ] = next_scores output [ \"prev_scores\" ] = prev_scores output [ \"logits\" ] = logits output [ \"labels\" ] = logits . argmax ( - 1 ) return output def postprocess ( self , docs : Sequence [ PDFDoc ], output : Dict ) -> Sequence [ PDFDoc ]: for b , label in zip ( ( b for doc in docs for b in doc . lines ), output [ \"labels\" ] . cpu () . tolist (), ): if b . text == \"\" : b . label = None else : b . label = self . label_vocabulary . decode ( label ) return docs","title":"HarmonizedClassifier"},{"location":"reference/components/classifiers/harmonized_classifier/#edspdf.components.classifiers.harmonized_classifier.HarmonizedClassifier.__init__","text":"Runs a deep learning classifier model on the boxes. PARAMETER DESCRIPTION labels Initial labels of the classifier (will be completed during initialization) TYPE: Sequence [ str ] embedding Embedding module to encode the PDF boxes TYPE: Module activation Name of the activation function TYPE: ActivationFunction DEFAULT: 'gelu' do_harmonize Perform the harmonization process TYPE: bool DEFAULT: True n_relative_positions Maximum range of embeddable relative positions between boxes during the harmonization process TYPE: int DEFAULT: 64 dropout_p Dropout probability used on the output of the box and textual encoders TYPE: float DEFAULT: 0.15 scorer Scoring function TYPE: Scorer DEFAULT: classifier_scorer Source code in edspdf/components/classifiers/harmonized_classifier.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __init__ ( self , labels : Sequence [ str ], embedding : Module , activation : ActivationFunction = \"gelu\" , do_harmonize : bool = True , n_relative_positions : int = 64 , dropout_p : float = 0.15 , scorer : Scorer = classifier_scorer , ): \"\"\" Runs a deep learning classifier model on the boxes. Parameters ---------- labels: Sequence[str] Initial labels of the classifier (will be completed during initialization) embedding: Module Embedding module to encode the PDF boxes activation: ActivationFunction Name of the activation function do_harmonize: bool Perform the harmonization process n_relative_positions: int Maximum range of embeddable relative positions between boxes during the harmonization process dropout_p: float Dropout probability used on the output of the box and textual encoders scorer: Scorer Scoring function \"\"\" super () . __init__ () self . label_vocabulary : Vocabulary = Vocabulary ( list ( dict . fromkeys ([ \"pollution\" , * labels ])) ) self . embedding : Module = embedding size = self . embedding . output_size self . do_harmonize = do_harmonize if self . do_harmonize : self . n_relative_positions = n_relative_positions self . box_preprocessor = BoxLayoutPreprocessor () self . adjacency = RelativeAttention ( size = self . embedding . output_size , n_heads = 2 , do_pooling = False , head_size = self . embedding . output_size , position_embedding = torch . nn . Parameter ( torch . randn (( n_relative_positions , self . embedding . output_size )) ), dropout_p = 0 , n_coordinates = 2 , mode = ( RelativeAttentionMode . c2c , RelativeAttentionMode . c2p , RelativeAttentionMode . p2c , ), ) self . linear = torch . nn . Linear ( size , size ) self . classifier : torch . nn . Linear = None # noqa self . activation = get_activation_function ( activation ) self . dropout = torch . nn . Dropout ( dropout_p ) # Scoring function self . scorer = scorer","title":"__init__()"},{"location":"reference/components/classifiers/mask/","text":"edspdf.components.classifiers.mask MaskClassifier Bases: Component Mask classifier, that reproduces the PdfBox behaviour. Source code in edspdf/components/classifiers/mask.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class MaskClassifier ( Component ): \"\"\" Mask classifier, that reproduces the PdfBox behaviour. \"\"\" def __init__ ( self , * ms : Box , threshold : float = 1.0 , ): super () . __init__ () masks = list ( ms ) masks . append ( Box ( label = \"pollution\" , x0 =- 10000 , x1 = 10000 , y0 =- 10000 , y1 = 10000 , ) ) self . masks = masks self . threshold = threshold def __call__ ( self , doc : PDFDoc ) -> PDFDoc : doc . lines = align_box_labels ( src_boxes = self . masks , dst_boxes = doc . lines , threshold = self . threshold , ) return doc","title":"mask"},{"location":"reference/components/classifiers/mask/#edspdfcomponentsclassifiersmask","text":"","title":"edspdf.components.classifiers.mask"},{"location":"reference/components/classifiers/mask/#edspdf.components.classifiers.mask.MaskClassifier","text":"Bases: Component Mask classifier, that reproduces the PdfBox behaviour. Source code in edspdf/components/classifiers/mask.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 class MaskClassifier ( Component ): \"\"\" Mask classifier, that reproduces the PdfBox behaviour. \"\"\" def __init__ ( self , * ms : Box , threshold : float = 1.0 , ): super () . __init__ () masks = list ( ms ) masks . append ( Box ( label = \"pollution\" , x0 =- 10000 , x1 = 10000 , y0 =- 10000 , y1 = 10000 , ) ) self . masks = masks self . threshold = threshold def __call__ ( self , doc : PDFDoc ) -> PDFDoc : doc . lines = align_box_labels ( src_boxes = self . masks , dst_boxes = doc . lines , threshold = self . threshold , ) return doc","title":"MaskClassifier"},{"location":"reference/components/classifiers/random/","text":"edspdf.components.classifiers.random RandomClassifier Bases: Component Random classifier, for chaos purposes. Classifies each line to a random element. Source code in edspdf/components/classifiers/random.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @registry . factory . register ( \"random-classifier\" ) class RandomClassifier ( Component ): \"\"\" Random classifier, for chaos purposes. Classifies each line to a random element. \"\"\" def __init__ ( self , labels : Union [ List [ str ], Dict [ str , float ]], seed : Optional [ int ] = 0 , ) -> None : super () . __init__ () if isinstance ( labels , list ): labels = { c : 1 for c in labels } self . labels = { c : w / sum ( labels . values ()) for c , w in labels . items ()} self . rgn = np . random . default_rng ( seed = seed ) def __call__ ( self , doc : PDFDoc ) -> PDFDoc : prediction = self . rgn . choice ( list ( self . labels . keys ()), p = list ( self . labels . values ()), size = len ( doc . lines ), ) for b , label in zip ( doc . lines , prediction ): b . label = label return doc","title":"random"},{"location":"reference/components/classifiers/random/#edspdfcomponentsclassifiersrandom","text":"","title":"edspdf.components.classifiers.random"},{"location":"reference/components/classifiers/random/#edspdf.components.classifiers.random.RandomClassifier","text":"Bases: Component Random classifier, for chaos purposes. Classifies each line to a random element. Source code in edspdf/components/classifiers/random.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 @registry . factory . register ( \"random-classifier\" ) class RandomClassifier ( Component ): \"\"\" Random classifier, for chaos purposes. Classifies each line to a random element. \"\"\" def __init__ ( self , labels : Union [ List [ str ], Dict [ str , float ]], seed : Optional [ int ] = 0 , ) -> None : super () . __init__ () if isinstance ( labels , list ): labels = { c : 1 for c in labels } self . labels = { c : w / sum ( labels . values ()) for c , w in labels . items ()} self . rgn = np . random . default_rng ( seed = seed ) def __call__ ( self , doc : PDFDoc ) -> PDFDoc : prediction = self . rgn . choice ( list ( self . labels . keys ()), p = list ( self . labels . values ()), size = len ( doc . lines ), ) for b , label in zip ( doc . lines , prediction ): b . label = label return doc","title":"RandomClassifier"},{"location":"reference/components/extractors/","text":"edspdf.components.extractors","title":"`edspdf.components.extractors`"},{"location":"reference/components/extractors/#edspdfcomponentsextractors","text":"","title":"edspdf.components.extractors"},{"location":"reference/components/extractors/pdfminer/","text":"edspdf.components.extractors.pdfminer PdfMinerExtractor Bases: Component Source code in edspdf/components/extractors/pdfminer.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 @registry . factory . register ( \"pdfminer-extractor\" ) class PdfMinerExtractor ( Component ): def __init__ ( self , line_overlap : float = 0.5 , char_margin : float = 2.05 , line_margin : float = 0.5 , word_margin : float = 0.1 , boxes_flow : Optional [ float ] = 0.5 , detect_vertical : bool = False , all_texts : bool = False , extract_style : bool = False , raise_on_error : bool = False , ): \"\"\" Extractor object. Given a PDF byte stream, produces a list of elements. Parameters ---------- line_overlap : float See PDFMiner documentation char_margin : float See PDFMiner documentation line_margin : float See PDFMiner documentation word_margin : float See PDFMiner documentation boxes_flow : Optional[float] See PDFMiner documentation detect_vertical : bool See PDFMiner documentation all_texts : bool See PDFMiner documentation extract_style : bool Whether to extract style (font, size, ...) information for each line of the document. Default: False \"\"\" super () . __init__ () self . laparams = LAParams ( line_overlap = line_overlap , char_margin = char_margin , line_margin = line_margin , word_margin = word_margin , boxes_flow = boxes_flow , detect_vertical = detect_vertical , all_texts = all_texts , ) self . extract_style = extract_style self . raise_on_error = raise_on_error def __call__ ( self , doc : Union [ PDFDoc , bytes ]) -> PDFDoc : \"\"\" Extract blocks from a PDF from all blocks in the PDF. Arguments --------- doc: PDF document Returns ------- PDFDoc: PDF document \"\"\" if not isinstance ( doc , PDFDoc ): content = bytes ( doc ) doc = PDFDoc ( id = str ( hash ( content )), content = content ) content = doc . content content_stream = BytesIO ( content ) try : layout = list ( extract_pages ( content_stream , laparams = self . laparams )) except PDFException : if self . raise_on_error : raise doc . lines = [] doc . error = True return doc lines = [] page_count = 0 for page_no , page in enumerate ( layout ): page_count += 1 w = page . width h = page . height for bloc in page : if not isinstance ( bloc , LTTextBoxHorizontal ): continue bloc : LTTextBoxHorizontal for line in bloc : text , styles = extract_style_from_line ( line ) if len ( text ) == 0 : continue lines . append ( TextBox ( page = page_no , x0 = line . x0 / w , x1 = line . x1 / w , y0 = 1 - line . y1 / h , y1 = 1 - line . y0 / h , page_width = w , page_height = h , text = text , styles = styles if self . extract_style else (), ) ) doc . lines = sorted ( [ line for line in lines if line . x0 >= 0 and line . y0 >= 0 and line . x1 <= 1 and line . y1 <= 1 ] ) return doc __init__ ( line_overlap = 0.5 , char_margin = 2.05 , line_margin = 0.5 , word_margin = 0.1 , boxes_flow = 0.5 , detect_vertical = False , all_texts = False , extract_style = False , raise_on_error = False ) Extractor object. Given a PDF byte stream, produces a list of elements. PARAMETER DESCRIPTION line_overlap See PDFMiner documentation TYPE: float DEFAULT: 0.5 char_margin See PDFMiner documentation TYPE: float DEFAULT: 2.05 line_margin See PDFMiner documentation TYPE: float DEFAULT: 0.5 word_margin See PDFMiner documentation TYPE: float DEFAULT: 0.1 boxes_flow See PDFMiner documentation TYPE: Optional [ float ] DEFAULT: 0.5 detect_vertical See PDFMiner documentation TYPE: bool DEFAULT: False all_texts See PDFMiner documentation TYPE: bool DEFAULT: False extract_style Whether to extract style (font, size, ...) information for each line of the document. Default: False TYPE: bool DEFAULT: False Source code in edspdf/components/extractors/pdfminer.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , line_overlap : float = 0.5 , char_margin : float = 2.05 , line_margin : float = 0.5 , word_margin : float = 0.1 , boxes_flow : Optional [ float ] = 0.5 , detect_vertical : bool = False , all_texts : bool = False , extract_style : bool = False , raise_on_error : bool = False , ): \"\"\" Extractor object. Given a PDF byte stream, produces a list of elements. Parameters ---------- line_overlap : float See PDFMiner documentation char_margin : float See PDFMiner documentation line_margin : float See PDFMiner documentation word_margin : float See PDFMiner documentation boxes_flow : Optional[float] See PDFMiner documentation detect_vertical : bool See PDFMiner documentation all_texts : bool See PDFMiner documentation extract_style : bool Whether to extract style (font, size, ...) information for each line of the document. Default: False \"\"\" super () . __init__ () self . laparams = LAParams ( line_overlap = line_overlap , char_margin = char_margin , line_margin = line_margin , word_margin = word_margin , boxes_flow = boxes_flow , detect_vertical = detect_vertical , all_texts = all_texts , ) self . extract_style = extract_style self . raise_on_error = raise_on_error __call__ ( doc ) Extract blocks from a PDF from all blocks in the PDF. Arguments doc: PDF document RETURNS DESCRIPTION PDFDoc PDF document TYPE: PDFDoc Source code in edspdf/components/extractors/pdfminer.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __call__ ( self , doc : Union [ PDFDoc , bytes ]) -> PDFDoc : \"\"\" Extract blocks from a PDF from all blocks in the PDF. Arguments --------- doc: PDF document Returns ------- PDFDoc: PDF document \"\"\" if not isinstance ( doc , PDFDoc ): content = bytes ( doc ) doc = PDFDoc ( id = str ( hash ( content )), content = content ) content = doc . content content_stream = BytesIO ( content ) try : layout = list ( extract_pages ( content_stream , laparams = self . laparams )) except PDFException : if self . raise_on_error : raise doc . lines = [] doc . error = True return doc lines = [] page_count = 0 for page_no , page in enumerate ( layout ): page_count += 1 w = page . width h = page . height for bloc in page : if not isinstance ( bloc , LTTextBoxHorizontal ): continue bloc : LTTextBoxHorizontal for line in bloc : text , styles = extract_style_from_line ( line ) if len ( text ) == 0 : continue lines . append ( TextBox ( page = page_no , x0 = line . x0 / w , x1 = line . x1 / w , y0 = 1 - line . y1 / h , y1 = 1 - line . y0 / h , page_width = w , page_height = h , text = text , styles = styles if self . extract_style else (), ) ) doc . lines = sorted ( [ line for line in lines if line . x0 >= 0 and line . y0 >= 0 and line . x1 <= 1 and line . y1 <= 1 ] ) return doc","title":"pdfminer"},{"location":"reference/components/extractors/pdfminer/#edspdfcomponentsextractorspdfminer","text":"","title":"edspdf.components.extractors.pdfminer"},{"location":"reference/components/extractors/pdfminer/#edspdf.components.extractors.pdfminer.PdfMinerExtractor","text":"Bases: Component Source code in edspdf/components/extractors/pdfminer.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 @registry . factory . register ( \"pdfminer-extractor\" ) class PdfMinerExtractor ( Component ): def __init__ ( self , line_overlap : float = 0.5 , char_margin : float = 2.05 , line_margin : float = 0.5 , word_margin : float = 0.1 , boxes_flow : Optional [ float ] = 0.5 , detect_vertical : bool = False , all_texts : bool = False , extract_style : bool = False , raise_on_error : bool = False , ): \"\"\" Extractor object. Given a PDF byte stream, produces a list of elements. Parameters ---------- line_overlap : float See PDFMiner documentation char_margin : float See PDFMiner documentation line_margin : float See PDFMiner documentation word_margin : float See PDFMiner documentation boxes_flow : Optional[float] See PDFMiner documentation detect_vertical : bool See PDFMiner documentation all_texts : bool See PDFMiner documentation extract_style : bool Whether to extract style (font, size, ...) information for each line of the document. Default: False \"\"\" super () . __init__ () self . laparams = LAParams ( line_overlap = line_overlap , char_margin = char_margin , line_margin = line_margin , word_margin = word_margin , boxes_flow = boxes_flow , detect_vertical = detect_vertical , all_texts = all_texts , ) self . extract_style = extract_style self . raise_on_error = raise_on_error def __call__ ( self , doc : Union [ PDFDoc , bytes ]) -> PDFDoc : \"\"\" Extract blocks from a PDF from all blocks in the PDF. Arguments --------- doc: PDF document Returns ------- PDFDoc: PDF document \"\"\" if not isinstance ( doc , PDFDoc ): content = bytes ( doc ) doc = PDFDoc ( id = str ( hash ( content )), content = content ) content = doc . content content_stream = BytesIO ( content ) try : layout = list ( extract_pages ( content_stream , laparams = self . laparams )) except PDFException : if self . raise_on_error : raise doc . lines = [] doc . error = True return doc lines = [] page_count = 0 for page_no , page in enumerate ( layout ): page_count += 1 w = page . width h = page . height for bloc in page : if not isinstance ( bloc , LTTextBoxHorizontal ): continue bloc : LTTextBoxHorizontal for line in bloc : text , styles = extract_style_from_line ( line ) if len ( text ) == 0 : continue lines . append ( TextBox ( page = page_no , x0 = line . x0 / w , x1 = line . x1 / w , y0 = 1 - line . y1 / h , y1 = 1 - line . y0 / h , page_width = w , page_height = h , text = text , styles = styles if self . extract_style else (), ) ) doc . lines = sorted ( [ line for line in lines if line . x0 >= 0 and line . y0 >= 0 and line . x1 <= 1 and line . y1 <= 1 ] ) return doc","title":"PdfMinerExtractor"},{"location":"reference/components/extractors/pdfminer/#edspdf.components.extractors.pdfminer.PdfMinerExtractor.__init__","text":"Extractor object. Given a PDF byte stream, produces a list of elements. PARAMETER DESCRIPTION line_overlap See PDFMiner documentation TYPE: float DEFAULT: 0.5 char_margin See PDFMiner documentation TYPE: float DEFAULT: 2.05 line_margin See PDFMiner documentation TYPE: float DEFAULT: 0.5 word_margin See PDFMiner documentation TYPE: float DEFAULT: 0.1 boxes_flow See PDFMiner documentation TYPE: Optional [ float ] DEFAULT: 0.5 detect_vertical See PDFMiner documentation TYPE: bool DEFAULT: False all_texts See PDFMiner documentation TYPE: bool DEFAULT: False extract_style Whether to extract style (font, size, ...) information for each line of the document. Default: False TYPE: bool DEFAULT: False Source code in edspdf/components/extractors/pdfminer.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def __init__ ( self , line_overlap : float = 0.5 , char_margin : float = 2.05 , line_margin : float = 0.5 , word_margin : float = 0.1 , boxes_flow : Optional [ float ] = 0.5 , detect_vertical : bool = False , all_texts : bool = False , extract_style : bool = False , raise_on_error : bool = False , ): \"\"\" Extractor object. Given a PDF byte stream, produces a list of elements. Parameters ---------- line_overlap : float See PDFMiner documentation char_margin : float See PDFMiner documentation line_margin : float See PDFMiner documentation word_margin : float See PDFMiner documentation boxes_flow : Optional[float] See PDFMiner documentation detect_vertical : bool See PDFMiner documentation all_texts : bool See PDFMiner documentation extract_style : bool Whether to extract style (font, size, ...) information for each line of the document. Default: False \"\"\" super () . __init__ () self . laparams = LAParams ( line_overlap = line_overlap , char_margin = char_margin , line_margin = line_margin , word_margin = word_margin , boxes_flow = boxes_flow , detect_vertical = detect_vertical , all_texts = all_texts , ) self . extract_style = extract_style self . raise_on_error = raise_on_error","title":"__init__()"},{"location":"reference/components/extractors/pdfminer/#edspdf.components.extractors.pdfminer.PdfMinerExtractor.__call__","text":"Extract blocks from a PDF from all blocks in the PDF.","title":"__call__()"},{"location":"reference/components/extractors/pdfminer/#edspdf.components.extractors.pdfminer.PdfMinerExtractor.__call__--arguments","text":"doc: PDF document RETURNS DESCRIPTION PDFDoc PDF document TYPE: PDFDoc Source code in edspdf/components/extractors/pdfminer.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __call__ ( self , doc : Union [ PDFDoc , bytes ]) -> PDFDoc : \"\"\" Extract blocks from a PDF from all blocks in the PDF. Arguments --------- doc: PDF document Returns ------- PDFDoc: PDF document \"\"\" if not isinstance ( doc , PDFDoc ): content = bytes ( doc ) doc = PDFDoc ( id = str ( hash ( content )), content = content ) content = doc . content content_stream = BytesIO ( content ) try : layout = list ( extract_pages ( content_stream , laparams = self . laparams )) except PDFException : if self . raise_on_error : raise doc . lines = [] doc . error = True return doc lines = [] page_count = 0 for page_no , page in enumerate ( layout ): page_count += 1 w = page . width h = page . height for bloc in page : if not isinstance ( bloc , LTTextBoxHorizontal ): continue bloc : LTTextBoxHorizontal for line in bloc : text , styles = extract_style_from_line ( line ) if len ( text ) == 0 : continue lines . append ( TextBox ( page = page_no , x0 = line . x0 / w , x1 = line . x1 / w , y0 = 1 - line . y1 / h , y1 = 1 - line . y0 / h , page_width = w , page_height = h , text = text , styles = styles if self . extract_style else (), ) ) doc . lines = sorted ( [ line for line in lines if line . x0 >= 0 and line . y0 >= 0 and line . x1 <= 1 and line . y1 <= 1 ] ) return doc","title":"Arguments"},{"location":"reference/layers/","text":"edspdf.layers","title":"`edspdf.layers`"},{"location":"reference/layers/#edspdflayers","text":"","title":"edspdf.layers"},{"location":"reference/layers/box_embedding/","text":"edspdf.layers.box_embedding BoxEmbedding Bases: Module Source code in edspdf/layers/box_embedding.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @registry . factory . register ( \"box-embedding\" ) class BoxEmbedding ( Module ): def __init__ ( self , size : int , dropout_p : float = 0.2 , layout_encoder : Optional [ Module ] = {}, text_encoder : Optional [ Module ] = None , contextualizer : Optional [ Module ] = None , ): \"\"\" Encodes boxes using a combination of layout and text features. Parameters ---------- size: int Size of the output box embedding dropout_p: float Dropout probability used on the output of the box and textual encoders text_encoder: Dict Config for the text encoder layout_encoder: Dict Config for the layout encoder \"\"\" super () . __init__ () assert size % 6 == 0 , \"Embedding dimension must be dividable by 6\" self . size = size self . layout_encoder = layout_encoder self . text_encoder = text_encoder self . contextualizer = contextualizer self . dropout = torch . nn . Dropout ( dropout_p ) @property def output_size ( self ): return self . size def initialize ( self , gold_data , ** kwargs ): super () . initialize ( gold_data , ** kwargs ) if self . text_encoder is not None : self . text_encoder . initialize ( gold_data , size = self . size ) if self . layout_encoder is not None : self . layout_encoder . initialize ( gold_data , size = self . size ) if self . contextualizer is not None : self . contextualizer . initialize ( gold_data , input_size = self . size ) def preprocess ( self , doc , supervision : bool = False ): return { \"boxes\" : self . layout_encoder . preprocess ( doc , supervision = supervision ) if self . layout_encoder is not None else None , \"texts\" : self . text_encoder . preprocess ( doc , supervision = supervision ) if self . text_encoder is not None else None , } def collate ( self , batch , device : torch . device ): return { \"texts\" : self . text_encoder . collate ( batch [ \"texts\" ], device ) if self . text_encoder is not None else None , \"boxes\" : self . layout_encoder . collate ( batch [ \"boxes\" ], device ) if self . layout_encoder is not None else None , } def forward ( self , batch , supervision = False ): embeds = sum ( [ self . dropout ( encoder . module_forward ( batch [ name ])) for name , encoder in ( ( \"boxes\" , self . layout_encoder ), ( \"texts\" , self . text_encoder ), ) if encoder is not None ] ) if self . contextualizer is not None : embeds = self . contextualizer ( embeds = embeds , boxes = batch [ \"boxes\" ], ) return embeds __init__ ( size , dropout_p = 0.2 , layout_encoder = {}, text_encoder = None , contextualizer = None ) Encodes boxes using a combination of layout and text features. PARAMETER DESCRIPTION size Size of the output box embedding TYPE: int dropout_p Dropout probability used on the output of the box and textual encoders TYPE: float DEFAULT: 0.2 text_encoder Config for the text encoder TYPE: Optional [ Module ] DEFAULT: None layout_encoder Config for the layout encoder TYPE: Optional [ Module ] DEFAULT: {} Source code in edspdf/layers/box_embedding.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , size : int , dropout_p : float = 0.2 , layout_encoder : Optional [ Module ] = {}, text_encoder : Optional [ Module ] = None , contextualizer : Optional [ Module ] = None , ): \"\"\" Encodes boxes using a combination of layout and text features. Parameters ---------- size: int Size of the output box embedding dropout_p: float Dropout probability used on the output of the box and textual encoders text_encoder: Dict Config for the text encoder layout_encoder: Dict Config for the layout encoder \"\"\" super () . __init__ () assert size % 6 == 0 , \"Embedding dimension must be dividable by 6\" self . size = size self . layout_encoder = layout_encoder self . text_encoder = text_encoder self . contextualizer = contextualizer self . dropout = torch . nn . Dropout ( dropout_p )","title":"box_embedding"},{"location":"reference/layers/box_embedding/#edspdflayersbox_embedding","text":"","title":"edspdf.layers.box_embedding"},{"location":"reference/layers/box_embedding/#edspdf.layers.box_embedding.BoxEmbedding","text":"Bases: Module Source code in edspdf/layers/box_embedding.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 @registry . factory . register ( \"box-embedding\" ) class BoxEmbedding ( Module ): def __init__ ( self , size : int , dropout_p : float = 0.2 , layout_encoder : Optional [ Module ] = {}, text_encoder : Optional [ Module ] = None , contextualizer : Optional [ Module ] = None , ): \"\"\" Encodes boxes using a combination of layout and text features. Parameters ---------- size: int Size of the output box embedding dropout_p: float Dropout probability used on the output of the box and textual encoders text_encoder: Dict Config for the text encoder layout_encoder: Dict Config for the layout encoder \"\"\" super () . __init__ () assert size % 6 == 0 , \"Embedding dimension must be dividable by 6\" self . size = size self . layout_encoder = layout_encoder self . text_encoder = text_encoder self . contextualizer = contextualizer self . dropout = torch . nn . Dropout ( dropout_p ) @property def output_size ( self ): return self . size def initialize ( self , gold_data , ** kwargs ): super () . initialize ( gold_data , ** kwargs ) if self . text_encoder is not None : self . text_encoder . initialize ( gold_data , size = self . size ) if self . layout_encoder is not None : self . layout_encoder . initialize ( gold_data , size = self . size ) if self . contextualizer is not None : self . contextualizer . initialize ( gold_data , input_size = self . size ) def preprocess ( self , doc , supervision : bool = False ): return { \"boxes\" : self . layout_encoder . preprocess ( doc , supervision = supervision ) if self . layout_encoder is not None else None , \"texts\" : self . text_encoder . preprocess ( doc , supervision = supervision ) if self . text_encoder is not None else None , } def collate ( self , batch , device : torch . device ): return { \"texts\" : self . text_encoder . collate ( batch [ \"texts\" ], device ) if self . text_encoder is not None else None , \"boxes\" : self . layout_encoder . collate ( batch [ \"boxes\" ], device ) if self . layout_encoder is not None else None , } def forward ( self , batch , supervision = False ): embeds = sum ( [ self . dropout ( encoder . module_forward ( batch [ name ])) for name , encoder in ( ( \"boxes\" , self . layout_encoder ), ( \"texts\" , self . text_encoder ), ) if encoder is not None ] ) if self . contextualizer is not None : embeds = self . contextualizer ( embeds = embeds , boxes = batch [ \"boxes\" ], ) return embeds","title":"BoxEmbedding"},{"location":"reference/layers/box_embedding/#edspdf.layers.box_embedding.BoxEmbedding.__init__","text":"Encodes boxes using a combination of layout and text features. PARAMETER DESCRIPTION size Size of the output box embedding TYPE: int dropout_p Dropout probability used on the output of the box and textual encoders TYPE: float DEFAULT: 0.2 text_encoder Config for the text encoder TYPE: Optional [ Module ] DEFAULT: None layout_encoder Config for the layout encoder TYPE: Optional [ Module ] DEFAULT: {} Source code in edspdf/layers/box_embedding.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , size : int , dropout_p : float = 0.2 , layout_encoder : Optional [ Module ] = {}, text_encoder : Optional [ Module ] = None , contextualizer : Optional [ Module ] = None , ): \"\"\" Encodes boxes using a combination of layout and text features. Parameters ---------- size: int Size of the output box embedding dropout_p: float Dropout probability used on the output of the box and textual encoders text_encoder: Dict Config for the text encoder layout_encoder: Dict Config for the layout encoder \"\"\" super () . __init__ () assert size % 6 == 0 , \"Embedding dimension must be dividable by 6\" self . size = size self . layout_encoder = layout_encoder self . text_encoder = text_encoder self . contextualizer = contextualizer self . dropout = torch . nn . Dropout ( dropout_p )","title":"__init__()"},{"location":"reference/layers/box_layout_embedding/","text":"edspdf.layers.box_layout_embedding BoxLayoutEmbedding Bases: Module Encodes a box using its geometrical features, as extracted by the BoxLayoutPreprocessor module. Source code in edspdf/layers/box_layout_embedding.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 @registry . factory . register ( \"box-layout-embedding\" ) class BoxLayoutEmbedding ( Module ): \"\"\" Encodes a box using its geometrical features, as extracted by the BoxLayoutPreprocessor module. \"\"\" def __init__ ( self , n_positions : int , size : Optional [ int ] = None , x_mode : PositionEmbeddingMode = \"sin\" , y_mode : PositionEmbeddingMode = \"sin\" , w_mode : PositionEmbeddingMode = \"sin\" , h_mode : PositionEmbeddingMode = \"sin\" , ): \"\"\" Parameters ---------- size: int Size of the output box embedding n_positions: int Number of position embeddings stored in the PositionEmbedding module x_mode: PositionEmbeddingMode Position embedding mode of the x coordinates y_mode: PositionEmbeddingMode Position embedding mode of the x coordinates w_mode: PositionEmbeddingMode Position embedding mode of the width features h_mode: PositionEmbeddingMode Position embedding mode of the height features \"\"\" super () . __init__ () self . n_positions = n_positions self . size = size self . x_mode = x_mode self . y_mode = y_mode self . w_mode = w_mode self . h_mode = h_mode self . x_embedding = None self . y_embedding = None self . w_embedding = None self . h_embedding = None self . first_page_embedding = None self . last_page_embedding = None self . box_preprocessor = BoxLayoutPreprocessor () self . preprocess = self . box_preprocessor . preprocess self . collate = self . box_preprocessor . collate def initialize ( self , gold_data : Iterable , size : int = None , ** kwargs ): super () . initialize ( gold_data , size = size , ** kwargs ) n_pos , size = self . n_positions , self . size self . x_embedding = self . _make_embed ( n_pos , size // 6 , self . x_mode ) self . y_embedding = self . _make_embed ( n_pos , size // 6 , self . y_mode ) self . w_embedding = self . _make_embed ( n_pos , size // 6 , self . w_mode ) self . h_embedding = self . _make_embed ( n_pos , size // 6 , self . h_mode ) self . first_page_embedding = torch . nn . Parameter ( torch . randn ( self . size )) self . last_page_embedding = torch . nn . Parameter ( torch . randn ( self . size )) @classmethod def _make_embed ( cls , n_positions , size , mode ): if mode == \"sin\" : return SinusoidalEmbedding ( n_positions , size ) else : return torch . nn . Embedding ( n_positions , size ) def forward ( self , batch ): return ( torch . cat ( [ self . x_embedding ( ( batch [ \"xmin\" ] * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), self . y_embedding ( ( batch [ \"ymin\" ] * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), self . x_embedding ( ( batch [ \"xmax\" ] * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), self . y_embedding ( ( batch [ \"ymax\" ] * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), self . w_embedding ( ( batch [ \"width\" ] * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), self . h_embedding ( ( batch [ \"height\" ] * 5 * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), ], dim =- 1 , ) + self . first_page_embedding * batch [ \"first_page\" ][ ... , None ] + self . last_page_embedding * batch [ \"last_page\" ][ ... , None ] ) __init__ ( n_positions , size = None , x_mode = 'sin' , y_mode = 'sin' , w_mode = 'sin' , h_mode = 'sin' ) PARAMETER DESCRIPTION size Size of the output box embedding TYPE: Optional [ int ] DEFAULT: None n_positions Number of position embeddings stored in the PositionEmbedding module TYPE: int x_mode Position embedding mode of the x coordinates TYPE: PositionEmbeddingMode DEFAULT: 'sin' y_mode Position embedding mode of the x coordinates TYPE: PositionEmbeddingMode DEFAULT: 'sin' w_mode Position embedding mode of the width features TYPE: PositionEmbeddingMode DEFAULT: 'sin' h_mode Position embedding mode of the height features TYPE: PositionEmbeddingMode DEFAULT: 'sin' Source code in edspdf/layers/box_layout_embedding.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , n_positions : int , size : Optional [ int ] = None , x_mode : PositionEmbeddingMode = \"sin\" , y_mode : PositionEmbeddingMode = \"sin\" , w_mode : PositionEmbeddingMode = \"sin\" , h_mode : PositionEmbeddingMode = \"sin\" , ): \"\"\" Parameters ---------- size: int Size of the output box embedding n_positions: int Number of position embeddings stored in the PositionEmbedding module x_mode: PositionEmbeddingMode Position embedding mode of the x coordinates y_mode: PositionEmbeddingMode Position embedding mode of the x coordinates w_mode: PositionEmbeddingMode Position embedding mode of the width features h_mode: PositionEmbeddingMode Position embedding mode of the height features \"\"\" super () . __init__ () self . n_positions = n_positions self . size = size self . x_mode = x_mode self . y_mode = y_mode self . w_mode = w_mode self . h_mode = h_mode self . x_embedding = None self . y_embedding = None self . w_embedding = None self . h_embedding = None self . first_page_embedding = None self . last_page_embedding = None self . box_preprocessor = BoxLayoutPreprocessor () self . preprocess = self . box_preprocessor . preprocess self . collate = self . box_preprocessor . collate","title":"box_layout_embedding"},{"location":"reference/layers/box_layout_embedding/#edspdflayersbox_layout_embedding","text":"","title":"edspdf.layers.box_layout_embedding"},{"location":"reference/layers/box_layout_embedding/#edspdf.layers.box_layout_embedding.BoxLayoutEmbedding","text":"Bases: Module Encodes a box using its geometrical features, as extracted by the BoxLayoutPreprocessor module. Source code in edspdf/layers/box_layout_embedding.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 @registry . factory . register ( \"box-layout-embedding\" ) class BoxLayoutEmbedding ( Module ): \"\"\" Encodes a box using its geometrical features, as extracted by the BoxLayoutPreprocessor module. \"\"\" def __init__ ( self , n_positions : int , size : Optional [ int ] = None , x_mode : PositionEmbeddingMode = \"sin\" , y_mode : PositionEmbeddingMode = \"sin\" , w_mode : PositionEmbeddingMode = \"sin\" , h_mode : PositionEmbeddingMode = \"sin\" , ): \"\"\" Parameters ---------- size: int Size of the output box embedding n_positions: int Number of position embeddings stored in the PositionEmbedding module x_mode: PositionEmbeddingMode Position embedding mode of the x coordinates y_mode: PositionEmbeddingMode Position embedding mode of the x coordinates w_mode: PositionEmbeddingMode Position embedding mode of the width features h_mode: PositionEmbeddingMode Position embedding mode of the height features \"\"\" super () . __init__ () self . n_positions = n_positions self . size = size self . x_mode = x_mode self . y_mode = y_mode self . w_mode = w_mode self . h_mode = h_mode self . x_embedding = None self . y_embedding = None self . w_embedding = None self . h_embedding = None self . first_page_embedding = None self . last_page_embedding = None self . box_preprocessor = BoxLayoutPreprocessor () self . preprocess = self . box_preprocessor . preprocess self . collate = self . box_preprocessor . collate def initialize ( self , gold_data : Iterable , size : int = None , ** kwargs ): super () . initialize ( gold_data , size = size , ** kwargs ) n_pos , size = self . n_positions , self . size self . x_embedding = self . _make_embed ( n_pos , size // 6 , self . x_mode ) self . y_embedding = self . _make_embed ( n_pos , size // 6 , self . y_mode ) self . w_embedding = self . _make_embed ( n_pos , size // 6 , self . w_mode ) self . h_embedding = self . _make_embed ( n_pos , size // 6 , self . h_mode ) self . first_page_embedding = torch . nn . Parameter ( torch . randn ( self . size )) self . last_page_embedding = torch . nn . Parameter ( torch . randn ( self . size )) @classmethod def _make_embed ( cls , n_positions , size , mode ): if mode == \"sin\" : return SinusoidalEmbedding ( n_positions , size ) else : return torch . nn . Embedding ( n_positions , size ) def forward ( self , batch ): return ( torch . cat ( [ self . x_embedding ( ( batch [ \"xmin\" ] * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), self . y_embedding ( ( batch [ \"ymin\" ] * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), self . x_embedding ( ( batch [ \"xmax\" ] * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), self . y_embedding ( ( batch [ \"ymax\" ] * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), self . w_embedding ( ( batch [ \"width\" ] * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), self . h_embedding ( ( batch [ \"height\" ] * 5 * self . n_positions ) . clamp ( max = self . n_positions - 1 ) . long () ), ], dim =- 1 , ) + self . first_page_embedding * batch [ \"first_page\" ][ ... , None ] + self . last_page_embedding * batch [ \"last_page\" ][ ... , None ] )","title":"BoxLayoutEmbedding"},{"location":"reference/layers/box_layout_embedding/#edspdf.layers.box_layout_embedding.BoxLayoutEmbedding.__init__","text":"PARAMETER DESCRIPTION size Size of the output box embedding TYPE: Optional [ int ] DEFAULT: None n_positions Number of position embeddings stored in the PositionEmbedding module TYPE: int x_mode Position embedding mode of the x coordinates TYPE: PositionEmbeddingMode DEFAULT: 'sin' y_mode Position embedding mode of the x coordinates TYPE: PositionEmbeddingMode DEFAULT: 'sin' w_mode Position embedding mode of the width features TYPE: PositionEmbeddingMode DEFAULT: 'sin' h_mode Position embedding mode of the height features TYPE: PositionEmbeddingMode DEFAULT: 'sin' Source code in edspdf/layers/box_layout_embedding.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , n_positions : int , size : Optional [ int ] = None , x_mode : PositionEmbeddingMode = \"sin\" , y_mode : PositionEmbeddingMode = \"sin\" , w_mode : PositionEmbeddingMode = \"sin\" , h_mode : PositionEmbeddingMode = \"sin\" , ): \"\"\" Parameters ---------- size: int Size of the output box embedding n_positions: int Number of position embeddings stored in the PositionEmbedding module x_mode: PositionEmbeddingMode Position embedding mode of the x coordinates y_mode: PositionEmbeddingMode Position embedding mode of the x coordinates w_mode: PositionEmbeddingMode Position embedding mode of the width features h_mode: PositionEmbeddingMode Position embedding mode of the height features \"\"\" super () . __init__ () self . n_positions = n_positions self . size = size self . x_mode = x_mode self . y_mode = y_mode self . w_mode = w_mode self . h_mode = h_mode self . x_embedding = None self . y_embedding = None self . w_embedding = None self . h_embedding = None self . first_page_embedding = None self . last_page_embedding = None self . box_preprocessor = BoxLayoutPreprocessor () self . preprocess = self . box_preprocessor . preprocess self . collate = self . box_preprocessor . collate","title":"__init__()"},{"location":"reference/layers/box_layout_preprocessor/","text":"edspdf.layers.box_layout_preprocessor BoxLayoutPreprocessor Bases: Module [ PDFDoc , BoxBatch ] The box preprocessor is singleton since its is not configurable. The following features of each box of an input PDFDoc document are encoded as 1D tensors: boxes_page : page index of the box boxes_first_page : is the box on the first page boxes_last_page : is the box on the last page boxes_xmin : left position of the box boxes_ymin : bottom position of the box boxes_xmax : right position of the box boxes_ymax : top position of the box boxes_w : width position of the box boxes_h : height position of the box The preprocessor also returns an additional tensors: page_boxes_id : box indices per page to index the above 1D tensors (LongTensor: n_pages * n_boxes) Source code in edspdf/layers/box_layout_preprocessor.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @registry . factory . register ( \"box-preprocessor\" ) class BoxLayoutPreprocessor ( Module [ PDFDoc , BoxBatch ]): \"\"\" The box preprocessor is singleton since its is not configurable. The following features of each box of an input PDFDoc document are encoded as 1D tensors: - `boxes_page`: page index of the box - `boxes_first_page`: is the box on the first page - `boxes_last_page`: is the box on the last page - `boxes_xmin`: left position of the box - `boxes_ymin`: bottom position of the box - `boxes_xmax`: right position of the box - `boxes_ymax`: top position of the box - `boxes_w`: width position of the box - `boxes_h`: height position of the box The preprocessor also returns an additional tensors: - `page_boxes_id`: box indices per page to index the above 1D tensors (LongTensor: n_pages * n_boxes) \"\"\" INSTANCE = None def __new__ ( cls ): if BoxLayoutPreprocessor . INSTANCE is None : BoxLayoutPreprocessor . INSTANCE = super () . __new__ ( cls ) return BoxLayoutPreprocessor . INSTANCE def preprocess_boxes ( self , boxes ): box_pages = [ box . page for box in boxes ] last_page = max ( box_pages , default = 0 ) return { \"page\" : [ b . page for b in boxes ], \"xmin\" : [ b . x0 for b in boxes ], \"ymin\" : [ b . y0 for b in boxes ], \"xmax\" : [ b . x1 for b in boxes ], \"ymax\" : [ b . y1 for b in boxes ], \"width\" : [( b . x1 - b . x0 ) for b in boxes ], \"height\" : [( b . y1 - b . y0 ) for b in boxes ], \"first_page\" : [ b . page == 0 for b in boxes ], \"last_page\" : [ b . page == last_page for b in boxes ], } def preprocess ( self , doc : PDFDoc , supervision : bool = False ): return self . preprocess_boxes ( doc . lines ) def collate ( self , batch , device : torch . device ) -> BoxBatch : page_boxes_id = defaultdict ( lambda : []) doc_pages = [[] for _ in range ( len ( batch [ \"page\" ]))] box_i = 0 for doc_i , doc_boxes_page in enumerate ( batch [ \"page\" ]): for box_page in doc_boxes_page : page_boxes_id [( doc_i , box_page )] . append ( box_i ) doc_pages [ doc_i ] . append ( box_i ) box_i += 1 page_boxes_id = pad_2d ( list ( page_boxes_id . values ()), pad =- 1 , device = device ) ( boxes_page , boxes_xmin , boxes_ymin , boxes_xmax , boxes_ymax , boxes_w , boxes_h , boxes_first_page , boxes_last_page , ) = torch . as_tensor ( [ flatten ( batch [ \"page\" ]), flatten ( batch [ \"xmin\" ]), flatten ( batch [ \"ymin\" ]), flatten ( batch [ \"xmax\" ]), flatten ( batch [ \"ymax\" ]), flatten ( batch [ \"width\" ]), flatten ( batch [ \"height\" ]), flatten ( batch [ \"first_page\" ]), flatten ( batch [ \"last_page\" ]), ], dtype = torch . float , device = device , ) . unbind ( 0 ) return { \"page\" : boxes_page . long (), \"xmin\" : boxes_xmin , \"ymin\" : boxes_ymin , \"xmax\" : boxes_xmax , \"ymax\" : boxes_ymax , \"width\" : boxes_w , \"height\" : boxes_h , \"first_page\" : boxes_first_page , \"last_page\" : boxes_last_page , \"page_ids\" : page_boxes_id . long (), } def forward ( self , * args , ** kwargs ) -> Dict [ str , Any ]: pass","title":"box_layout_preprocessor"},{"location":"reference/layers/box_layout_preprocessor/#edspdflayersbox_layout_preprocessor","text":"","title":"edspdf.layers.box_layout_preprocessor"},{"location":"reference/layers/box_layout_preprocessor/#edspdf.layers.box_layout_preprocessor.BoxLayoutPreprocessor","text":"Bases: Module [ PDFDoc , BoxBatch ] The box preprocessor is singleton since its is not configurable. The following features of each box of an input PDFDoc document are encoded as 1D tensors: boxes_page : page index of the box boxes_first_page : is the box on the first page boxes_last_page : is the box on the last page boxes_xmin : left position of the box boxes_ymin : bottom position of the box boxes_xmax : right position of the box boxes_ymax : top position of the box boxes_w : width position of the box boxes_h : height position of the box The preprocessor also returns an additional tensors: page_boxes_id : box indices per page to index the above 1D tensors (LongTensor: n_pages * n_boxes) Source code in edspdf/layers/box_layout_preprocessor.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @registry . factory . register ( \"box-preprocessor\" ) class BoxLayoutPreprocessor ( Module [ PDFDoc , BoxBatch ]): \"\"\" The box preprocessor is singleton since its is not configurable. The following features of each box of an input PDFDoc document are encoded as 1D tensors: - `boxes_page`: page index of the box - `boxes_first_page`: is the box on the first page - `boxes_last_page`: is the box on the last page - `boxes_xmin`: left position of the box - `boxes_ymin`: bottom position of the box - `boxes_xmax`: right position of the box - `boxes_ymax`: top position of the box - `boxes_w`: width position of the box - `boxes_h`: height position of the box The preprocessor also returns an additional tensors: - `page_boxes_id`: box indices per page to index the above 1D tensors (LongTensor: n_pages * n_boxes) \"\"\" INSTANCE = None def __new__ ( cls ): if BoxLayoutPreprocessor . INSTANCE is None : BoxLayoutPreprocessor . INSTANCE = super () . __new__ ( cls ) return BoxLayoutPreprocessor . INSTANCE def preprocess_boxes ( self , boxes ): box_pages = [ box . page for box in boxes ] last_page = max ( box_pages , default = 0 ) return { \"page\" : [ b . page for b in boxes ], \"xmin\" : [ b . x0 for b in boxes ], \"ymin\" : [ b . y0 for b in boxes ], \"xmax\" : [ b . x1 for b in boxes ], \"ymax\" : [ b . y1 for b in boxes ], \"width\" : [( b . x1 - b . x0 ) for b in boxes ], \"height\" : [( b . y1 - b . y0 ) for b in boxes ], \"first_page\" : [ b . page == 0 for b in boxes ], \"last_page\" : [ b . page == last_page for b in boxes ], } def preprocess ( self , doc : PDFDoc , supervision : bool = False ): return self . preprocess_boxes ( doc . lines ) def collate ( self , batch , device : torch . device ) -> BoxBatch : page_boxes_id = defaultdict ( lambda : []) doc_pages = [[] for _ in range ( len ( batch [ \"page\" ]))] box_i = 0 for doc_i , doc_boxes_page in enumerate ( batch [ \"page\" ]): for box_page in doc_boxes_page : page_boxes_id [( doc_i , box_page )] . append ( box_i ) doc_pages [ doc_i ] . append ( box_i ) box_i += 1 page_boxes_id = pad_2d ( list ( page_boxes_id . values ()), pad =- 1 , device = device ) ( boxes_page , boxes_xmin , boxes_ymin , boxes_xmax , boxes_ymax , boxes_w , boxes_h , boxes_first_page , boxes_last_page , ) = torch . as_tensor ( [ flatten ( batch [ \"page\" ]), flatten ( batch [ \"xmin\" ]), flatten ( batch [ \"ymin\" ]), flatten ( batch [ \"xmax\" ]), flatten ( batch [ \"ymax\" ]), flatten ( batch [ \"width\" ]), flatten ( batch [ \"height\" ]), flatten ( batch [ \"first_page\" ]), flatten ( batch [ \"last_page\" ]), ], dtype = torch . float , device = device , ) . unbind ( 0 ) return { \"page\" : boxes_page . long (), \"xmin\" : boxes_xmin , \"ymin\" : boxes_ymin , \"xmax\" : boxes_xmax , \"ymax\" : boxes_ymax , \"width\" : boxes_w , \"height\" : boxes_h , \"first_page\" : boxes_first_page , \"last_page\" : boxes_last_page , \"page_ids\" : page_boxes_id . long (), } def forward ( self , * args , ** kwargs ) -> Dict [ str , Any ]: pass","title":"BoxLayoutPreprocessor"},{"location":"reference/layers/box_text_embedding/","text":"edspdf.layers.box_text_embedding BoxTextEmbedding Bases: Module A module that embeds the textual features of the blocks Source code in edspdf/layers/box_text_embedding.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @registry . factory . register ( \"box-text-embedding\" ) class BoxTextEmbedding ( Module ): \"\"\" A module that embeds the textual features of the blocks \"\"\" def __init__ ( self , pooler : Module , size : Optional [ int ] = None , ): \"\"\" Parameters ---------- size: int Size of the output box embedding pooler: Dict The module used to encode the textual features of the blocks \"\"\" super () . __init__ () self . size = size self . shape_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . prefix_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . suffix_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . norm_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . shape_embedding = None self . prefix_embedding = None self . suffix_embedding = None self . norm_embedding = None self . hpos_embedding = None self . vpos_embedding = None self . first_page_embedding = None self . last_page_embedding = None self . pooler = pooler punct = \"[:punct:]\" + \" \\\" '\u02ca\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\" num_like = r \"\\d+(?:[.,]\\d+)?\" default = rf \"[^\\d { punct } '\\n[[:space:]]+(?:['\u02ca](?=[[:alpha:]]|$))?\" self . word_regex = regex . compile ( rf \"( { num_like } |[ { punct } ]|[\\n\\r\\t]|[^\\S\\r\\n\\t]+| { default } )([^\\S\\r\\n\\t])?\" ) @property def output_size ( self ): return self . size def initialize ( self , gold_data , size : int = None , ** kwargs ): super () . initialize ( gold_data , size = size , ** kwargs ) self . pooler . initialize ( gold_data , input_size = size ) shape_init = self . shape_voc . initialization () prefix_init = self . prefix_voc . initialization () suffix_init = self . suffix_voc . initialization () norm_init = self . norm_voc . initialization () with shape_init , prefix_init , suffix_init , norm_init : # noqa: E501 with self . no_cache (): for doc in gold_data : self . preprocess ( doc , supervision = True ) self . shape_embedding = torch . nn . Embedding ( len ( self . shape_voc ), self . size ) self . prefix_embedding = torch . nn . Embedding ( len ( self . prefix_voc ), self . size ) self . suffix_embedding = torch . nn . Embedding ( len ( self . suffix_voc ), self . size ) self . norm_embedding = torch . nn . Embedding ( len ( self . norm_voc ), self . size ) def preprocess ( self , doc , supervision : bool = False ): text_boxes = doc . lines tokens_shape = [[] for _ in text_boxes ] tokens_prefix = [[] for _ in text_boxes ] tokens_suffix = [[] for _ in text_boxes ] tokens_norm = [[] for _ in text_boxes ] for i , b in enumerate ( text_boxes ): words = [ m . group ( 0 ) for m in self . word_regex . finditer ( b . text )] for word in words : # ascii_str = unidecode(word) ascii_str = word tokens_shape [ i ] . append ( self . shape_voc . encode ( word_shape ( ascii_str ))) tokens_prefix [ i ] . append ( self . prefix_voc . encode ( ascii_str . lower ()[: 3 ])) tokens_suffix [ i ] . append ( self . suffix_voc . encode ( ascii_str . lower ()[ - 3 :])) tokens_norm [ i ] . append ( self . norm_voc . encode ( ascii_str . lower ())) return { \"tokens_shape\" : tokens_shape , \"tokens_prefix\" : tokens_prefix , \"tokens_suffix\" : tokens_suffix , \"tokens_norm\" : tokens_norm , } def collate ( self , batch , device : torch . device ): shapes = pad_2d ( flatten ( batch [ \"tokens_shape\" ]), pad =- 1 , device = device ) mask = shapes != - 1 shapes [ ~ mask ] = 0 return { \"tokens_shape\" : shapes , \"tokens_prefix\" : pad_2d ( flatten ( batch [ \"tokens_prefix\" ]), pad = 0 , device = device ), \"tokens_suffix\" : pad_2d ( flatten ( batch [ \"tokens_suffix\" ]), pad = 0 , device = device ), \"tokens_norm\" : pad_2d ( flatten ( batch [ \"tokens_norm\" ]), pad = 0 , device = device ), \"tokens_mask\" : mask , } def forward ( self , batch , supervision = False ): text_embeds = self . pooler ( embeds = ( self . shape_embedding ( batch [ \"tokens_shape\" ]) + self . prefix_embedding ( batch [ \"tokens_prefix\" ]) + self . suffix_embedding ( batch [ \"tokens_suffix\" ]) + self . norm_embedding ( batch [ \"tokens_norm\" ]) ), mask = batch [ \"tokens_mask\" ], ) return text_embeds __init__ ( pooler , size = None ) PARAMETER DESCRIPTION size Size of the output box embedding TYPE: Optional [ int ] DEFAULT: None pooler The module used to encode the textual features of the blocks TYPE: Module Source code in edspdf/layers/box_text_embedding.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __init__ ( self , pooler : Module , size : Optional [ int ] = None , ): \"\"\" Parameters ---------- size: int Size of the output box embedding pooler: Dict The module used to encode the textual features of the blocks \"\"\" super () . __init__ () self . size = size self . shape_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . prefix_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . suffix_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . norm_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . shape_embedding = None self . prefix_embedding = None self . suffix_embedding = None self . norm_embedding = None self . hpos_embedding = None self . vpos_embedding = None self . first_page_embedding = None self . last_page_embedding = None self . pooler = pooler punct = \"[:punct:]\" + \" \\\" '\u02ca\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\" num_like = r \"\\d+(?:[.,]\\d+)?\" default = rf \"[^\\d { punct } '\\n[[:space:]]+(?:['\u02ca](?=[[:alpha:]]|$))?\" self . word_regex = regex . compile ( rf \"( { num_like } |[ { punct } ]|[\\n\\r\\t]|[^\\S\\r\\n\\t]+| { default } )([^\\S\\r\\n\\t])?\" ) word_shape ( text ) Converts a word into its shape following the algorithm used in the spaCy library. https://github.com/explosion/spaCy/blob/b69d249a/spacy/lang/lex_attrs.py#L118 PARAMETER DESCRIPTION text TYPE: str RETURNS DESCRIPTION str The word shape Source code in edspdf/layers/box_text_embedding.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def word_shape ( text : str ) -> str : \"\"\" Converts a word into its shape following the algorithm used in the spaCy library. https://github.com/explosion/spaCy/blob/b69d249a/spacy/lang/lex_attrs.py#L118 Parameters ---------- text: str Returns ------- str The word shape \"\"\" if len ( text ) >= 100 : return \"LONG\" shape = [] last = \"\" seq = 0 for char in text : if char . isalpha (): if char . isupper (): shape_char = \"X\" else : shape_char = \"x\" elif char . isdigit (): shape_char = \"d\" else : shape_char = char if shape_char == last : seq += 1 else : seq = 0 last = shape_char if seq < 4 : shape . append ( shape_char ) return \"\" . join ( shape )","title":"box_text_embedding"},{"location":"reference/layers/box_text_embedding/#edspdflayersbox_text_embedding","text":"","title":"edspdf.layers.box_text_embedding"},{"location":"reference/layers/box_text_embedding/#edspdf.layers.box_text_embedding.BoxTextEmbedding","text":"Bases: Module A module that embeds the textual features of the blocks Source code in edspdf/layers/box_text_embedding.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 @registry . factory . register ( \"box-text-embedding\" ) class BoxTextEmbedding ( Module ): \"\"\" A module that embeds the textual features of the blocks \"\"\" def __init__ ( self , pooler : Module , size : Optional [ int ] = None , ): \"\"\" Parameters ---------- size: int Size of the output box embedding pooler: Dict The module used to encode the textual features of the blocks \"\"\" super () . __init__ () self . size = size self . shape_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . prefix_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . suffix_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . norm_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . shape_embedding = None self . prefix_embedding = None self . suffix_embedding = None self . norm_embedding = None self . hpos_embedding = None self . vpos_embedding = None self . first_page_embedding = None self . last_page_embedding = None self . pooler = pooler punct = \"[:punct:]\" + \" \\\" '\u02ca\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\" num_like = r \"\\d+(?:[.,]\\d+)?\" default = rf \"[^\\d { punct } '\\n[[:space:]]+(?:['\u02ca](?=[[:alpha:]]|$))?\" self . word_regex = regex . compile ( rf \"( { num_like } |[ { punct } ]|[\\n\\r\\t]|[^\\S\\r\\n\\t]+| { default } )([^\\S\\r\\n\\t])?\" ) @property def output_size ( self ): return self . size def initialize ( self , gold_data , size : int = None , ** kwargs ): super () . initialize ( gold_data , size = size , ** kwargs ) self . pooler . initialize ( gold_data , input_size = size ) shape_init = self . shape_voc . initialization () prefix_init = self . prefix_voc . initialization () suffix_init = self . suffix_voc . initialization () norm_init = self . norm_voc . initialization () with shape_init , prefix_init , suffix_init , norm_init : # noqa: E501 with self . no_cache (): for doc in gold_data : self . preprocess ( doc , supervision = True ) self . shape_embedding = torch . nn . Embedding ( len ( self . shape_voc ), self . size ) self . prefix_embedding = torch . nn . Embedding ( len ( self . prefix_voc ), self . size ) self . suffix_embedding = torch . nn . Embedding ( len ( self . suffix_voc ), self . size ) self . norm_embedding = torch . nn . Embedding ( len ( self . norm_voc ), self . size ) def preprocess ( self , doc , supervision : bool = False ): text_boxes = doc . lines tokens_shape = [[] for _ in text_boxes ] tokens_prefix = [[] for _ in text_boxes ] tokens_suffix = [[] for _ in text_boxes ] tokens_norm = [[] for _ in text_boxes ] for i , b in enumerate ( text_boxes ): words = [ m . group ( 0 ) for m in self . word_regex . finditer ( b . text )] for word in words : # ascii_str = unidecode(word) ascii_str = word tokens_shape [ i ] . append ( self . shape_voc . encode ( word_shape ( ascii_str ))) tokens_prefix [ i ] . append ( self . prefix_voc . encode ( ascii_str . lower ()[: 3 ])) tokens_suffix [ i ] . append ( self . suffix_voc . encode ( ascii_str . lower ()[ - 3 :])) tokens_norm [ i ] . append ( self . norm_voc . encode ( ascii_str . lower ())) return { \"tokens_shape\" : tokens_shape , \"tokens_prefix\" : tokens_prefix , \"tokens_suffix\" : tokens_suffix , \"tokens_norm\" : tokens_norm , } def collate ( self , batch , device : torch . device ): shapes = pad_2d ( flatten ( batch [ \"tokens_shape\" ]), pad =- 1 , device = device ) mask = shapes != - 1 shapes [ ~ mask ] = 0 return { \"tokens_shape\" : shapes , \"tokens_prefix\" : pad_2d ( flatten ( batch [ \"tokens_prefix\" ]), pad = 0 , device = device ), \"tokens_suffix\" : pad_2d ( flatten ( batch [ \"tokens_suffix\" ]), pad = 0 , device = device ), \"tokens_norm\" : pad_2d ( flatten ( batch [ \"tokens_norm\" ]), pad = 0 , device = device ), \"tokens_mask\" : mask , } def forward ( self , batch , supervision = False ): text_embeds = self . pooler ( embeds = ( self . shape_embedding ( batch [ \"tokens_shape\" ]) + self . prefix_embedding ( batch [ \"tokens_prefix\" ]) + self . suffix_embedding ( batch [ \"tokens_suffix\" ]) + self . norm_embedding ( batch [ \"tokens_norm\" ]) ), mask = batch [ \"tokens_mask\" ], ) return text_embeds","title":"BoxTextEmbedding"},{"location":"reference/layers/box_text_embedding/#edspdf.layers.box_text_embedding.BoxTextEmbedding.__init__","text":"PARAMETER DESCRIPTION size Size of the output box embedding TYPE: Optional [ int ] DEFAULT: None pooler The module used to encode the textual features of the blocks TYPE: Module Source code in edspdf/layers/box_text_embedding.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def __init__ ( self , pooler : Module , size : Optional [ int ] = None , ): \"\"\" Parameters ---------- size: int Size of the output box embedding pooler: Dict The module used to encode the textual features of the blocks \"\"\" super () . __init__ () self . size = size self . shape_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . prefix_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . suffix_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . norm_voc = Vocabulary ([ \"__unk__\" ], default = 0 ) self . shape_embedding = None self . prefix_embedding = None self . suffix_embedding = None self . norm_embedding = None self . hpos_embedding = None self . vpos_embedding = None self . first_page_embedding = None self . last_page_embedding = None self . pooler = pooler punct = \"[:punct:]\" + \" \\\" '\u02ca\uff02\u3003\u05f2\u1cd3\u2033\u05f4\u2036\u02f6\u02ba\u201c\u201d\u02dd\" num_like = r \"\\d+(?:[.,]\\d+)?\" default = rf \"[^\\d { punct } '\\n[[:space:]]+(?:['\u02ca](?=[[:alpha:]]|$))?\" self . word_regex = regex . compile ( rf \"( { num_like } |[ { punct } ]|[\\n\\r\\t]|[^\\S\\r\\n\\t]+| { default } )([^\\S\\r\\n\\t])?\" )","title":"__init__()"},{"location":"reference/layers/box_text_embedding/#edspdf.layers.box_text_embedding.word_shape","text":"Converts a word into its shape following the algorithm used in the spaCy library. https://github.com/explosion/spaCy/blob/b69d249a/spacy/lang/lex_attrs.py#L118 PARAMETER DESCRIPTION text TYPE: str RETURNS DESCRIPTION str The word shape Source code in edspdf/layers/box_text_embedding.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 def word_shape ( text : str ) -> str : \"\"\" Converts a word into its shape following the algorithm used in the spaCy library. https://github.com/explosion/spaCy/blob/b69d249a/spacy/lang/lex_attrs.py#L118 Parameters ---------- text: str Returns ------- str The word shape \"\"\" if len ( text ) >= 100 : return \"LONG\" shape = [] last = \"\" seq = 0 for char in text : if char . isalpha (): if char . isupper (): shape_char = \"X\" else : shape_char = \"x\" elif char . isdigit (): shape_char = \"d\" else : shape_char = char if shape_char == last : seq += 1 else : seq = 0 last = shape_char if seq < 4 : shape . append ( shape_char ) return \"\" . join ( shape )","title":"word_shape()"},{"location":"reference/layers/box_transformer/","text":"edspdf.layers.box_transformer BoxTransformerLayer Bases: torch . nn . Module BoxTransformerLayer combining a self attention layer and a linear->activation->linear transformation. Source code in edspdf/layers/box_transformer.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 class BoxTransformerLayer ( torch . nn . Module ): \"\"\" BoxTransformerLayer combining a self attention layer and a linear->activation->linear transformation. \"\"\" def __init__ ( self , input_size : int , num_heads : int = 2 , dropout_p : float = 0.15 , head_size : Optional [ int ] = None , activation : ActivationFunction = \"gelu\" , init_resweight : float = 0.0 , attention_mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"c2p\" , \"p2c\" ), position_embedding : Optional [ Union [ torch . FloatTensor , torch . nn . Parameter ] ] = None , ): \"\"\" Parameters ---------- input_size: int Input embedding size num_heads: int Number of attention heads in the attention layer dropout_p: float Dropout probability both for the attention layer and embedding projections head_size: int Head sizes of the attention layer activation: ActivationFunction Activation function used in the linear->activation->linear transformation init_resweight: float Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. attention_mode: Sequence[RelativeAttentionMode] Mode of relative position infused attention layer. See the [relative attention](relative_attention) documentation for more information. position_embedding: torch.FloatTensor Position embedding to use as key/query position embedding in the attention computation. \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( dropout_p ) self . attention = RelativeAttention ( size = input_size , n_heads = num_heads , do_pooling = True , head_size = head_size , position_embedding = position_embedding , dropout_p = dropout_p , n_coordinates = 2 , mode = attention_mode , ) self . resweight = torch . nn . Parameter ( torch . Tensor ([ float ( init_resweight )])) self . norm = torch . nn . LayerNorm ( input_size ) self . activation = get_activation_function ( activation ) self . resweight2 = torch . nn . Parameter ( torch . Tensor ([ float ( init_resweight )])) self . norm2 = torch . nn . LayerNorm ( input_size ) self . linear1 = torch . nn . Linear ( input_size , input_size * 2 ) self . linear2 = torch . nn . Linear ( input_size * 2 , input_size ) def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , relative_positions : torch . LongTensor , no_position_mask : Optional [ torch . BoolTensor ] = None , ) -> Tuple [ torch . FloatTensor , torch . FloatTensor ]: \"\"\" Forward pass of the BoxTransformerLayer Parameters ---------- embeds: torch.FloatTensor Embeddings to contextualize Shape: `n_samples * n_keys * input_size` mask: torch.BoolTensor Mask of the embeddings. 0 means padding element. Shape: `n_samples * n_keys` relative_positions: torch.LongTensor Position of the keys relatively to the query elements Shape: `n_samples * n_queries * n_keys * n_coordinates (2 for x/y)` no_position_mask: Optional[torch.BoolTensor] Key / query pairs for which the position attention terms should be disabled. Shape: `n_samples * n_queries * n_keys` Returns ------- Tuple[torch.FloatTensor, torch.FloatTensor] - Contextualized embeddings Shape: `n_samples * n_queries * n_keys` - Attention logits Shape: `n_samples * n_queries * n_keys * n_heads` \"\"\" update , attn = self . attention ( embeds , embeds , embeds , mask , relative_positions = relative_positions , no_position_mask = no_position_mask , ) embeds = embeds + self . dropout ( update ) * self . resweight embeds = self . norm ( embeds ) update = self . linear2 ( self . dropout ( self . activation ( self . linear1 ( embeds )))) embeds = embeds + self . dropout ( update ) * self . resweight2 embeds = self . norm2 ( embeds ) return embeds , attn __init__ ( input_size , num_heads = 2 , dropout_p = 0.15 , head_size = None , activation = 'gelu' , init_resweight = 0.0 , attention_mode = ( 'c2c' , 'c2p' , 'p2c' ), position_embedding = None ) PARAMETER DESCRIPTION input_size Input embedding size TYPE: int num_heads Number of attention heads in the attention layer TYPE: int DEFAULT: 2 dropout_p Dropout probability both for the attention layer and embedding projections TYPE: float DEFAULT: 0.15 head_size Head sizes of the attention layer TYPE: Optional [ int ] DEFAULT: None activation Activation function used in the linear->activation->linear transformation TYPE: ActivationFunction DEFAULT: 'gelu' init_resweight Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. TYPE: float DEFAULT: 0.0 attention_mode Mode of relative position infused attention layer. See the relative attention documentation for more information. TYPE: Sequence [ RelativeAttentionMode ] DEFAULT: ('c2c', 'c2p', 'p2c') position_embedding Position embedding to use as key/query position embedding in the attention computation. TYPE: Optional [ Union [ torch . FloatTensor , torch . nn . Parameter ]] DEFAULT: None Source code in edspdf/layers/box_transformer.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , input_size : int , num_heads : int = 2 , dropout_p : float = 0.15 , head_size : Optional [ int ] = None , activation : ActivationFunction = \"gelu\" , init_resweight : float = 0.0 , attention_mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"c2p\" , \"p2c\" ), position_embedding : Optional [ Union [ torch . FloatTensor , torch . nn . Parameter ] ] = None , ): \"\"\" Parameters ---------- input_size: int Input embedding size num_heads: int Number of attention heads in the attention layer dropout_p: float Dropout probability both for the attention layer and embedding projections head_size: int Head sizes of the attention layer activation: ActivationFunction Activation function used in the linear->activation->linear transformation init_resweight: float Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. attention_mode: Sequence[RelativeAttentionMode] Mode of relative position infused attention layer. See the [relative attention](relative_attention) documentation for more information. position_embedding: torch.FloatTensor Position embedding to use as key/query position embedding in the attention computation. \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( dropout_p ) self . attention = RelativeAttention ( size = input_size , n_heads = num_heads , do_pooling = True , head_size = head_size , position_embedding = position_embedding , dropout_p = dropout_p , n_coordinates = 2 , mode = attention_mode , ) self . resweight = torch . nn . Parameter ( torch . Tensor ([ float ( init_resweight )])) self . norm = torch . nn . LayerNorm ( input_size ) self . activation = get_activation_function ( activation ) self . resweight2 = torch . nn . Parameter ( torch . Tensor ([ float ( init_resweight )])) self . norm2 = torch . nn . LayerNorm ( input_size ) self . linear1 = torch . nn . Linear ( input_size , input_size * 2 ) self . linear2 = torch . nn . Linear ( input_size * 2 , input_size ) forward ( embeds , mask , relative_positions , no_position_mask = None ) Forward pass of the BoxTransformerLayer PARAMETER DESCRIPTION embeds Embeddings to contextualize Shape: n_samples * n_keys * input_size TYPE: torch . FloatTensor mask Mask of the embeddings. 0 means padding element. Shape: n_samples * n_keys TYPE: torch . BoolTensor relative_positions Position of the keys relatively to the query elements Shape: n_samples * n_queries * n_keys * n_coordinates (2 for x/y) TYPE: torch . LongTensor no_position_mask Key / query pairs for which the position attention terms should be disabled. Shape: n_samples * n_queries * n_keys TYPE: Optional [ torch . BoolTensor ] DEFAULT: None RETURNS DESCRIPTION Tuple [ torch . FloatTensor , torch . FloatTensor ] Contextualized embeddings Shape: n_samples * n_queries * n_keys Attention logits Shape: n_samples * n_queries * n_keys * n_heads Source code in edspdf/layers/box_transformer.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , relative_positions : torch . LongTensor , no_position_mask : Optional [ torch . BoolTensor ] = None , ) -> Tuple [ torch . FloatTensor , torch . FloatTensor ]: \"\"\" Forward pass of the BoxTransformerLayer Parameters ---------- embeds: torch.FloatTensor Embeddings to contextualize Shape: `n_samples * n_keys * input_size` mask: torch.BoolTensor Mask of the embeddings. 0 means padding element. Shape: `n_samples * n_keys` relative_positions: torch.LongTensor Position of the keys relatively to the query elements Shape: `n_samples * n_queries * n_keys * n_coordinates (2 for x/y)` no_position_mask: Optional[torch.BoolTensor] Key / query pairs for which the position attention terms should be disabled. Shape: `n_samples * n_queries * n_keys` Returns ------- Tuple[torch.FloatTensor, torch.FloatTensor] - Contextualized embeddings Shape: `n_samples * n_queries * n_keys` - Attention logits Shape: `n_samples * n_queries * n_keys * n_heads` \"\"\" update , attn = self . attention ( embeds , embeds , embeds , mask , relative_positions = relative_positions , no_position_mask = no_position_mask , ) embeds = embeds + self . dropout ( update ) * self . resweight embeds = self . norm ( embeds ) update = self . linear2 ( self . dropout ( self . activation ( self . linear1 ( embeds )))) embeds = embeds + self . dropout ( update ) * self . resweight2 embeds = self . norm2 ( embeds ) return embeds , attn BoxTransformer Bases: Module Source code in edspdf/layers/box_transformer.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 @registry . factory . register ( \"box-transformer\" ) class BoxTransformer ( Module ): def __init__ ( self , input_size : Optional [ int ] = None , num_heads : int = 2 , dropout_p : float = 0.15 , head_size : Optional [ int ] = None , activation : ActivationFunction = \"gelu\" , init_resweight : float = 0.0 , n_relative_positions : Optional [ int ] = None , attention_mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"c2p\" , \"p2c\" ), n_layers : int = 2 , ): \"\"\" BoxTransformer combining a multiple BoxTransformerLayer Parameters ---------- input_size: int Input embedding size num_heads: int Number of attention heads in the attention layers n_relative_positions: int Maximum range of embeddable relative positions between boxes (further distances are capped to \u00b1n_relative_positions // 2) dropout_p: float Dropout probability both for the attention layers and embedding projections head_size: int Head sizes of the attention layers activation: ActivationFunction Activation function used in the linear->activation->linear transformations init_resweight: float Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. attention_mode: Sequence[RelativeAttentionMode] Mode of relative position infused attention layer. See the [relative attention](relative_attention) documentation for more information. n_layers: int Number of layers in the Transformer \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( dropout_p ) self . input_size = input_size self . num_heads = num_heads self . dropout_p = dropout_p self . head_size = head_size self . activation = activation self . init_resweight = init_resweight self . n_relative_positions = n_relative_positions self . attention_mode = attention_mode self . n_layers = n_layers def initialize ( self , gold_data : Iterable , input_size : int = None , ** kwargs ): super () . initialize ( gold_data , input_size = input_size , ** kwargs ) self . empty_embed = torch . nn . Parameter ( torch . randn ( self . input_size )) self . position_embedding = torch . nn . Parameter ( torch . randn ( ( self . n_relative_positions , self . input_size , ) ) ) self . layers = torch . nn . ModuleList ( [ BoxTransformerLayer ( input_size = self . input_size , num_heads = self . num_heads , head_size = self . head_size , dropout_p = self . dropout_p , activation = self . activation , init_resweight = self . init_resweight , attention_mode = self . attention_mode , position_embedding = self . position_embedding , ) for _ in range ( self . n_layers ) ] ) def forward ( self , embeds : torch . FloatTensor , boxes : Dict , ) -> Tuple [ torch . FloatTensor , List [ torch . FloatTensor ]]: \"\"\" Forward pass of the BoxTransformer Parameters ---------- embeds: torch.FloatTensor Embeddings to contextualize Shape: `n_samples * n_keys * input_size` boxes: Dict Layout features of the input elements Returns ------- Tuple[torch.FloatTensor, List[torch.FloatTensor]] - Output of the last BoxTransformerLayer Shape: `n_samples * n_queries * n_keys` - Attention logits of all layers Shape: `n_samples * n_queries * n_keys * n_heads` \"\"\" page_boxes = boxes [ \"page_ids\" ] mask = page_boxes != - 1 n_samples , n_boxes_per_sample = mask . shape n_pages = page_boxes . shape [ 0 ] device = page_boxes . device embeds_with_cls = torch . cat ( [ self . empty_embed * torch . ones ( n_pages , 1 , 1 , device = device ), embeds [ page_boxes ], ], dim = 1 , ) mask_with_cls = torch . cat ( [ torch . ones ( n_samples , 1 , dtype = torch . bool , device = device ), mask , ], dim = 1 , ) n = n_boxes_per_sample + 1 relative_positions = torch . zeros ( n_pages , n , n , 2 , dtype = torch . long , device = device ) relative_positions [:, 1 :, 1 :, :] = compute_pdf_relative_positions ( x0 = boxes [ \"xmin\" ][ page_boxes ], x1 = boxes [ \"xmax\" ][ page_boxes ], y0 = boxes [ \"ymin\" ][ page_boxes ], y1 = boxes [ \"ymax\" ][ page_boxes ], width = boxes [ \"width\" ][ page_boxes ], height = boxes [ \"height\" ][ page_boxes ], n_relative_positions = self . n_relative_positions , ) no_position_mask = torch . ones ( n_pages , n , n , dtype = torch . bool , device = device ) no_position_mask [:, 1 :, 1 :] = 0 attention = [] for layer in self . layers : embeds , attn = layer ( embeds_with_cls , mask_with_cls , relative_positions = relative_positions , no_position_mask = no_position_mask , ) attention . append ( attn ) return embeds [:, 1 :][ mask ] __init__ ( input_size = None , num_heads = 2 , dropout_p = 0.15 , head_size = None , activation = 'gelu' , init_resweight = 0.0 , n_relative_positions = None , attention_mode = ( 'c2c' , 'c2p' , 'p2c' ), n_layers = 2 ) BoxTransformer combining a multiple BoxTransformerLayer PARAMETER DESCRIPTION input_size Input embedding size TYPE: Optional [ int ] DEFAULT: None num_heads Number of attention heads in the attention layers TYPE: int DEFAULT: 2 n_relative_positions Maximum range of embeddable relative positions between boxes (further distances are capped to \u00b1n_relative_positions // 2) TYPE: Optional [ int ] DEFAULT: None dropout_p Dropout probability both for the attention layers and embedding projections TYPE: float DEFAULT: 0.15 head_size Head sizes of the attention layers TYPE: Optional [ int ] DEFAULT: None activation Activation function used in the linear->activation->linear transformations TYPE: ActivationFunction DEFAULT: 'gelu' init_resweight Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. TYPE: float DEFAULT: 0.0 attention_mode Mode of relative position infused attention layer. See the relative attention documentation for more information. TYPE: Sequence [ RelativeAttentionMode ] DEFAULT: ('c2c', 'c2p', 'p2c') n_layers Number of layers in the Transformer TYPE: int DEFAULT: 2 Source code in edspdf/layers/box_transformer.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , input_size : Optional [ int ] = None , num_heads : int = 2 , dropout_p : float = 0.15 , head_size : Optional [ int ] = None , activation : ActivationFunction = \"gelu\" , init_resweight : float = 0.0 , n_relative_positions : Optional [ int ] = None , attention_mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"c2p\" , \"p2c\" ), n_layers : int = 2 , ): \"\"\" BoxTransformer combining a multiple BoxTransformerLayer Parameters ---------- input_size: int Input embedding size num_heads: int Number of attention heads in the attention layers n_relative_positions: int Maximum range of embeddable relative positions between boxes (further distances are capped to \u00b1n_relative_positions // 2) dropout_p: float Dropout probability both for the attention layers and embedding projections head_size: int Head sizes of the attention layers activation: ActivationFunction Activation function used in the linear->activation->linear transformations init_resweight: float Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. attention_mode: Sequence[RelativeAttentionMode] Mode of relative position infused attention layer. See the [relative attention](relative_attention) documentation for more information. n_layers: int Number of layers in the Transformer \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( dropout_p ) self . input_size = input_size self . num_heads = num_heads self . dropout_p = dropout_p self . head_size = head_size self . activation = activation self . init_resweight = init_resweight self . n_relative_positions = n_relative_positions self . attention_mode = attention_mode self . n_layers = n_layers forward ( embeds , boxes ) Forward pass of the BoxTransformer PARAMETER DESCRIPTION embeds Embeddings to contextualize Shape: n_samples * n_keys * input_size TYPE: torch . FloatTensor boxes Layout features of the input elements TYPE: Dict RETURNS DESCRIPTION Tuple [ torch . FloatTensor , List [ torch . FloatTensor ]] Output of the last BoxTransformerLayer Shape: n_samples * n_queries * n_keys Attention logits of all layers Shape: n_samples * n_queries * n_keys * n_heads Source code in edspdf/layers/box_transformer.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def forward ( self , embeds : torch . FloatTensor , boxes : Dict , ) -> Tuple [ torch . FloatTensor , List [ torch . FloatTensor ]]: \"\"\" Forward pass of the BoxTransformer Parameters ---------- embeds: torch.FloatTensor Embeddings to contextualize Shape: `n_samples * n_keys * input_size` boxes: Dict Layout features of the input elements Returns ------- Tuple[torch.FloatTensor, List[torch.FloatTensor]] - Output of the last BoxTransformerLayer Shape: `n_samples * n_queries * n_keys` - Attention logits of all layers Shape: `n_samples * n_queries * n_keys * n_heads` \"\"\" page_boxes = boxes [ \"page_ids\" ] mask = page_boxes != - 1 n_samples , n_boxes_per_sample = mask . shape n_pages = page_boxes . shape [ 0 ] device = page_boxes . device embeds_with_cls = torch . cat ( [ self . empty_embed * torch . ones ( n_pages , 1 , 1 , device = device ), embeds [ page_boxes ], ], dim = 1 , ) mask_with_cls = torch . cat ( [ torch . ones ( n_samples , 1 , dtype = torch . bool , device = device ), mask , ], dim = 1 , ) n = n_boxes_per_sample + 1 relative_positions = torch . zeros ( n_pages , n , n , 2 , dtype = torch . long , device = device ) relative_positions [:, 1 :, 1 :, :] = compute_pdf_relative_positions ( x0 = boxes [ \"xmin\" ][ page_boxes ], x1 = boxes [ \"xmax\" ][ page_boxes ], y0 = boxes [ \"ymin\" ][ page_boxes ], y1 = boxes [ \"ymax\" ][ page_boxes ], width = boxes [ \"width\" ][ page_boxes ], height = boxes [ \"height\" ][ page_boxes ], n_relative_positions = self . n_relative_positions , ) no_position_mask = torch . ones ( n_pages , n , n , dtype = torch . bool , device = device ) no_position_mask [:, 1 :, 1 :] = 0 attention = [] for layer in self . layers : embeds , attn = layer ( embeds_with_cls , mask_with_cls , relative_positions = relative_positions , no_position_mask = no_position_mask , ) attention . append ( attn ) return embeds [:, 1 :][ mask ]","title":"box_transformer"},{"location":"reference/layers/box_transformer/#edspdflayersbox_transformer","text":"","title":"edspdf.layers.box_transformer"},{"location":"reference/layers/box_transformer/#edspdf.layers.box_transformer.BoxTransformerLayer","text":"Bases: torch . nn . Module BoxTransformerLayer combining a self attention layer and a linear->activation->linear transformation. Source code in edspdf/layers/box_transformer.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 class BoxTransformerLayer ( torch . nn . Module ): \"\"\" BoxTransformerLayer combining a self attention layer and a linear->activation->linear transformation. \"\"\" def __init__ ( self , input_size : int , num_heads : int = 2 , dropout_p : float = 0.15 , head_size : Optional [ int ] = None , activation : ActivationFunction = \"gelu\" , init_resweight : float = 0.0 , attention_mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"c2p\" , \"p2c\" ), position_embedding : Optional [ Union [ torch . FloatTensor , torch . nn . Parameter ] ] = None , ): \"\"\" Parameters ---------- input_size: int Input embedding size num_heads: int Number of attention heads in the attention layer dropout_p: float Dropout probability both for the attention layer and embedding projections head_size: int Head sizes of the attention layer activation: ActivationFunction Activation function used in the linear->activation->linear transformation init_resweight: float Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. attention_mode: Sequence[RelativeAttentionMode] Mode of relative position infused attention layer. See the [relative attention](relative_attention) documentation for more information. position_embedding: torch.FloatTensor Position embedding to use as key/query position embedding in the attention computation. \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( dropout_p ) self . attention = RelativeAttention ( size = input_size , n_heads = num_heads , do_pooling = True , head_size = head_size , position_embedding = position_embedding , dropout_p = dropout_p , n_coordinates = 2 , mode = attention_mode , ) self . resweight = torch . nn . Parameter ( torch . Tensor ([ float ( init_resweight )])) self . norm = torch . nn . LayerNorm ( input_size ) self . activation = get_activation_function ( activation ) self . resweight2 = torch . nn . Parameter ( torch . Tensor ([ float ( init_resweight )])) self . norm2 = torch . nn . LayerNorm ( input_size ) self . linear1 = torch . nn . Linear ( input_size , input_size * 2 ) self . linear2 = torch . nn . Linear ( input_size * 2 , input_size ) def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , relative_positions : torch . LongTensor , no_position_mask : Optional [ torch . BoolTensor ] = None , ) -> Tuple [ torch . FloatTensor , torch . FloatTensor ]: \"\"\" Forward pass of the BoxTransformerLayer Parameters ---------- embeds: torch.FloatTensor Embeddings to contextualize Shape: `n_samples * n_keys * input_size` mask: torch.BoolTensor Mask of the embeddings. 0 means padding element. Shape: `n_samples * n_keys` relative_positions: torch.LongTensor Position of the keys relatively to the query elements Shape: `n_samples * n_queries * n_keys * n_coordinates (2 for x/y)` no_position_mask: Optional[torch.BoolTensor] Key / query pairs for which the position attention terms should be disabled. Shape: `n_samples * n_queries * n_keys` Returns ------- Tuple[torch.FloatTensor, torch.FloatTensor] - Contextualized embeddings Shape: `n_samples * n_queries * n_keys` - Attention logits Shape: `n_samples * n_queries * n_keys * n_heads` \"\"\" update , attn = self . attention ( embeds , embeds , embeds , mask , relative_positions = relative_positions , no_position_mask = no_position_mask , ) embeds = embeds + self . dropout ( update ) * self . resweight embeds = self . norm ( embeds ) update = self . linear2 ( self . dropout ( self . activation ( self . linear1 ( embeds )))) embeds = embeds + self . dropout ( update ) * self . resweight2 embeds = self . norm2 ( embeds ) return embeds , attn","title":"BoxTransformerLayer"},{"location":"reference/layers/box_transformer/#edspdf.layers.box_transformer.BoxTransformerLayer.__init__","text":"PARAMETER DESCRIPTION input_size Input embedding size TYPE: int num_heads Number of attention heads in the attention layer TYPE: int DEFAULT: 2 dropout_p Dropout probability both for the attention layer and embedding projections TYPE: float DEFAULT: 0.15 head_size Head sizes of the attention layer TYPE: Optional [ int ] DEFAULT: None activation Activation function used in the linear->activation->linear transformation TYPE: ActivationFunction DEFAULT: 'gelu' init_resweight Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. TYPE: float DEFAULT: 0.0 attention_mode Mode of relative position infused attention layer. See the relative attention documentation for more information. TYPE: Sequence [ RelativeAttentionMode ] DEFAULT: ('c2c', 'c2p', 'p2c') position_embedding Position embedding to use as key/query position embedding in the attention computation. TYPE: Optional [ Union [ torch . FloatTensor , torch . nn . Parameter ]] DEFAULT: None Source code in edspdf/layers/box_transformer.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , input_size : int , num_heads : int = 2 , dropout_p : float = 0.15 , head_size : Optional [ int ] = None , activation : ActivationFunction = \"gelu\" , init_resweight : float = 0.0 , attention_mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"c2p\" , \"p2c\" ), position_embedding : Optional [ Union [ torch . FloatTensor , torch . nn . Parameter ] ] = None , ): \"\"\" Parameters ---------- input_size: int Input embedding size num_heads: int Number of attention heads in the attention layer dropout_p: float Dropout probability both for the attention layer and embedding projections head_size: int Head sizes of the attention layer activation: ActivationFunction Activation function used in the linear->activation->linear transformation init_resweight: float Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. attention_mode: Sequence[RelativeAttentionMode] Mode of relative position infused attention layer. See the [relative attention](relative_attention) documentation for more information. position_embedding: torch.FloatTensor Position embedding to use as key/query position embedding in the attention computation. \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( dropout_p ) self . attention = RelativeAttention ( size = input_size , n_heads = num_heads , do_pooling = True , head_size = head_size , position_embedding = position_embedding , dropout_p = dropout_p , n_coordinates = 2 , mode = attention_mode , ) self . resweight = torch . nn . Parameter ( torch . Tensor ([ float ( init_resweight )])) self . norm = torch . nn . LayerNorm ( input_size ) self . activation = get_activation_function ( activation ) self . resweight2 = torch . nn . Parameter ( torch . Tensor ([ float ( init_resweight )])) self . norm2 = torch . nn . LayerNorm ( input_size ) self . linear1 = torch . nn . Linear ( input_size , input_size * 2 ) self . linear2 = torch . nn . Linear ( input_size * 2 , input_size )","title":"__init__()"},{"location":"reference/layers/box_transformer/#edspdf.layers.box_transformer.BoxTransformerLayer.forward","text":"Forward pass of the BoxTransformerLayer PARAMETER DESCRIPTION embeds Embeddings to contextualize Shape: n_samples * n_keys * input_size TYPE: torch . FloatTensor mask Mask of the embeddings. 0 means padding element. Shape: n_samples * n_keys TYPE: torch . BoolTensor relative_positions Position of the keys relatively to the query elements Shape: n_samples * n_queries * n_keys * n_coordinates (2 for x/y) TYPE: torch . LongTensor no_position_mask Key / query pairs for which the position attention terms should be disabled. Shape: n_samples * n_queries * n_keys TYPE: Optional [ torch . BoolTensor ] DEFAULT: None RETURNS DESCRIPTION Tuple [ torch . FloatTensor , torch . FloatTensor ] Contextualized embeddings Shape: n_samples * n_queries * n_keys Attention logits Shape: n_samples * n_queries * n_keys * n_heads Source code in edspdf/layers/box_transformer.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor , relative_positions : torch . LongTensor , no_position_mask : Optional [ torch . BoolTensor ] = None , ) -> Tuple [ torch . FloatTensor , torch . FloatTensor ]: \"\"\" Forward pass of the BoxTransformerLayer Parameters ---------- embeds: torch.FloatTensor Embeddings to contextualize Shape: `n_samples * n_keys * input_size` mask: torch.BoolTensor Mask of the embeddings. 0 means padding element. Shape: `n_samples * n_keys` relative_positions: torch.LongTensor Position of the keys relatively to the query elements Shape: `n_samples * n_queries * n_keys * n_coordinates (2 for x/y)` no_position_mask: Optional[torch.BoolTensor] Key / query pairs for which the position attention terms should be disabled. Shape: `n_samples * n_queries * n_keys` Returns ------- Tuple[torch.FloatTensor, torch.FloatTensor] - Contextualized embeddings Shape: `n_samples * n_queries * n_keys` - Attention logits Shape: `n_samples * n_queries * n_keys * n_heads` \"\"\" update , attn = self . attention ( embeds , embeds , embeds , mask , relative_positions = relative_positions , no_position_mask = no_position_mask , ) embeds = embeds + self . dropout ( update ) * self . resweight embeds = self . norm ( embeds ) update = self . linear2 ( self . dropout ( self . activation ( self . linear1 ( embeds )))) embeds = embeds + self . dropout ( update ) * self . resweight2 embeds = self . norm2 ( embeds ) return embeds , attn","title":"forward()"},{"location":"reference/layers/box_transformer/#edspdf.layers.box_transformer.BoxTransformer","text":"Bases: Module Source code in edspdf/layers/box_transformer.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 @registry . factory . register ( \"box-transformer\" ) class BoxTransformer ( Module ): def __init__ ( self , input_size : Optional [ int ] = None , num_heads : int = 2 , dropout_p : float = 0.15 , head_size : Optional [ int ] = None , activation : ActivationFunction = \"gelu\" , init_resweight : float = 0.0 , n_relative_positions : Optional [ int ] = None , attention_mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"c2p\" , \"p2c\" ), n_layers : int = 2 , ): \"\"\" BoxTransformer combining a multiple BoxTransformerLayer Parameters ---------- input_size: int Input embedding size num_heads: int Number of attention heads in the attention layers n_relative_positions: int Maximum range of embeddable relative positions between boxes (further distances are capped to \u00b1n_relative_positions // 2) dropout_p: float Dropout probability both for the attention layers and embedding projections head_size: int Head sizes of the attention layers activation: ActivationFunction Activation function used in the linear->activation->linear transformations init_resweight: float Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. attention_mode: Sequence[RelativeAttentionMode] Mode of relative position infused attention layer. See the [relative attention](relative_attention) documentation for more information. n_layers: int Number of layers in the Transformer \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( dropout_p ) self . input_size = input_size self . num_heads = num_heads self . dropout_p = dropout_p self . head_size = head_size self . activation = activation self . init_resweight = init_resweight self . n_relative_positions = n_relative_positions self . attention_mode = attention_mode self . n_layers = n_layers def initialize ( self , gold_data : Iterable , input_size : int = None , ** kwargs ): super () . initialize ( gold_data , input_size = input_size , ** kwargs ) self . empty_embed = torch . nn . Parameter ( torch . randn ( self . input_size )) self . position_embedding = torch . nn . Parameter ( torch . randn ( ( self . n_relative_positions , self . input_size , ) ) ) self . layers = torch . nn . ModuleList ( [ BoxTransformerLayer ( input_size = self . input_size , num_heads = self . num_heads , head_size = self . head_size , dropout_p = self . dropout_p , activation = self . activation , init_resweight = self . init_resweight , attention_mode = self . attention_mode , position_embedding = self . position_embedding , ) for _ in range ( self . n_layers ) ] ) def forward ( self , embeds : torch . FloatTensor , boxes : Dict , ) -> Tuple [ torch . FloatTensor , List [ torch . FloatTensor ]]: \"\"\" Forward pass of the BoxTransformer Parameters ---------- embeds: torch.FloatTensor Embeddings to contextualize Shape: `n_samples * n_keys * input_size` boxes: Dict Layout features of the input elements Returns ------- Tuple[torch.FloatTensor, List[torch.FloatTensor]] - Output of the last BoxTransformerLayer Shape: `n_samples * n_queries * n_keys` - Attention logits of all layers Shape: `n_samples * n_queries * n_keys * n_heads` \"\"\" page_boxes = boxes [ \"page_ids\" ] mask = page_boxes != - 1 n_samples , n_boxes_per_sample = mask . shape n_pages = page_boxes . shape [ 0 ] device = page_boxes . device embeds_with_cls = torch . cat ( [ self . empty_embed * torch . ones ( n_pages , 1 , 1 , device = device ), embeds [ page_boxes ], ], dim = 1 , ) mask_with_cls = torch . cat ( [ torch . ones ( n_samples , 1 , dtype = torch . bool , device = device ), mask , ], dim = 1 , ) n = n_boxes_per_sample + 1 relative_positions = torch . zeros ( n_pages , n , n , 2 , dtype = torch . long , device = device ) relative_positions [:, 1 :, 1 :, :] = compute_pdf_relative_positions ( x0 = boxes [ \"xmin\" ][ page_boxes ], x1 = boxes [ \"xmax\" ][ page_boxes ], y0 = boxes [ \"ymin\" ][ page_boxes ], y1 = boxes [ \"ymax\" ][ page_boxes ], width = boxes [ \"width\" ][ page_boxes ], height = boxes [ \"height\" ][ page_boxes ], n_relative_positions = self . n_relative_positions , ) no_position_mask = torch . ones ( n_pages , n , n , dtype = torch . bool , device = device ) no_position_mask [:, 1 :, 1 :] = 0 attention = [] for layer in self . layers : embeds , attn = layer ( embeds_with_cls , mask_with_cls , relative_positions = relative_positions , no_position_mask = no_position_mask , ) attention . append ( attn ) return embeds [:, 1 :][ mask ]","title":"BoxTransformer"},{"location":"reference/layers/box_transformer/#edspdf.layers.box_transformer.BoxTransformer.__init__","text":"BoxTransformer combining a multiple BoxTransformerLayer PARAMETER DESCRIPTION input_size Input embedding size TYPE: Optional [ int ] DEFAULT: None num_heads Number of attention heads in the attention layers TYPE: int DEFAULT: 2 n_relative_positions Maximum range of embeddable relative positions between boxes (further distances are capped to \u00b1n_relative_positions // 2) TYPE: Optional [ int ] DEFAULT: None dropout_p Dropout probability both for the attention layers and embedding projections TYPE: float DEFAULT: 0.15 head_size Head sizes of the attention layers TYPE: Optional [ int ] DEFAULT: None activation Activation function used in the linear->activation->linear transformations TYPE: ActivationFunction DEFAULT: 'gelu' init_resweight Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. TYPE: float DEFAULT: 0.0 attention_mode Mode of relative position infused attention layer. See the relative attention documentation for more information. TYPE: Sequence [ RelativeAttentionMode ] DEFAULT: ('c2c', 'c2p', 'p2c') n_layers Number of layers in the Transformer TYPE: int DEFAULT: 2 Source code in edspdf/layers/box_transformer.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 def __init__ ( self , input_size : Optional [ int ] = None , num_heads : int = 2 , dropout_p : float = 0.15 , head_size : Optional [ int ] = None , activation : ActivationFunction = \"gelu\" , init_resweight : float = 0.0 , n_relative_positions : Optional [ int ] = None , attention_mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"c2p\" , \"p2c\" ), n_layers : int = 2 , ): \"\"\" BoxTransformer combining a multiple BoxTransformerLayer Parameters ---------- input_size: int Input embedding size num_heads: int Number of attention heads in the attention layers n_relative_positions: int Maximum range of embeddable relative positions between boxes (further distances are capped to \u00b1n_relative_positions // 2) dropout_p: float Dropout probability both for the attention layers and embedding projections head_size: int Head sizes of the attention layers activation: ActivationFunction Activation function used in the linear->activation->linear transformations init_resweight: float Initial weight of the residual gates. At 0, the layer acts (initially) as an identity function, and at 1 as a standard Transformer layer. Initializing with a value close to 0 can help the training converge. attention_mode: Sequence[RelativeAttentionMode] Mode of relative position infused attention layer. See the [relative attention](relative_attention) documentation for more information. n_layers: int Number of layers in the Transformer \"\"\" super () . __init__ () self . dropout = torch . nn . Dropout ( dropout_p ) self . input_size = input_size self . num_heads = num_heads self . dropout_p = dropout_p self . head_size = head_size self . activation = activation self . init_resweight = init_resweight self . n_relative_positions = n_relative_positions self . attention_mode = attention_mode self . n_layers = n_layers","title":"__init__()"},{"location":"reference/layers/box_transformer/#edspdf.layers.box_transformer.BoxTransformer.forward","text":"Forward pass of the BoxTransformer PARAMETER DESCRIPTION embeds Embeddings to contextualize Shape: n_samples * n_keys * input_size TYPE: torch . FloatTensor boxes Layout features of the input elements TYPE: Dict RETURNS DESCRIPTION Tuple [ torch . FloatTensor , List [ torch . FloatTensor ]] Output of the last BoxTransformerLayer Shape: n_samples * n_queries * n_keys Attention logits of all layers Shape: n_samples * n_queries * n_keys * n_heads Source code in edspdf/layers/box_transformer.py 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def forward ( self , embeds : torch . FloatTensor , boxes : Dict , ) -> Tuple [ torch . FloatTensor , List [ torch . FloatTensor ]]: \"\"\" Forward pass of the BoxTransformer Parameters ---------- embeds: torch.FloatTensor Embeddings to contextualize Shape: `n_samples * n_keys * input_size` boxes: Dict Layout features of the input elements Returns ------- Tuple[torch.FloatTensor, List[torch.FloatTensor]] - Output of the last BoxTransformerLayer Shape: `n_samples * n_queries * n_keys` - Attention logits of all layers Shape: `n_samples * n_queries * n_keys * n_heads` \"\"\" page_boxes = boxes [ \"page_ids\" ] mask = page_boxes != - 1 n_samples , n_boxes_per_sample = mask . shape n_pages = page_boxes . shape [ 0 ] device = page_boxes . device embeds_with_cls = torch . cat ( [ self . empty_embed * torch . ones ( n_pages , 1 , 1 , device = device ), embeds [ page_boxes ], ], dim = 1 , ) mask_with_cls = torch . cat ( [ torch . ones ( n_samples , 1 , dtype = torch . bool , device = device ), mask , ], dim = 1 , ) n = n_boxes_per_sample + 1 relative_positions = torch . zeros ( n_pages , n , n , 2 , dtype = torch . long , device = device ) relative_positions [:, 1 :, 1 :, :] = compute_pdf_relative_positions ( x0 = boxes [ \"xmin\" ][ page_boxes ], x1 = boxes [ \"xmax\" ][ page_boxes ], y0 = boxes [ \"ymin\" ][ page_boxes ], y1 = boxes [ \"ymax\" ][ page_boxes ], width = boxes [ \"width\" ][ page_boxes ], height = boxes [ \"height\" ][ page_boxes ], n_relative_positions = self . n_relative_positions , ) no_position_mask = torch . ones ( n_pages , n , n , dtype = torch . bool , device = device ) no_position_mask [:, 1 :, 1 :] = 0 attention = [] for layer in self . layers : embeds , attn = layer ( embeds_with_cls , mask_with_cls , relative_positions = relative_positions , no_position_mask = no_position_mask , ) attention . append ( attn ) return embeds [:, 1 :][ mask ]","title":"forward()"},{"location":"reference/layers/cnn_pooler/","text":"edspdf.layers.cnn_pooler CnnPooler Bases: Module One dimension CNN encoding multi-kernel layer. Input embeddings are convoluted using linear kernels each parametrized with a (window) size of kernel_size[kernel_i] The output of the kernels are concatenated together, max-pooled and finally projected to a size of output_size . Source code in edspdf/layers/cnn_pooler.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 @registry . factory . register ( \"cnn-pooler\" ) class CnnPooler ( Module ): \"\"\" One dimension CNN encoding multi-kernel layer. Input embeddings are convoluted using linear kernels each parametrized with a (window) size of `kernel_size[kernel_i]` The output of the kernels are concatenated together, max-pooled and finally projected to a size of `output_size`. \"\"\" def __init__ ( self , input_size : Optional [ int ] = None , output_size : Optional [ int ] = None , out_channels : Optional [ int ] = None , kernel_sizes : Sequence [ int ] = ( 3 , 4 , 5 ), activation : ActivationFunction = \"relu\" , ): \"\"\" Parameters ---------- input_size: int Size of the input embeddings output_size: Optional[int] Size of the output embeddings Defaults to the `input_size` out_channels: int Number of channels kernel_sizes: Sequence[int] Window size of each kernel activation: str Activation function to use \"\"\" super () . __init__ () self . input_size = input_size self . output_size = output_size self . out_channels = out_channels self . kernel_sizes = kernel_sizes self . activation = get_activation_function ( activation ) def initialize ( self , gold_data , ** kwargs ): super () . initialize ( gold_data , ** kwargs ) if self . out_channels is None : self . out_channels = self . input_size if self . output_size is None : self . output_size = self . input_size self . convolutions = torch . nn . ModuleList ( torch . nn . Conv1d ( in_channels = self . input_size , out_channels = self . out_channels , kernel_size = kernel_size , padding = 0 , ) for kernel_size in self . kernel_sizes ) self . linear = torch . nn . Linear ( in_features = self . out_channels * len ( self . kernel_sizes ), out_features = self . output_size , ) def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor ): \"\"\" Encode embeddings with a 1d convolutional network Parameters ---------- embeds: torch.FloatTensor Input embeddings Shape: `n_samples * n_elements * input_size` mask: torch.BoolTensor Input mask. 0 values are for padding elements. Padding elements are masked with a 0 value before the convolution. Shape: `n_samples * n_elements * input_size` Returns ------- torch.FloatTensor Shape: `n_samples * output_size` \"\"\" embeds = embeds . masked_fill ( ~ mask . unsqueeze ( - 1 ), 0 ) . permute ( 0 , 2 , 1 ) # sample word dim -> sample dim word embeds = torch . cat ( [ self . activation ( # pad by the appropriate amount on both sides of each sentence conv ( F . pad ( embeds , pad = [ conv . kernel_size [ 0 ] // 2 , ( conv . kernel_size [ 0 ] - 1 ) // 2 , ], ) ) . permute ( 0 , 2 , 1 ) . masked_fill ( ~ mask . unsqueeze ( - 1 ), 0 ) ) for conv in self . convolutions ], dim = 2 , ) pooled = embeds . max ( 1 ) . values return self . linear ( pooled ) __init__ ( input_size = None , output_size = None , out_channels = None , kernel_sizes = ( 3 , 4 , 5 ), activation = 'relu' ) PARAMETER DESCRIPTION input_size Size of the input embeddings TYPE: Optional [ int ] DEFAULT: None output_size Size of the output embeddings Defaults to the input_size TYPE: Optional [ int ] DEFAULT: None out_channels Number of channels TYPE: Optional [ int ] DEFAULT: None kernel_sizes Window size of each kernel TYPE: Sequence [ int ] DEFAULT: (3, 4, 5) activation Activation function to use TYPE: ActivationFunction DEFAULT: 'relu' Source code in edspdf/layers/cnn_pooler.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , input_size : Optional [ int ] = None , output_size : Optional [ int ] = None , out_channels : Optional [ int ] = None , kernel_sizes : Sequence [ int ] = ( 3 , 4 , 5 ), activation : ActivationFunction = \"relu\" , ): \"\"\" Parameters ---------- input_size: int Size of the input embeddings output_size: Optional[int] Size of the output embeddings Defaults to the `input_size` out_channels: int Number of channels kernel_sizes: Sequence[int] Window size of each kernel activation: str Activation function to use \"\"\" super () . __init__ () self . input_size = input_size self . output_size = output_size self . out_channels = out_channels self . kernel_sizes = kernel_sizes self . activation = get_activation_function ( activation ) forward ( embeds , mask ) Encode embeddings with a 1d convolutional network PARAMETER DESCRIPTION embeds Input embeddings Shape: n_samples * n_elements * input_size TYPE: torch . FloatTensor mask Input mask. 0 values are for padding elements. Padding elements are masked with a 0 value before the convolution. Shape: n_samples * n_elements * input_size TYPE: torch . BoolTensor RETURNS DESCRIPTION torch . FloatTensor Shape Source code in edspdf/layers/cnn_pooler.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor ): \"\"\" Encode embeddings with a 1d convolutional network Parameters ---------- embeds: torch.FloatTensor Input embeddings Shape: `n_samples * n_elements * input_size` mask: torch.BoolTensor Input mask. 0 values are for padding elements. Padding elements are masked with a 0 value before the convolution. Shape: `n_samples * n_elements * input_size` Returns ------- torch.FloatTensor Shape: `n_samples * output_size` \"\"\" embeds = embeds . masked_fill ( ~ mask . unsqueeze ( - 1 ), 0 ) . permute ( 0 , 2 , 1 ) # sample word dim -> sample dim word embeds = torch . cat ( [ self . activation ( # pad by the appropriate amount on both sides of each sentence conv ( F . pad ( embeds , pad = [ conv . kernel_size [ 0 ] // 2 , ( conv . kernel_size [ 0 ] - 1 ) // 2 , ], ) ) . permute ( 0 , 2 , 1 ) . masked_fill ( ~ mask . unsqueeze ( - 1 ), 0 ) ) for conv in self . convolutions ], dim = 2 , ) pooled = embeds . max ( 1 ) . values return self . linear ( pooled )","title":"cnn_pooler"},{"location":"reference/layers/cnn_pooler/#edspdflayerscnn_pooler","text":"","title":"edspdf.layers.cnn_pooler"},{"location":"reference/layers/cnn_pooler/#edspdf.layers.cnn_pooler.CnnPooler","text":"Bases: Module One dimension CNN encoding multi-kernel layer. Input embeddings are convoluted using linear kernels each parametrized with a (window) size of kernel_size[kernel_i] The output of the kernels are concatenated together, max-pooled and finally projected to a size of output_size . Source code in edspdf/layers/cnn_pooler.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 @registry . factory . register ( \"cnn-pooler\" ) class CnnPooler ( Module ): \"\"\" One dimension CNN encoding multi-kernel layer. Input embeddings are convoluted using linear kernels each parametrized with a (window) size of `kernel_size[kernel_i]` The output of the kernels are concatenated together, max-pooled and finally projected to a size of `output_size`. \"\"\" def __init__ ( self , input_size : Optional [ int ] = None , output_size : Optional [ int ] = None , out_channels : Optional [ int ] = None , kernel_sizes : Sequence [ int ] = ( 3 , 4 , 5 ), activation : ActivationFunction = \"relu\" , ): \"\"\" Parameters ---------- input_size: int Size of the input embeddings output_size: Optional[int] Size of the output embeddings Defaults to the `input_size` out_channels: int Number of channels kernel_sizes: Sequence[int] Window size of each kernel activation: str Activation function to use \"\"\" super () . __init__ () self . input_size = input_size self . output_size = output_size self . out_channels = out_channels self . kernel_sizes = kernel_sizes self . activation = get_activation_function ( activation ) def initialize ( self , gold_data , ** kwargs ): super () . initialize ( gold_data , ** kwargs ) if self . out_channels is None : self . out_channels = self . input_size if self . output_size is None : self . output_size = self . input_size self . convolutions = torch . nn . ModuleList ( torch . nn . Conv1d ( in_channels = self . input_size , out_channels = self . out_channels , kernel_size = kernel_size , padding = 0 , ) for kernel_size in self . kernel_sizes ) self . linear = torch . nn . Linear ( in_features = self . out_channels * len ( self . kernel_sizes ), out_features = self . output_size , ) def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor ): \"\"\" Encode embeddings with a 1d convolutional network Parameters ---------- embeds: torch.FloatTensor Input embeddings Shape: `n_samples * n_elements * input_size` mask: torch.BoolTensor Input mask. 0 values are for padding elements. Padding elements are masked with a 0 value before the convolution. Shape: `n_samples * n_elements * input_size` Returns ------- torch.FloatTensor Shape: `n_samples * output_size` \"\"\" embeds = embeds . masked_fill ( ~ mask . unsqueeze ( - 1 ), 0 ) . permute ( 0 , 2 , 1 ) # sample word dim -> sample dim word embeds = torch . cat ( [ self . activation ( # pad by the appropriate amount on both sides of each sentence conv ( F . pad ( embeds , pad = [ conv . kernel_size [ 0 ] // 2 , ( conv . kernel_size [ 0 ] - 1 ) // 2 , ], ) ) . permute ( 0 , 2 , 1 ) . masked_fill ( ~ mask . unsqueeze ( - 1 ), 0 ) ) for conv in self . convolutions ], dim = 2 , ) pooled = embeds . max ( 1 ) . values return self . linear ( pooled )","title":"CnnPooler"},{"location":"reference/layers/cnn_pooler/#edspdf.layers.cnn_pooler.CnnPooler.__init__","text":"PARAMETER DESCRIPTION input_size Size of the input embeddings TYPE: Optional [ int ] DEFAULT: None output_size Size of the output embeddings Defaults to the input_size TYPE: Optional [ int ] DEFAULT: None out_channels Number of channels TYPE: Optional [ int ] DEFAULT: None kernel_sizes Window size of each kernel TYPE: Sequence [ int ] DEFAULT: (3, 4, 5) activation Activation function to use TYPE: ActivationFunction DEFAULT: 'relu' Source code in edspdf/layers/cnn_pooler.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def __init__ ( self , input_size : Optional [ int ] = None , output_size : Optional [ int ] = None , out_channels : Optional [ int ] = None , kernel_sizes : Sequence [ int ] = ( 3 , 4 , 5 ), activation : ActivationFunction = \"relu\" , ): \"\"\" Parameters ---------- input_size: int Size of the input embeddings output_size: Optional[int] Size of the output embeddings Defaults to the `input_size` out_channels: int Number of channels kernel_sizes: Sequence[int] Window size of each kernel activation: str Activation function to use \"\"\" super () . __init__ () self . input_size = input_size self . output_size = output_size self . out_channels = out_channels self . kernel_sizes = kernel_sizes self . activation = get_activation_function ( activation )","title":"__init__()"},{"location":"reference/layers/cnn_pooler/#edspdf.layers.cnn_pooler.CnnPooler.forward","text":"Encode embeddings with a 1d convolutional network PARAMETER DESCRIPTION embeds Input embeddings Shape: n_samples * n_elements * input_size TYPE: torch . FloatTensor mask Input mask. 0 values are for padding elements. Padding elements are masked with a 0 value before the convolution. Shape: n_samples * n_elements * input_size TYPE: torch . BoolTensor RETURNS DESCRIPTION torch . FloatTensor Shape Source code in edspdf/layers/cnn_pooler.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def forward ( self , embeds : torch . FloatTensor , mask : torch . BoolTensor ): \"\"\" Encode embeddings with a 1d convolutional network Parameters ---------- embeds: torch.FloatTensor Input embeddings Shape: `n_samples * n_elements * input_size` mask: torch.BoolTensor Input mask. 0 values are for padding elements. Padding elements are masked with a 0 value before the convolution. Shape: `n_samples * n_elements * input_size` Returns ------- torch.FloatTensor Shape: `n_samples * output_size` \"\"\" embeds = embeds . masked_fill ( ~ mask . unsqueeze ( - 1 ), 0 ) . permute ( 0 , 2 , 1 ) # sample word dim -> sample dim word embeds = torch . cat ( [ self . activation ( # pad by the appropriate amount on both sides of each sentence conv ( F . pad ( embeds , pad = [ conv . kernel_size [ 0 ] // 2 , ( conv . kernel_size [ 0 ] - 1 ) // 2 , ], ) ) . permute ( 0 , 2 , 1 ) . masked_fill ( ~ mask . unsqueeze ( - 1 ), 0 ) ) for conv in self . convolutions ], dim = 2 , ) pooled = embeds . max ( 1 ) . values return self . linear ( pooled )","title":"forward()"},{"location":"reference/layers/relative_attention/","text":"edspdf.layers.relative_attention RelativeAttention Bases: torch . nn . Module A self/cross-attention layer that takes relative position of elements into account to compute the attention weights. When running a relative attention layer, key and queries are represented using content and position embeddings, where position embeddings are retrieved using the relative position of keys relative to queries Source code in edspdf/layers/relative_attention.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 @registry . factory . register ( \"relative-attention\" ) class RelativeAttention ( torch . nn . Module ): \"\"\" A self/cross-attention layer that takes relative position of elements into account to compute the attention weights. When running a relative attention layer, key and queries are represented using content and position embeddings, where position embeddings are retrieved using the relative position of keys relative to queries \"\"\" def __init__ ( self , size : int , n_heads : int , query_size : Optional [ int ] = None , key_size : Optional [ int ] = None , value_size : Optional [ int ] = None , head_size : Optional [ int ] = None , position_embedding : Optional [ Union [ FloatTensor , Parameter ]] = None , dropout_p : float = 0.1 , same_key_query_proj : bool = False , same_positional_key_query_proj : bool = False , n_coordinates : int = 1 , head_bias : bool = True , do_pooling : bool = True , mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"p2c\" , \"c2p\" ), n_additional_heads : int = 0 , ): \"\"\" Parameters ---------- size: int The size of the output embeddings Also serves as default if query_size, pos_size, or key_size is None n_heads: int The number of attention heads query_size: Optional[int] The size of the query embeddings. key_size: Optional[int] The size of the key embeddings. value_size: Optional[int] The size of the value embeddings head_size: Optional[int] The size of each query / key / value chunk used in the attention dot product Default: `key_size / n_heads` position_embedding: Optional[torch.FloatTensor] The position embedding used as key and query embeddings dropout_p: float Dropout probability applied on the attention weights Default: 0.1 same_key_query_proj: bool Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False same_positional_key_query_proj Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False n_coordinates: int The number of positional coordinates For instance, text is 1D so 1 coordinate, images are 2D so 2 coordinates ... Default: 1 head_bias: bool Whether to learn a bias term to add to the attention logits This is only useful if you plan to use the attention logits for subsequent operations, since attention weights are unaffected by bias terms. do_pooling: bool Whether to compute the output embedding. If you only plan to use attention logits, you should disable this parameter. Default: True mode: Sequence[RelativeAttentionMode] Whether to compute content to content (c2c), content to position (c2p) or position to content (p2c) attention terms. Setting `mode=('c2c\")` disable relative position attention terms: this is the standard attention layer. To get a better intuition about these different types of attention, here is a formulation as fictitious search samples from a word in a (1D) text: \u2014 content-content : \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words whose content contains information about temporality\" \u2014 content-position: \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words that are 3 positions after of me\" \u2014 position-content : \"regardless of my content, I will attend to the word one position after from me if it contains information about temporality, two words after me if it contains information about location, etc.\" n_additional_heads: int The number of additional head logits to compute. Those are not used to compute output embeddings, but may be useful in subsequent operation. Default: 0 \"\"\" super () . __init__ () if query_size is None : query_size = size if key_size is None : key_size = size if value_size is None : value_size = key_size if head_size is None and key_size is not None : assert key_size % n_heads == 0 head_size = key_size // n_heads value_head_size = None if do_pooling and size is not None : assert size % n_heads == 0 value_head_size = size // n_heads self . n_coordinates = n_coordinates self . n_heads = n_heads + n_additional_heads self . n_additional_heads = n_additional_heads self . mode = mode n_query_heads = n_heads + n_additional_heads self . content_key_proj = torch . nn . Linear ( key_size , n_query_heads * head_size ) if isinstance ( position_embedding , torch . nn . Parameter ): self . position_embedding = position_embedding else : self . register_buffer ( \"position_embedding\" , position_embedding ) if same_key_query_proj : self . content_query_proj = self . content_key_proj else : self . content_query_proj = torch . nn . Linear ( query_size , n_query_heads * head_size , ) if do_pooling : self . content_value_proj = torch . nn . Linear ( value_size , value_head_size * n_heads ) pos_size = self . position_embedding . shape [ - 1 ] self . position_key_proj = GroupedLinear ( pos_size // n_coordinates , head_size * n_query_heads // n_coordinates , n_groups = n_coordinates , ) if same_key_query_proj or same_positional_key_query_proj : self . position_query_proj = self . position_key_proj else : self . position_query_proj = GroupedLinear ( pos_size // n_coordinates , head_size * n_query_heads // n_coordinates , n_groups = n_coordinates , ) self . dropout = torch . nn . Dropout ( dropout_p ) if head_bias : self . bias = torch . nn . Parameter ( torch . zeros ( n_query_heads )) self . output_size = size def forward ( self , content_queries : torch . FloatTensor , content_keys : Optional [ torch . FloatTensor ] = None , content_values : Optional [ torch . FloatTensor ] = None , mask : Optional [ torch . BoolTensor ] = None , relative_positions : Optional [ torch . LongTensor ] = None , no_position_mask : Optional [ torch . BoolTensor ] = None , base_attn : Optional [ torch . FloatTensor ] = None , ) -> Union [ Tuple [ torch . FloatTensor , torch . FloatTensor ], torch . FloatTensor ]: \"\"\" Forward pass of the RelativeAttention layer. Parameters ---------- content_queries: torch.FloatTensor The content query embedding to use in the attention computation Shape: `n_samples * n_queries * query_size` content_keys: Optional[torch.FloatTensor] The content key embedding to use in the attention computation. If None, defaults to the `content_queries` Shape: `n_samples * n_keys * query_size` content_values: Optional[torch.FloatTensor] The content values embedding to use in the final pooling computation. If None, pooling won't be performed. Shape: `n_samples * n_keys * query_size` mask: Optional[torch.BoolTensor] The content key embedding to use in the attention computation. If None, defaults to the `content_queries` Shape: either - `n_samples * n_keys` - `n_samples * n_queries * n_keys` - `n_samples * n_queries * n_keys * n_heads` relative_positions: Optional[torch.LongTensor] The relative position of keys relative to queries If None, positional attention terms won't be computed. Shape: `n_samples * n_queries * n_keys * n_coordinates` no_position_mask: Optional[torch.BoolTensor] Key / query pairs for which the position attention terms should be disabled. Shape: `n_samples * n_queries * n_keys` base_attn: Optional[torch.FloatTensor] Attention logits to add to the computed attention logits Shape: `n_samples * n_queries * n_keys * n_heads` Returns ------- Union[Tuple[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor] - the output contextualized embeddings (only if content_values is not None and the `do_pooling` attribute is set to True) Shape: n_sample * n_keys * `size` - the attention logits Shape: n_sample * n_keys * n_queries * (n_heads + n_additional_heads) \"\"\" if content_keys is None : content_keys = content_queries attn = ( torch . zeros ( content_queries . shape [ 0 ], content_queries . shape [ 1 ], content_keys . shape [ 1 ], self . n_heads , device = content_queries . device , ) if base_attn is None else base_attn ) attn_weights = [] if 0 not in content_queries . shape and 0 not in content_keys . shape : content_keys = make_heads ( self . content_key_proj ( self . dropout ( content_keys )), self . n_heads ) content_queries = make_heads ( self . content_query_proj ( self . dropout ( content_queries )), self . n_heads ) if content_values is not None : content_values = make_heads ( self . content_value_proj ( self . dropout ( content_values )), self . n_heads - self . n_additional_heads , ) size = content_queries . shape [ - 1 ] if \"c2c\" in self . mode : content_to_content_attn = torch . einsum ( \"nihd,njhd->nijh\" , content_queries , content_keys ) / math . sqrt ( size ) attn_weights . append ( content_to_content_attn ) if relative_positions is not None and ( \"p2c\" in self . mode or \"c2p\" in self . mode ): position_keys = make_heads ( self . position_key_proj ( self . dropout ( self . position_embedding )), ( self . n_coordinates , self . n_heads ), ) position_queries = make_heads ( self . position_query_proj ( self . dropout ( self . position_embedding )), ( self . n_coordinates , self . n_heads ), ) relative_positions = ( position_queries . shape [ 0 ] // 2 + relative_positions ) . clamp ( 0 , position_queries . shape [ 0 ] - 1 ) if \"c2p\" in self . mode : content_to_position_attn = torch . einsum ( \"nihxd,zxhd->nizhx\" , make_heads ( content_queries , self . n_coordinates ), position_keys , ) content_to_position_attn = gather ( content_to_position_attn , index = relative_positions . unsqueeze ( - 2 ), dim = 2 , ) . sum ( - 1 ) / math . sqrt ( size ) if no_position_mask is not None : content_to_position_attn = content_to_position_attn . masked_fill ( no_position_mask [ ... , None ], 0 ) attn_weights . append ( content_to_position_attn ) if \"p2c\" in self . mode : position_to_content_attn = torch . einsum ( \"zxhd,njhxd->nzjhx\" , position_queries , make_heads ( content_keys , self . n_coordinates ), ) position_to_content_attn = gather ( position_to_content_attn , index = relative_positions . unsqueeze ( - 2 ), dim = 1 , ) . sum ( - 1 ) / math . sqrt ( size ) if no_position_mask is not None : position_to_content_attn = position_to_content_attn . masked_fill ( no_position_mask [ ... , None ], 0 ) attn_weights . append ( position_to_content_attn ) attn = attn + sum ( attn_weights ) / math . sqrt ( len ( attn_weights )) if hasattr ( self , \"bias\" ): attn = attn + self . bias if content_values is not None : if mask . ndim == 2 : mask = mask [:, None , :, None ] if mask . ndim == 3 : mask = mask [:, :, :, None ] weights = self . dropout ( attn [ ... , self . n_additional_heads :] . masked_fill ( ~ mask , IMPOSSIBLE ) . softmax ( - 2 ) ) pooled = torch . einsum ( \"nijh,njhd->nihd\" , weights , content_values ) pooled = pooled . reshape ( * pooled . shape [: - 2 ], - 1 ) return pooled , attn return attn __init__ ( size , n_heads , query_size = None , key_size = None , value_size = None , head_size = None , position_embedding = None , dropout_p = 0.1 , same_key_query_proj = False , same_positional_key_query_proj = False , n_coordinates = 1 , head_bias = True , do_pooling = True , mode = ( 'c2c' , 'p2c' , 'c2p' ), n_additional_heads = 0 ) PARAMETER DESCRIPTION size The size of the output embeddings Also serves as default if query_size, pos_size, or key_size is None TYPE: int n_heads The number of attention heads TYPE: int query_size The size of the query embeddings. TYPE: Optional [ int ] DEFAULT: None key_size The size of the key embeddings. TYPE: Optional [ int ] DEFAULT: None value_size The size of the value embeddings TYPE: Optional [ int ] DEFAULT: None head_size The size of each query / key / value chunk used in the attention dot product Default: key_size / n_heads TYPE: Optional [ int ] DEFAULT: None position_embedding The position embedding used as key and query embeddings TYPE: Optional [ Union [ FloatTensor , Parameter ]] DEFAULT: None dropout_p Dropout probability applied on the attention weights Default: 0.1 TYPE: float DEFAULT: 0.1 same_key_query_proj Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False TYPE: bool DEFAULT: False same_positional_key_query_proj Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False TYPE: bool DEFAULT: False n_coordinates The number of positional coordinates For instance, text is 1D so 1 coordinate, images are 2D so 2 coordinates ... Default: 1 TYPE: int DEFAULT: 1 head_bias Whether to learn a bias term to add to the attention logits This is only useful if you plan to use the attention logits for subsequent operations, since attention weights are unaffected by bias terms. TYPE: bool DEFAULT: True do_pooling Whether to compute the output embedding. If you only plan to use attention logits, you should disable this parameter. Default: True TYPE: bool DEFAULT: True mode Whether to compute content to content (c2c), content to position (c2p) or position to content (p2c) attention terms. Setting mode=('c2c\") disable relative position attention terms: this is the standard attention layer. To get a better intuition about these different types of attention, here is a formulation as fictitious search samples from a word in a (1D) text: \u2014 content-content : \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words whose content contains information about temporality\" \u2014 content-position: \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words that are 3 positions after of me\" \u2014 position-content : \"regardless of my content, I will attend to the word one position after from me if it contains information about temporality, two words after me if it contains information about location, etc.\" TYPE: Sequence [ RelativeAttentionMode ] DEFAULT: ('c2c', 'p2c', 'c2p') n_additional_heads The number of additional head logits to compute. Those are not used to compute output embeddings, but may be useful in subsequent operation. Default: 0 TYPE: int DEFAULT: 0 Source code in edspdf/layers/relative_attention.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def __init__ ( self , size : int , n_heads : int , query_size : Optional [ int ] = None , key_size : Optional [ int ] = None , value_size : Optional [ int ] = None , head_size : Optional [ int ] = None , position_embedding : Optional [ Union [ FloatTensor , Parameter ]] = None , dropout_p : float = 0.1 , same_key_query_proj : bool = False , same_positional_key_query_proj : bool = False , n_coordinates : int = 1 , head_bias : bool = True , do_pooling : bool = True , mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"p2c\" , \"c2p\" ), n_additional_heads : int = 0 , ): \"\"\" Parameters ---------- size: int The size of the output embeddings Also serves as default if query_size, pos_size, or key_size is None n_heads: int The number of attention heads query_size: Optional[int] The size of the query embeddings. key_size: Optional[int] The size of the key embeddings. value_size: Optional[int] The size of the value embeddings head_size: Optional[int] The size of each query / key / value chunk used in the attention dot product Default: `key_size / n_heads` position_embedding: Optional[torch.FloatTensor] The position embedding used as key and query embeddings dropout_p: float Dropout probability applied on the attention weights Default: 0.1 same_key_query_proj: bool Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False same_positional_key_query_proj Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False n_coordinates: int The number of positional coordinates For instance, text is 1D so 1 coordinate, images are 2D so 2 coordinates ... Default: 1 head_bias: bool Whether to learn a bias term to add to the attention logits This is only useful if you plan to use the attention logits for subsequent operations, since attention weights are unaffected by bias terms. do_pooling: bool Whether to compute the output embedding. If you only plan to use attention logits, you should disable this parameter. Default: True mode: Sequence[RelativeAttentionMode] Whether to compute content to content (c2c), content to position (c2p) or position to content (p2c) attention terms. Setting `mode=('c2c\")` disable relative position attention terms: this is the standard attention layer. To get a better intuition about these different types of attention, here is a formulation as fictitious search samples from a word in a (1D) text: \u2014 content-content : \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words whose content contains information about temporality\" \u2014 content-position: \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words that are 3 positions after of me\" \u2014 position-content : \"regardless of my content, I will attend to the word one position after from me if it contains information about temporality, two words after me if it contains information about location, etc.\" n_additional_heads: int The number of additional head logits to compute. Those are not used to compute output embeddings, but may be useful in subsequent operation. Default: 0 \"\"\" super () . __init__ () if query_size is None : query_size = size if key_size is None : key_size = size if value_size is None : value_size = key_size if head_size is None and key_size is not None : assert key_size % n_heads == 0 head_size = key_size // n_heads value_head_size = None if do_pooling and size is not None : assert size % n_heads == 0 value_head_size = size // n_heads self . n_coordinates = n_coordinates self . n_heads = n_heads + n_additional_heads self . n_additional_heads = n_additional_heads self . mode = mode n_query_heads = n_heads + n_additional_heads self . content_key_proj = torch . nn . Linear ( key_size , n_query_heads * head_size ) if isinstance ( position_embedding , torch . nn . Parameter ): self . position_embedding = position_embedding else : self . register_buffer ( \"position_embedding\" , position_embedding ) if same_key_query_proj : self . content_query_proj = self . content_key_proj else : self . content_query_proj = torch . nn . Linear ( query_size , n_query_heads * head_size , ) if do_pooling : self . content_value_proj = torch . nn . Linear ( value_size , value_head_size * n_heads ) pos_size = self . position_embedding . shape [ - 1 ] self . position_key_proj = GroupedLinear ( pos_size // n_coordinates , head_size * n_query_heads // n_coordinates , n_groups = n_coordinates , ) if same_key_query_proj or same_positional_key_query_proj : self . position_query_proj = self . position_key_proj else : self . position_query_proj = GroupedLinear ( pos_size // n_coordinates , head_size * n_query_heads // n_coordinates , n_groups = n_coordinates , ) self . dropout = torch . nn . Dropout ( dropout_p ) if head_bias : self . bias = torch . nn . Parameter ( torch . zeros ( n_query_heads )) self . output_size = size forward ( content_queries , content_keys = None , content_values = None , mask = None , relative_positions = None , no_position_mask = None , base_attn = None ) Forward pass of the RelativeAttention layer. PARAMETER DESCRIPTION content_queries The content query embedding to use in the attention computation Shape: n_samples * n_queries * query_size TYPE: torch . FloatTensor content_keys The content key embedding to use in the attention computation. If None, defaults to the content_queries Shape: n_samples * n_keys * query_size TYPE: Optional [ torch . FloatTensor ] DEFAULT: None content_values The content values embedding to use in the final pooling computation. If None, pooling won't be performed. Shape: n_samples * n_keys * query_size TYPE: Optional [ torch . FloatTensor ] DEFAULT: None mask The content key embedding to use in the attention computation. If None, defaults to the content_queries Shape: either - n_samples * n_keys - n_samples * n_queries * n_keys - n_samples * n_queries * n_keys * n_heads TYPE: Optional [ torch . BoolTensor ] DEFAULT: None relative_positions The relative position of keys relative to queries If None, positional attention terms won't be computed. Shape: n_samples * n_queries * n_keys * n_coordinates TYPE: Optional [ torch . LongTensor ] DEFAULT: None no_position_mask Key / query pairs for which the position attention terms should be disabled. Shape: n_samples * n_queries * n_keys TYPE: Optional [ torch . BoolTensor ] DEFAULT: None base_attn Attention logits to add to the computed attention logits Shape: n_samples * n_queries * n_keys * n_heads TYPE: Optional [ torch . FloatTensor ] DEFAULT: None RETURNS DESCRIPTION Union [ Tuple [ torch . FloatTensor , torch . FloatTensor ], torch . FloatTensor ] the output contextualized embeddings (only if content_values is not None and the do_pooling attribute is set to True) Shape: n_sample * n_keys * size the attention logits Shape: n_sample * n_keys * n_queries * (n_heads + n_additional_heads) Source code in edspdf/layers/relative_attention.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def forward ( self , content_queries : torch . FloatTensor , content_keys : Optional [ torch . FloatTensor ] = None , content_values : Optional [ torch . FloatTensor ] = None , mask : Optional [ torch . BoolTensor ] = None , relative_positions : Optional [ torch . LongTensor ] = None , no_position_mask : Optional [ torch . BoolTensor ] = None , base_attn : Optional [ torch . FloatTensor ] = None , ) -> Union [ Tuple [ torch . FloatTensor , torch . FloatTensor ], torch . FloatTensor ]: \"\"\" Forward pass of the RelativeAttention layer. Parameters ---------- content_queries: torch.FloatTensor The content query embedding to use in the attention computation Shape: `n_samples * n_queries * query_size` content_keys: Optional[torch.FloatTensor] The content key embedding to use in the attention computation. If None, defaults to the `content_queries` Shape: `n_samples * n_keys * query_size` content_values: Optional[torch.FloatTensor] The content values embedding to use in the final pooling computation. If None, pooling won't be performed. Shape: `n_samples * n_keys * query_size` mask: Optional[torch.BoolTensor] The content key embedding to use in the attention computation. If None, defaults to the `content_queries` Shape: either - `n_samples * n_keys` - `n_samples * n_queries * n_keys` - `n_samples * n_queries * n_keys * n_heads` relative_positions: Optional[torch.LongTensor] The relative position of keys relative to queries If None, positional attention terms won't be computed. Shape: `n_samples * n_queries * n_keys * n_coordinates` no_position_mask: Optional[torch.BoolTensor] Key / query pairs for which the position attention terms should be disabled. Shape: `n_samples * n_queries * n_keys` base_attn: Optional[torch.FloatTensor] Attention logits to add to the computed attention logits Shape: `n_samples * n_queries * n_keys * n_heads` Returns ------- Union[Tuple[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor] - the output contextualized embeddings (only if content_values is not None and the `do_pooling` attribute is set to True) Shape: n_sample * n_keys * `size` - the attention logits Shape: n_sample * n_keys * n_queries * (n_heads + n_additional_heads) \"\"\" if content_keys is None : content_keys = content_queries attn = ( torch . zeros ( content_queries . shape [ 0 ], content_queries . shape [ 1 ], content_keys . shape [ 1 ], self . n_heads , device = content_queries . device , ) if base_attn is None else base_attn ) attn_weights = [] if 0 not in content_queries . shape and 0 not in content_keys . shape : content_keys = make_heads ( self . content_key_proj ( self . dropout ( content_keys )), self . n_heads ) content_queries = make_heads ( self . content_query_proj ( self . dropout ( content_queries )), self . n_heads ) if content_values is not None : content_values = make_heads ( self . content_value_proj ( self . dropout ( content_values )), self . n_heads - self . n_additional_heads , ) size = content_queries . shape [ - 1 ] if \"c2c\" in self . mode : content_to_content_attn = torch . einsum ( \"nihd,njhd->nijh\" , content_queries , content_keys ) / math . sqrt ( size ) attn_weights . append ( content_to_content_attn ) if relative_positions is not None and ( \"p2c\" in self . mode or \"c2p\" in self . mode ): position_keys = make_heads ( self . position_key_proj ( self . dropout ( self . position_embedding )), ( self . n_coordinates , self . n_heads ), ) position_queries = make_heads ( self . position_query_proj ( self . dropout ( self . position_embedding )), ( self . n_coordinates , self . n_heads ), ) relative_positions = ( position_queries . shape [ 0 ] // 2 + relative_positions ) . clamp ( 0 , position_queries . shape [ 0 ] - 1 ) if \"c2p\" in self . mode : content_to_position_attn = torch . einsum ( \"nihxd,zxhd->nizhx\" , make_heads ( content_queries , self . n_coordinates ), position_keys , ) content_to_position_attn = gather ( content_to_position_attn , index = relative_positions . unsqueeze ( - 2 ), dim = 2 , ) . sum ( - 1 ) / math . sqrt ( size ) if no_position_mask is not None : content_to_position_attn = content_to_position_attn . masked_fill ( no_position_mask [ ... , None ], 0 ) attn_weights . append ( content_to_position_attn ) if \"p2c\" in self . mode : position_to_content_attn = torch . einsum ( \"zxhd,njhxd->nzjhx\" , position_queries , make_heads ( content_keys , self . n_coordinates ), ) position_to_content_attn = gather ( position_to_content_attn , index = relative_positions . unsqueeze ( - 2 ), dim = 1 , ) . sum ( - 1 ) / math . sqrt ( size ) if no_position_mask is not None : position_to_content_attn = position_to_content_attn . masked_fill ( no_position_mask [ ... , None ], 0 ) attn_weights . append ( position_to_content_attn ) attn = attn + sum ( attn_weights ) / math . sqrt ( len ( attn_weights )) if hasattr ( self , \"bias\" ): attn = attn + self . bias if content_values is not None : if mask . ndim == 2 : mask = mask [:, None , :, None ] if mask . ndim == 3 : mask = mask [:, :, :, None ] weights = self . dropout ( attn [ ... , self . n_additional_heads :] . masked_fill ( ~ mask , IMPOSSIBLE ) . softmax ( - 2 ) ) pooled = torch . einsum ( \"nijh,njhd->nihd\" , weights , content_values ) pooled = pooled . reshape ( * pooled . shape [: - 2 ], - 1 ) return pooled , attn return attn","title":"relative_attention"},{"location":"reference/layers/relative_attention/#edspdflayersrelative_attention","text":"","title":"edspdf.layers.relative_attention"},{"location":"reference/layers/relative_attention/#edspdf.layers.relative_attention.RelativeAttention","text":"Bases: torch . nn . Module A self/cross-attention layer that takes relative position of elements into account to compute the attention weights. When running a relative attention layer, key and queries are represented using content and position embeddings, where position embeddings are retrieved using the relative position of keys relative to queries Source code in edspdf/layers/relative_attention.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 @registry . factory . register ( \"relative-attention\" ) class RelativeAttention ( torch . nn . Module ): \"\"\" A self/cross-attention layer that takes relative position of elements into account to compute the attention weights. When running a relative attention layer, key and queries are represented using content and position embeddings, where position embeddings are retrieved using the relative position of keys relative to queries \"\"\" def __init__ ( self , size : int , n_heads : int , query_size : Optional [ int ] = None , key_size : Optional [ int ] = None , value_size : Optional [ int ] = None , head_size : Optional [ int ] = None , position_embedding : Optional [ Union [ FloatTensor , Parameter ]] = None , dropout_p : float = 0.1 , same_key_query_proj : bool = False , same_positional_key_query_proj : bool = False , n_coordinates : int = 1 , head_bias : bool = True , do_pooling : bool = True , mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"p2c\" , \"c2p\" ), n_additional_heads : int = 0 , ): \"\"\" Parameters ---------- size: int The size of the output embeddings Also serves as default if query_size, pos_size, or key_size is None n_heads: int The number of attention heads query_size: Optional[int] The size of the query embeddings. key_size: Optional[int] The size of the key embeddings. value_size: Optional[int] The size of the value embeddings head_size: Optional[int] The size of each query / key / value chunk used in the attention dot product Default: `key_size / n_heads` position_embedding: Optional[torch.FloatTensor] The position embedding used as key and query embeddings dropout_p: float Dropout probability applied on the attention weights Default: 0.1 same_key_query_proj: bool Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False same_positional_key_query_proj Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False n_coordinates: int The number of positional coordinates For instance, text is 1D so 1 coordinate, images are 2D so 2 coordinates ... Default: 1 head_bias: bool Whether to learn a bias term to add to the attention logits This is only useful if you plan to use the attention logits for subsequent operations, since attention weights are unaffected by bias terms. do_pooling: bool Whether to compute the output embedding. If you only plan to use attention logits, you should disable this parameter. Default: True mode: Sequence[RelativeAttentionMode] Whether to compute content to content (c2c), content to position (c2p) or position to content (p2c) attention terms. Setting `mode=('c2c\")` disable relative position attention terms: this is the standard attention layer. To get a better intuition about these different types of attention, here is a formulation as fictitious search samples from a word in a (1D) text: \u2014 content-content : \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words whose content contains information about temporality\" \u2014 content-position: \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words that are 3 positions after of me\" \u2014 position-content : \"regardless of my content, I will attend to the word one position after from me if it contains information about temporality, two words after me if it contains information about location, etc.\" n_additional_heads: int The number of additional head logits to compute. Those are not used to compute output embeddings, but may be useful in subsequent operation. Default: 0 \"\"\" super () . __init__ () if query_size is None : query_size = size if key_size is None : key_size = size if value_size is None : value_size = key_size if head_size is None and key_size is not None : assert key_size % n_heads == 0 head_size = key_size // n_heads value_head_size = None if do_pooling and size is not None : assert size % n_heads == 0 value_head_size = size // n_heads self . n_coordinates = n_coordinates self . n_heads = n_heads + n_additional_heads self . n_additional_heads = n_additional_heads self . mode = mode n_query_heads = n_heads + n_additional_heads self . content_key_proj = torch . nn . Linear ( key_size , n_query_heads * head_size ) if isinstance ( position_embedding , torch . nn . Parameter ): self . position_embedding = position_embedding else : self . register_buffer ( \"position_embedding\" , position_embedding ) if same_key_query_proj : self . content_query_proj = self . content_key_proj else : self . content_query_proj = torch . nn . Linear ( query_size , n_query_heads * head_size , ) if do_pooling : self . content_value_proj = torch . nn . Linear ( value_size , value_head_size * n_heads ) pos_size = self . position_embedding . shape [ - 1 ] self . position_key_proj = GroupedLinear ( pos_size // n_coordinates , head_size * n_query_heads // n_coordinates , n_groups = n_coordinates , ) if same_key_query_proj or same_positional_key_query_proj : self . position_query_proj = self . position_key_proj else : self . position_query_proj = GroupedLinear ( pos_size // n_coordinates , head_size * n_query_heads // n_coordinates , n_groups = n_coordinates , ) self . dropout = torch . nn . Dropout ( dropout_p ) if head_bias : self . bias = torch . nn . Parameter ( torch . zeros ( n_query_heads )) self . output_size = size def forward ( self , content_queries : torch . FloatTensor , content_keys : Optional [ torch . FloatTensor ] = None , content_values : Optional [ torch . FloatTensor ] = None , mask : Optional [ torch . BoolTensor ] = None , relative_positions : Optional [ torch . LongTensor ] = None , no_position_mask : Optional [ torch . BoolTensor ] = None , base_attn : Optional [ torch . FloatTensor ] = None , ) -> Union [ Tuple [ torch . FloatTensor , torch . FloatTensor ], torch . FloatTensor ]: \"\"\" Forward pass of the RelativeAttention layer. Parameters ---------- content_queries: torch.FloatTensor The content query embedding to use in the attention computation Shape: `n_samples * n_queries * query_size` content_keys: Optional[torch.FloatTensor] The content key embedding to use in the attention computation. If None, defaults to the `content_queries` Shape: `n_samples * n_keys * query_size` content_values: Optional[torch.FloatTensor] The content values embedding to use in the final pooling computation. If None, pooling won't be performed. Shape: `n_samples * n_keys * query_size` mask: Optional[torch.BoolTensor] The content key embedding to use in the attention computation. If None, defaults to the `content_queries` Shape: either - `n_samples * n_keys` - `n_samples * n_queries * n_keys` - `n_samples * n_queries * n_keys * n_heads` relative_positions: Optional[torch.LongTensor] The relative position of keys relative to queries If None, positional attention terms won't be computed. Shape: `n_samples * n_queries * n_keys * n_coordinates` no_position_mask: Optional[torch.BoolTensor] Key / query pairs for which the position attention terms should be disabled. Shape: `n_samples * n_queries * n_keys` base_attn: Optional[torch.FloatTensor] Attention logits to add to the computed attention logits Shape: `n_samples * n_queries * n_keys * n_heads` Returns ------- Union[Tuple[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor] - the output contextualized embeddings (only if content_values is not None and the `do_pooling` attribute is set to True) Shape: n_sample * n_keys * `size` - the attention logits Shape: n_sample * n_keys * n_queries * (n_heads + n_additional_heads) \"\"\" if content_keys is None : content_keys = content_queries attn = ( torch . zeros ( content_queries . shape [ 0 ], content_queries . shape [ 1 ], content_keys . shape [ 1 ], self . n_heads , device = content_queries . device , ) if base_attn is None else base_attn ) attn_weights = [] if 0 not in content_queries . shape and 0 not in content_keys . shape : content_keys = make_heads ( self . content_key_proj ( self . dropout ( content_keys )), self . n_heads ) content_queries = make_heads ( self . content_query_proj ( self . dropout ( content_queries )), self . n_heads ) if content_values is not None : content_values = make_heads ( self . content_value_proj ( self . dropout ( content_values )), self . n_heads - self . n_additional_heads , ) size = content_queries . shape [ - 1 ] if \"c2c\" in self . mode : content_to_content_attn = torch . einsum ( \"nihd,njhd->nijh\" , content_queries , content_keys ) / math . sqrt ( size ) attn_weights . append ( content_to_content_attn ) if relative_positions is not None and ( \"p2c\" in self . mode or \"c2p\" in self . mode ): position_keys = make_heads ( self . position_key_proj ( self . dropout ( self . position_embedding )), ( self . n_coordinates , self . n_heads ), ) position_queries = make_heads ( self . position_query_proj ( self . dropout ( self . position_embedding )), ( self . n_coordinates , self . n_heads ), ) relative_positions = ( position_queries . shape [ 0 ] // 2 + relative_positions ) . clamp ( 0 , position_queries . shape [ 0 ] - 1 ) if \"c2p\" in self . mode : content_to_position_attn = torch . einsum ( \"nihxd,zxhd->nizhx\" , make_heads ( content_queries , self . n_coordinates ), position_keys , ) content_to_position_attn = gather ( content_to_position_attn , index = relative_positions . unsqueeze ( - 2 ), dim = 2 , ) . sum ( - 1 ) / math . sqrt ( size ) if no_position_mask is not None : content_to_position_attn = content_to_position_attn . masked_fill ( no_position_mask [ ... , None ], 0 ) attn_weights . append ( content_to_position_attn ) if \"p2c\" in self . mode : position_to_content_attn = torch . einsum ( \"zxhd,njhxd->nzjhx\" , position_queries , make_heads ( content_keys , self . n_coordinates ), ) position_to_content_attn = gather ( position_to_content_attn , index = relative_positions . unsqueeze ( - 2 ), dim = 1 , ) . sum ( - 1 ) / math . sqrt ( size ) if no_position_mask is not None : position_to_content_attn = position_to_content_attn . masked_fill ( no_position_mask [ ... , None ], 0 ) attn_weights . append ( position_to_content_attn ) attn = attn + sum ( attn_weights ) / math . sqrt ( len ( attn_weights )) if hasattr ( self , \"bias\" ): attn = attn + self . bias if content_values is not None : if mask . ndim == 2 : mask = mask [:, None , :, None ] if mask . ndim == 3 : mask = mask [:, :, :, None ] weights = self . dropout ( attn [ ... , self . n_additional_heads :] . masked_fill ( ~ mask , IMPOSSIBLE ) . softmax ( - 2 ) ) pooled = torch . einsum ( \"nijh,njhd->nihd\" , weights , content_values ) pooled = pooled . reshape ( * pooled . shape [: - 2 ], - 1 ) return pooled , attn return attn","title":"RelativeAttention"},{"location":"reference/layers/relative_attention/#edspdf.layers.relative_attention.RelativeAttention.__init__","text":"PARAMETER DESCRIPTION size The size of the output embeddings Also serves as default if query_size, pos_size, or key_size is None TYPE: int n_heads The number of attention heads TYPE: int query_size The size of the query embeddings. TYPE: Optional [ int ] DEFAULT: None key_size The size of the key embeddings. TYPE: Optional [ int ] DEFAULT: None value_size The size of the value embeddings TYPE: Optional [ int ] DEFAULT: None head_size The size of each query / key / value chunk used in the attention dot product Default: key_size / n_heads TYPE: Optional [ int ] DEFAULT: None position_embedding The position embedding used as key and query embeddings TYPE: Optional [ Union [ FloatTensor , Parameter ]] DEFAULT: None dropout_p Dropout probability applied on the attention weights Default: 0.1 TYPE: float DEFAULT: 0.1 same_key_query_proj Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False TYPE: bool DEFAULT: False same_positional_key_query_proj Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False TYPE: bool DEFAULT: False n_coordinates The number of positional coordinates For instance, text is 1D so 1 coordinate, images are 2D so 2 coordinates ... Default: 1 TYPE: int DEFAULT: 1 head_bias Whether to learn a bias term to add to the attention logits This is only useful if you plan to use the attention logits for subsequent operations, since attention weights are unaffected by bias terms. TYPE: bool DEFAULT: True do_pooling Whether to compute the output embedding. If you only plan to use attention logits, you should disable this parameter. Default: True TYPE: bool DEFAULT: True mode Whether to compute content to content (c2c), content to position (c2p) or position to content (p2c) attention terms. Setting mode=('c2c\") disable relative position attention terms: this is the standard attention layer. To get a better intuition about these different types of attention, here is a formulation as fictitious search samples from a word in a (1D) text: \u2014 content-content : \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words whose content contains information about temporality\" \u2014 content-position: \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words that are 3 positions after of me\" \u2014 position-content : \"regardless of my content, I will attend to the word one position after from me if it contains information about temporality, two words after me if it contains information about location, etc.\" TYPE: Sequence [ RelativeAttentionMode ] DEFAULT: ('c2c', 'p2c', 'c2p') n_additional_heads The number of additional head logits to compute. Those are not used to compute output embeddings, but may be useful in subsequent operation. Default: 0 TYPE: int DEFAULT: 0 Source code in edspdf/layers/relative_attention.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def __init__ ( self , size : int , n_heads : int , query_size : Optional [ int ] = None , key_size : Optional [ int ] = None , value_size : Optional [ int ] = None , head_size : Optional [ int ] = None , position_embedding : Optional [ Union [ FloatTensor , Parameter ]] = None , dropout_p : float = 0.1 , same_key_query_proj : bool = False , same_positional_key_query_proj : bool = False , n_coordinates : int = 1 , head_bias : bool = True , do_pooling : bool = True , mode : Sequence [ RelativeAttentionMode ] = ( \"c2c\" , \"p2c\" , \"c2p\" ), n_additional_heads : int = 0 , ): \"\"\" Parameters ---------- size: int The size of the output embeddings Also serves as default if query_size, pos_size, or key_size is None n_heads: int The number of attention heads query_size: Optional[int] The size of the query embeddings. key_size: Optional[int] The size of the key embeddings. value_size: Optional[int] The size of the value embeddings head_size: Optional[int] The size of each query / key / value chunk used in the attention dot product Default: `key_size / n_heads` position_embedding: Optional[torch.FloatTensor] The position embedding used as key and query embeddings dropout_p: float Dropout probability applied on the attention weights Default: 0.1 same_key_query_proj: bool Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False same_positional_key_query_proj Whether to use the same projection operator for content key and queries when computing the pre-attention key and query embedding chunks Default: False n_coordinates: int The number of positional coordinates For instance, text is 1D so 1 coordinate, images are 2D so 2 coordinates ... Default: 1 head_bias: bool Whether to learn a bias term to add to the attention logits This is only useful if you plan to use the attention logits for subsequent operations, since attention weights are unaffected by bias terms. do_pooling: bool Whether to compute the output embedding. If you only plan to use attention logits, you should disable this parameter. Default: True mode: Sequence[RelativeAttentionMode] Whether to compute content to content (c2c), content to position (c2p) or position to content (p2c) attention terms. Setting `mode=('c2c\")` disable relative position attention terms: this is the standard attention layer. To get a better intuition about these different types of attention, here is a formulation as fictitious search samples from a word in a (1D) text: \u2014 content-content : \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words whose content contains information about temporality\" \u2014 content-position: \"my content is \u2019ultrasound\u2019 so I\u2019m looking for other words that are 3 positions after of me\" \u2014 position-content : \"regardless of my content, I will attend to the word one position after from me if it contains information about temporality, two words after me if it contains information about location, etc.\" n_additional_heads: int The number of additional head logits to compute. Those are not used to compute output embeddings, but may be useful in subsequent operation. Default: 0 \"\"\" super () . __init__ () if query_size is None : query_size = size if key_size is None : key_size = size if value_size is None : value_size = key_size if head_size is None and key_size is not None : assert key_size % n_heads == 0 head_size = key_size // n_heads value_head_size = None if do_pooling and size is not None : assert size % n_heads == 0 value_head_size = size // n_heads self . n_coordinates = n_coordinates self . n_heads = n_heads + n_additional_heads self . n_additional_heads = n_additional_heads self . mode = mode n_query_heads = n_heads + n_additional_heads self . content_key_proj = torch . nn . Linear ( key_size , n_query_heads * head_size ) if isinstance ( position_embedding , torch . nn . Parameter ): self . position_embedding = position_embedding else : self . register_buffer ( \"position_embedding\" , position_embedding ) if same_key_query_proj : self . content_query_proj = self . content_key_proj else : self . content_query_proj = torch . nn . Linear ( query_size , n_query_heads * head_size , ) if do_pooling : self . content_value_proj = torch . nn . Linear ( value_size , value_head_size * n_heads ) pos_size = self . position_embedding . shape [ - 1 ] self . position_key_proj = GroupedLinear ( pos_size // n_coordinates , head_size * n_query_heads // n_coordinates , n_groups = n_coordinates , ) if same_key_query_proj or same_positional_key_query_proj : self . position_query_proj = self . position_key_proj else : self . position_query_proj = GroupedLinear ( pos_size // n_coordinates , head_size * n_query_heads // n_coordinates , n_groups = n_coordinates , ) self . dropout = torch . nn . Dropout ( dropout_p ) if head_bias : self . bias = torch . nn . Parameter ( torch . zeros ( n_query_heads )) self . output_size = size","title":"__init__()"},{"location":"reference/layers/relative_attention/#edspdf.layers.relative_attention.RelativeAttention.forward","text":"Forward pass of the RelativeAttention layer. PARAMETER DESCRIPTION content_queries The content query embedding to use in the attention computation Shape: n_samples * n_queries * query_size TYPE: torch . FloatTensor content_keys The content key embedding to use in the attention computation. If None, defaults to the content_queries Shape: n_samples * n_keys * query_size TYPE: Optional [ torch . FloatTensor ] DEFAULT: None content_values The content values embedding to use in the final pooling computation. If None, pooling won't be performed. Shape: n_samples * n_keys * query_size TYPE: Optional [ torch . FloatTensor ] DEFAULT: None mask The content key embedding to use in the attention computation. If None, defaults to the content_queries Shape: either - n_samples * n_keys - n_samples * n_queries * n_keys - n_samples * n_queries * n_keys * n_heads TYPE: Optional [ torch . BoolTensor ] DEFAULT: None relative_positions The relative position of keys relative to queries If None, positional attention terms won't be computed. Shape: n_samples * n_queries * n_keys * n_coordinates TYPE: Optional [ torch . LongTensor ] DEFAULT: None no_position_mask Key / query pairs for which the position attention terms should be disabled. Shape: n_samples * n_queries * n_keys TYPE: Optional [ torch . BoolTensor ] DEFAULT: None base_attn Attention logits to add to the computed attention logits Shape: n_samples * n_queries * n_keys * n_heads TYPE: Optional [ torch . FloatTensor ] DEFAULT: None RETURNS DESCRIPTION Union [ Tuple [ torch . FloatTensor , torch . FloatTensor ], torch . FloatTensor ] the output contextualized embeddings (only if content_values is not None and the do_pooling attribute is set to True) Shape: n_sample * n_keys * size the attention logits Shape: n_sample * n_keys * n_queries * (n_heads + n_additional_heads) Source code in edspdf/layers/relative_attention.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 def forward ( self , content_queries : torch . FloatTensor , content_keys : Optional [ torch . FloatTensor ] = None , content_values : Optional [ torch . FloatTensor ] = None , mask : Optional [ torch . BoolTensor ] = None , relative_positions : Optional [ torch . LongTensor ] = None , no_position_mask : Optional [ torch . BoolTensor ] = None , base_attn : Optional [ torch . FloatTensor ] = None , ) -> Union [ Tuple [ torch . FloatTensor , torch . FloatTensor ], torch . FloatTensor ]: \"\"\" Forward pass of the RelativeAttention layer. Parameters ---------- content_queries: torch.FloatTensor The content query embedding to use in the attention computation Shape: `n_samples * n_queries * query_size` content_keys: Optional[torch.FloatTensor] The content key embedding to use in the attention computation. If None, defaults to the `content_queries` Shape: `n_samples * n_keys * query_size` content_values: Optional[torch.FloatTensor] The content values embedding to use in the final pooling computation. If None, pooling won't be performed. Shape: `n_samples * n_keys * query_size` mask: Optional[torch.BoolTensor] The content key embedding to use in the attention computation. If None, defaults to the `content_queries` Shape: either - `n_samples * n_keys` - `n_samples * n_queries * n_keys` - `n_samples * n_queries * n_keys * n_heads` relative_positions: Optional[torch.LongTensor] The relative position of keys relative to queries If None, positional attention terms won't be computed. Shape: `n_samples * n_queries * n_keys * n_coordinates` no_position_mask: Optional[torch.BoolTensor] Key / query pairs for which the position attention terms should be disabled. Shape: `n_samples * n_queries * n_keys` base_attn: Optional[torch.FloatTensor] Attention logits to add to the computed attention logits Shape: `n_samples * n_queries * n_keys * n_heads` Returns ------- Union[Tuple[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor] - the output contextualized embeddings (only if content_values is not None and the `do_pooling` attribute is set to True) Shape: n_sample * n_keys * `size` - the attention logits Shape: n_sample * n_keys * n_queries * (n_heads + n_additional_heads) \"\"\" if content_keys is None : content_keys = content_queries attn = ( torch . zeros ( content_queries . shape [ 0 ], content_queries . shape [ 1 ], content_keys . shape [ 1 ], self . n_heads , device = content_queries . device , ) if base_attn is None else base_attn ) attn_weights = [] if 0 not in content_queries . shape and 0 not in content_keys . shape : content_keys = make_heads ( self . content_key_proj ( self . dropout ( content_keys )), self . n_heads ) content_queries = make_heads ( self . content_query_proj ( self . dropout ( content_queries )), self . n_heads ) if content_values is not None : content_values = make_heads ( self . content_value_proj ( self . dropout ( content_values )), self . n_heads - self . n_additional_heads , ) size = content_queries . shape [ - 1 ] if \"c2c\" in self . mode : content_to_content_attn = torch . einsum ( \"nihd,njhd->nijh\" , content_queries , content_keys ) / math . sqrt ( size ) attn_weights . append ( content_to_content_attn ) if relative_positions is not None and ( \"p2c\" in self . mode or \"c2p\" in self . mode ): position_keys = make_heads ( self . position_key_proj ( self . dropout ( self . position_embedding )), ( self . n_coordinates , self . n_heads ), ) position_queries = make_heads ( self . position_query_proj ( self . dropout ( self . position_embedding )), ( self . n_coordinates , self . n_heads ), ) relative_positions = ( position_queries . shape [ 0 ] // 2 + relative_positions ) . clamp ( 0 , position_queries . shape [ 0 ] - 1 ) if \"c2p\" in self . mode : content_to_position_attn = torch . einsum ( \"nihxd,zxhd->nizhx\" , make_heads ( content_queries , self . n_coordinates ), position_keys , ) content_to_position_attn = gather ( content_to_position_attn , index = relative_positions . unsqueeze ( - 2 ), dim = 2 , ) . sum ( - 1 ) / math . sqrt ( size ) if no_position_mask is not None : content_to_position_attn = content_to_position_attn . masked_fill ( no_position_mask [ ... , None ], 0 ) attn_weights . append ( content_to_position_attn ) if \"p2c\" in self . mode : position_to_content_attn = torch . einsum ( \"zxhd,njhxd->nzjhx\" , position_queries , make_heads ( content_keys , self . n_coordinates ), ) position_to_content_attn = gather ( position_to_content_attn , index = relative_positions . unsqueeze ( - 2 ), dim = 1 , ) . sum ( - 1 ) / math . sqrt ( size ) if no_position_mask is not None : position_to_content_attn = position_to_content_attn . masked_fill ( no_position_mask [ ... , None ], 0 ) attn_weights . append ( position_to_content_attn ) attn = attn + sum ( attn_weights ) / math . sqrt ( len ( attn_weights )) if hasattr ( self , \"bias\" ): attn = attn + self . bias if content_values is not None : if mask . ndim == 2 : mask = mask [:, None , :, None ] if mask . ndim == 3 : mask = mask [:, :, :, None ] weights = self . dropout ( attn [ ... , self . n_additional_heads :] . masked_fill ( ~ mask , IMPOSSIBLE ) . softmax ( - 2 ) ) pooled = torch . einsum ( \"nijh,njhd->nihd\" , weights , content_values ) pooled = pooled . reshape ( * pooled . shape [: - 2 ], - 1 ) return pooled , attn return attn","title":"forward()"},{"location":"reference/layers/sinusoidal_embedding/","text":"edspdf.layers.sinusoidal_embedding SinusoidalEmbedding Bases: torch . nn . Module A position embedding lookup table that stores embeddings for a fixed number of positions. The value of each of the embedding_dim channels of the generated embedding is generated according to a trigonometric function (sin for even channels, cos for odd channels). The frequency of the signal in each pair of channels varies according to the temperature parameter. Any input position above the maximum value num_embeddings will be capped to num_embeddings - 1 Source code in edspdf/layers/sinusoidal_embedding.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 @registry . factory . register ( \"sinusoidal-embedding\" ) class SinusoidalEmbedding ( torch . nn . Module ): \"\"\" A position embedding lookup table that stores embeddings for a fixed number of positions. The value of each of the `embedding_dim` channels of the generated embedding is generated according to a trigonometric function (sin for even channels, cos for odd channels). The frequency of the signal in each pair of channels varies according to the temperature parameter. Any input position above the maximum value `num_embeddings` will be capped to `num_embeddings - 1` \"\"\" def __init__ ( self , num_embeddings : int , embedding_dim : int , temperature : float = 10000.0 , ): \"\"\" Parameters ---------- num_embeddings: int The maximum number of position embeddings store in this table embedding_dim: int The embedding size temperature: float The temperature controls the range of frequencies used by each channel of the embedding \"\"\" super () . __init__ () self . embedding_dim = embedding_dim self . num_embeddings = num_embeddings self . temperature = temperature weight = torch . zeros ( self . num_embeddings , self . embedding_dim ) position = torch . arange ( 0 , self . num_embeddings , dtype = torch . float ) . unsqueeze ( 1 ) div_term = torch . exp ( torch . arange ( 0 , self . embedding_dim , 2 ) . float () * ( - math . log ( self . temperature ) / self . embedding_dim ) ) weight [:, 0 :: 2 ] = torch . sin ( position * div_term ) weight [:, 1 :: 2 ] = torch . cos ( position * div_term ) self . register_buffer ( \"weight\" , weight ) def extra_repr ( self ) -> str : return f \" { self . num_embeddings } , { self . embedding_dim } \" def forward ( self , indices : torch . LongTensor ): \"\"\" Forward pass of the SinusoidalEmbedding module Parameters ---------- indices: torch.LongTensor Shape: ... Returns ------- torch.FloatTensor Shape: `... * embedding_dim` \"\"\" res = F . embedding ( indices . clamp ( 0 , len ( self . weight ) - 1 ), self . weight ) return res __init__ ( num_embeddings , embedding_dim , temperature = 10000.0 ) PARAMETER DESCRIPTION num_embeddings The maximum number of position embeddings store in this table TYPE: int embedding_dim The embedding size TYPE: int temperature The temperature controls the range of frequencies used by each channel of the embedding TYPE: float DEFAULT: 10000.0 Source code in edspdf/layers/sinusoidal_embedding.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , num_embeddings : int , embedding_dim : int , temperature : float = 10000.0 , ): \"\"\" Parameters ---------- num_embeddings: int The maximum number of position embeddings store in this table embedding_dim: int The embedding size temperature: float The temperature controls the range of frequencies used by each channel of the embedding \"\"\" super () . __init__ () self . embedding_dim = embedding_dim self . num_embeddings = num_embeddings self . temperature = temperature weight = torch . zeros ( self . num_embeddings , self . embedding_dim ) position = torch . arange ( 0 , self . num_embeddings , dtype = torch . float ) . unsqueeze ( 1 ) div_term = torch . exp ( torch . arange ( 0 , self . embedding_dim , 2 ) . float () * ( - math . log ( self . temperature ) / self . embedding_dim ) ) weight [:, 0 :: 2 ] = torch . sin ( position * div_term ) weight [:, 1 :: 2 ] = torch . cos ( position * div_term ) self . register_buffer ( \"weight\" , weight ) forward ( indices ) Forward pass of the SinusoidalEmbedding module PARAMETER DESCRIPTION indices Shape: ... TYPE: torch . LongTensor RETURNS DESCRIPTION torch . FloatTensor Shape Source code in edspdf/layers/sinusoidal_embedding.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def forward ( self , indices : torch . LongTensor ): \"\"\" Forward pass of the SinusoidalEmbedding module Parameters ---------- indices: torch.LongTensor Shape: ... Returns ------- torch.FloatTensor Shape: `... * embedding_dim` \"\"\" res = F . embedding ( indices . clamp ( 0 , len ( self . weight ) - 1 ), self . weight ) return res","title":"sinusoidal_embedding"},{"location":"reference/layers/sinusoidal_embedding/#edspdflayerssinusoidal_embedding","text":"","title":"edspdf.layers.sinusoidal_embedding"},{"location":"reference/layers/sinusoidal_embedding/#edspdf.layers.sinusoidal_embedding.SinusoidalEmbedding","text":"Bases: torch . nn . Module A position embedding lookup table that stores embeddings for a fixed number of positions. The value of each of the embedding_dim channels of the generated embedding is generated according to a trigonometric function (sin for even channels, cos for odd channels). The frequency of the signal in each pair of channels varies according to the temperature parameter. Any input position above the maximum value num_embeddings will be capped to num_embeddings - 1 Source code in edspdf/layers/sinusoidal_embedding.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 @registry . factory . register ( \"sinusoidal-embedding\" ) class SinusoidalEmbedding ( torch . nn . Module ): \"\"\" A position embedding lookup table that stores embeddings for a fixed number of positions. The value of each of the `embedding_dim` channels of the generated embedding is generated according to a trigonometric function (sin for even channels, cos for odd channels). The frequency of the signal in each pair of channels varies according to the temperature parameter. Any input position above the maximum value `num_embeddings` will be capped to `num_embeddings - 1` \"\"\" def __init__ ( self , num_embeddings : int , embedding_dim : int , temperature : float = 10000.0 , ): \"\"\" Parameters ---------- num_embeddings: int The maximum number of position embeddings store in this table embedding_dim: int The embedding size temperature: float The temperature controls the range of frequencies used by each channel of the embedding \"\"\" super () . __init__ () self . embedding_dim = embedding_dim self . num_embeddings = num_embeddings self . temperature = temperature weight = torch . zeros ( self . num_embeddings , self . embedding_dim ) position = torch . arange ( 0 , self . num_embeddings , dtype = torch . float ) . unsqueeze ( 1 ) div_term = torch . exp ( torch . arange ( 0 , self . embedding_dim , 2 ) . float () * ( - math . log ( self . temperature ) / self . embedding_dim ) ) weight [:, 0 :: 2 ] = torch . sin ( position * div_term ) weight [:, 1 :: 2 ] = torch . cos ( position * div_term ) self . register_buffer ( \"weight\" , weight ) def extra_repr ( self ) -> str : return f \" { self . num_embeddings } , { self . embedding_dim } \" def forward ( self , indices : torch . LongTensor ): \"\"\" Forward pass of the SinusoidalEmbedding module Parameters ---------- indices: torch.LongTensor Shape: ... Returns ------- torch.FloatTensor Shape: `... * embedding_dim` \"\"\" res = F . embedding ( indices . clamp ( 0 , len ( self . weight ) - 1 ), self . weight ) return res","title":"SinusoidalEmbedding"},{"location":"reference/layers/sinusoidal_embedding/#edspdf.layers.sinusoidal_embedding.SinusoidalEmbedding.__init__","text":"PARAMETER DESCRIPTION num_embeddings The maximum number of position embeddings store in this table TYPE: int embedding_dim The embedding size TYPE: int temperature The temperature controls the range of frequencies used by each channel of the embedding TYPE: float DEFAULT: 10000.0 Source code in edspdf/layers/sinusoidal_embedding.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __init__ ( self , num_embeddings : int , embedding_dim : int , temperature : float = 10000.0 , ): \"\"\" Parameters ---------- num_embeddings: int The maximum number of position embeddings store in this table embedding_dim: int The embedding size temperature: float The temperature controls the range of frequencies used by each channel of the embedding \"\"\" super () . __init__ () self . embedding_dim = embedding_dim self . num_embeddings = num_embeddings self . temperature = temperature weight = torch . zeros ( self . num_embeddings , self . embedding_dim ) position = torch . arange ( 0 , self . num_embeddings , dtype = torch . float ) . unsqueeze ( 1 ) div_term = torch . exp ( torch . arange ( 0 , self . embedding_dim , 2 ) . float () * ( - math . log ( self . temperature ) / self . embedding_dim ) ) weight [:, 0 :: 2 ] = torch . sin ( position * div_term ) weight [:, 1 :: 2 ] = torch . cos ( position * div_term ) self . register_buffer ( \"weight\" , weight )","title":"__init__()"},{"location":"reference/layers/sinusoidal_embedding/#edspdf.layers.sinusoidal_embedding.SinusoidalEmbedding.forward","text":"Forward pass of the SinusoidalEmbedding module PARAMETER DESCRIPTION indices Shape: ... TYPE: torch . LongTensor RETURNS DESCRIPTION torch . FloatTensor Shape Source code in edspdf/layers/sinusoidal_embedding.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def forward ( self , indices : torch . LongTensor ): \"\"\" Forward pass of the SinusoidalEmbedding module Parameters ---------- indices: torch.LongTensor Shape: ... Returns ------- torch.FloatTensor Shape: `... * embedding_dim` \"\"\" res = F . embedding ( indices . clamp ( 0 , len ( self . weight ) - 1 ), self . weight ) return res","title":"forward()"},{"location":"reference/layers/vocabulary/","text":"edspdf.layers.vocabulary Vocabulary Bases: torch . nn . Module , Generic [ T ] Vocabulary layer. This is not meant to be used as torch.nn.Module but subclassing torch.nn.Module makes the instances appear when printing a model, which is nice. Source code in edspdf/layers/vocabulary.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 @registry . factory . register ( \"vocabulary\" ) class Vocabulary ( torch . nn . Module , Generic [ T ]): \"\"\" Vocabulary layer. This is not meant to be used as torch.nn.Module but subclassing torch.nn.Module makes the instances appear when printing a model, which is nice. \"\"\" def __init__ ( self , items : Sequence [ T ] = None , default : int = - 100 ): \"\"\" Parameters ---------- items: Sequence[InputT] Initial vocabulary elements if any. Specific elements such as padding and unk can be set here to enforce their index in the vocabulary. default: int Default index to use for out of vocabulary elements Defaults to -100 \"\"\" super () . __init__ () if items is None : self . indices = {} self . initialized = False else : self . indices = { v : i for i , v in enumerate ( items )} self . initialized = True self . default = default def __len__ ( self ): return len ( self . indices ) @contextlib . contextmanager def initialization ( self ): \"\"\" Enters the initialization mode. Out of vocabulary elements will be assigned an index. \"\"\" self . initialized = False yield self . initialized = True def encode ( self , item ): \"\"\" Converts an element into its vocabulary index If the layer is in its initialization mode (`with vocab.initialization(): ...`), and the element is out of vocabulary, a new index will be created and returned. Otherwise, any oov element will be encoded with the `default` index. Parameters ---------- item: InputT Returns ------- int \"\"\" if self . initialized : return self . indices . get ( item , self . default ) # .setdefault(item, len(self.indices)) else : return self . indices . setdefault ( item , len ( self . indices ) ) # .setdefault(item, len(self.indices)) def decode ( self , idx ): \"\"\" Converts an index into its original value Parameters ---------- idx: int Returns ------- InputT \"\"\" return list ( self . indices . keys ())[ idx ] if idx >= 0 else None def extra_repr ( self ): return \"n= {} \" . format ( len ( self . indices )) __init__ ( items = None , default =- 100 ) PARAMETER DESCRIPTION items Initial vocabulary elements if any. Specific elements such as padding and unk can be set here to enforce their index in the vocabulary. TYPE: Sequence [ T ] DEFAULT: None default Default index to use for out of vocabulary elements Defaults to -100 TYPE: int DEFAULT: -100 Source code in edspdf/layers/vocabulary.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , items : Sequence [ T ] = None , default : int = - 100 ): \"\"\" Parameters ---------- items: Sequence[InputT] Initial vocabulary elements if any. Specific elements such as padding and unk can be set here to enforce their index in the vocabulary. default: int Default index to use for out of vocabulary elements Defaults to -100 \"\"\" super () . __init__ () if items is None : self . indices = {} self . initialized = False else : self . indices = { v : i for i , v in enumerate ( items )} self . initialized = True self . default = default initialization () Enters the initialization mode. Out of vocabulary elements will be assigned an index. Source code in edspdf/layers/vocabulary.py 43 44 45 46 47 48 49 50 51 @contextlib . contextmanager def initialization ( self ): \"\"\" Enters the initialization mode. Out of vocabulary elements will be assigned an index. \"\"\" self . initialized = False yield self . initialized = True encode ( item ) Converts an element into its vocabulary index If the layer is in its initialization mode ( with vocab.initialization(): ... ), and the element is out of vocabulary, a new index will be created and returned. Otherwise, any oov element will be encoded with the default index. PARAMETER DESCRIPTION item RETURNS DESCRIPTION int Source code in edspdf/layers/vocabulary.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def encode ( self , item ): \"\"\" Converts an element into its vocabulary index If the layer is in its initialization mode (`with vocab.initialization(): ...`), and the element is out of vocabulary, a new index will be created and returned. Otherwise, any oov element will be encoded with the `default` index. Parameters ---------- item: InputT Returns ------- int \"\"\" if self . initialized : return self . indices . get ( item , self . default ) # .setdefault(item, len(self.indices)) else : return self . indices . setdefault ( item , len ( self . indices ) ) # .setdefault(item, len(self.indices)) decode ( idx ) Converts an index into its original value PARAMETER DESCRIPTION idx RETURNS DESCRIPTION InputT Source code in edspdf/layers/vocabulary.py 77 78 79 80 81 82 83 84 85 86 87 88 89 def decode ( self , idx ): \"\"\" Converts an index into its original value Parameters ---------- idx: int Returns ------- InputT \"\"\" return list ( self . indices . keys ())[ idx ] if idx >= 0 else None","title":"vocabulary"},{"location":"reference/layers/vocabulary/#edspdflayersvocabulary","text":"","title":"edspdf.layers.vocabulary"},{"location":"reference/layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary","text":"Bases: torch . nn . Module , Generic [ T ] Vocabulary layer. This is not meant to be used as torch.nn.Module but subclassing torch.nn.Module makes the instances appear when printing a model, which is nice. Source code in edspdf/layers/vocabulary.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 @registry . factory . register ( \"vocabulary\" ) class Vocabulary ( torch . nn . Module , Generic [ T ]): \"\"\" Vocabulary layer. This is not meant to be used as torch.nn.Module but subclassing torch.nn.Module makes the instances appear when printing a model, which is nice. \"\"\" def __init__ ( self , items : Sequence [ T ] = None , default : int = - 100 ): \"\"\" Parameters ---------- items: Sequence[InputT] Initial vocabulary elements if any. Specific elements such as padding and unk can be set here to enforce their index in the vocabulary. default: int Default index to use for out of vocabulary elements Defaults to -100 \"\"\" super () . __init__ () if items is None : self . indices = {} self . initialized = False else : self . indices = { v : i for i , v in enumerate ( items )} self . initialized = True self . default = default def __len__ ( self ): return len ( self . indices ) @contextlib . contextmanager def initialization ( self ): \"\"\" Enters the initialization mode. Out of vocabulary elements will be assigned an index. \"\"\" self . initialized = False yield self . initialized = True def encode ( self , item ): \"\"\" Converts an element into its vocabulary index If the layer is in its initialization mode (`with vocab.initialization(): ...`), and the element is out of vocabulary, a new index will be created and returned. Otherwise, any oov element will be encoded with the `default` index. Parameters ---------- item: InputT Returns ------- int \"\"\" if self . initialized : return self . indices . get ( item , self . default ) # .setdefault(item, len(self.indices)) else : return self . indices . setdefault ( item , len ( self . indices ) ) # .setdefault(item, len(self.indices)) def decode ( self , idx ): \"\"\" Converts an index into its original value Parameters ---------- idx: int Returns ------- InputT \"\"\" return list ( self . indices . keys ())[ idx ] if idx >= 0 else None def extra_repr ( self ): return \"n= {} \" . format ( len ( self . indices ))","title":"Vocabulary"},{"location":"reference/layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary.__init__","text":"PARAMETER DESCRIPTION items Initial vocabulary elements if any. Specific elements such as padding and unk can be set here to enforce their index in the vocabulary. TYPE: Sequence [ T ] DEFAULT: None default Default index to use for out of vocabulary elements Defaults to -100 TYPE: int DEFAULT: -100 Source code in edspdf/layers/vocabulary.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , items : Sequence [ T ] = None , default : int = - 100 ): \"\"\" Parameters ---------- items: Sequence[InputT] Initial vocabulary elements if any. Specific elements such as padding and unk can be set here to enforce their index in the vocabulary. default: int Default index to use for out of vocabulary elements Defaults to -100 \"\"\" super () . __init__ () if items is None : self . indices = {} self . initialized = False else : self . indices = { v : i for i , v in enumerate ( items )} self . initialized = True self . default = default","title":"__init__()"},{"location":"reference/layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary.initialization","text":"Enters the initialization mode. Out of vocabulary elements will be assigned an index. Source code in edspdf/layers/vocabulary.py 43 44 45 46 47 48 49 50 51 @contextlib . contextmanager def initialization ( self ): \"\"\" Enters the initialization mode. Out of vocabulary elements will be assigned an index. \"\"\" self . initialized = False yield self . initialized = True","title":"initialization()"},{"location":"reference/layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary.encode","text":"Converts an element into its vocabulary index If the layer is in its initialization mode ( with vocab.initialization(): ... ), and the element is out of vocabulary, a new index will be created and returned. Otherwise, any oov element will be encoded with the default index. PARAMETER DESCRIPTION item RETURNS DESCRIPTION int Source code in edspdf/layers/vocabulary.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def encode ( self , item ): \"\"\" Converts an element into its vocabulary index If the layer is in its initialization mode (`with vocab.initialization(): ...`), and the element is out of vocabulary, a new index will be created and returned. Otherwise, any oov element will be encoded with the `default` index. Parameters ---------- item: InputT Returns ------- int \"\"\" if self . initialized : return self . indices . get ( item , self . default ) # .setdefault(item, len(self.indices)) else : return self . indices . setdefault ( item , len ( self . indices ) ) # .setdefault(item, len(self.indices))","title":"encode()"},{"location":"reference/layers/vocabulary/#edspdf.layers.vocabulary.Vocabulary.decode","text":"Converts an index into its original value PARAMETER DESCRIPTION idx RETURNS DESCRIPTION InputT Source code in edspdf/layers/vocabulary.py 77 78 79 80 81 82 83 84 85 86 87 88 89 def decode ( self , idx ): \"\"\" Converts an index into its original value Parameters ---------- idx: int Returns ------- InputT \"\"\" return list ( self . indices . keys ())[ idx ] if idx >= 0 else None","title":"decode()"},{"location":"reference/models/","text":"edspdf.models","title":"`edspdf.models`"},{"location":"reference/models/#edspdfmodels","text":"","title":"edspdf.models"},{"location":"reference/models/box/","text":"edspdf.models.box","title":"box"},{"location":"reference/models/box/#edspdfmodelsbox","text":"","title":"edspdf.models.box"},{"location":"reference/models/doc/","text":"edspdf.models.doc","title":"doc"},{"location":"reference/models/doc/#edspdfmodelsdoc","text":"","title":"edspdf.models.doc"},{"location":"reference/models/style/","text":"edspdf.models.style BaseStyle Bases: BaseModel Model acting as an abstraction for a style. Source code in edspdf/models/style.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class BaseStyle ( BaseModel ): \"\"\" Model acting as an abstraction for a style. \"\"\" # font: str # style: str # size: float italic : bool bold : bool fontname : Optional [ str ] = None dict = attrs . asdict Style Bases: BaseStyle Model acting as an abstraction for a style. Source code in edspdf/models/style.py 28 29 30 31 class Style ( BaseStyle ): \"\"\" Model acting as an abstraction for a style. \"\"\" StyledText Bases: BaseModel Abstraction of a word, containing the style and the text. Source code in edspdf/models/style.py 40 41 42 43 44 45 46 47 48 class StyledText ( BaseModel ): \"\"\" Abstraction of a word, containing the style and the text. \"\"\" text : str style : Style dict = attrs . asdict","title":"style"},{"location":"reference/models/style/#edspdfmodelsstyle","text":"","title":"edspdf.models.style"},{"location":"reference/models/style/#edspdf.models.style.BaseStyle","text":"Bases: BaseModel Model acting as an abstraction for a style. Source code in edspdf/models/style.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class BaseStyle ( BaseModel ): \"\"\" Model acting as an abstraction for a style. \"\"\" # font: str # style: str # size: float italic : bool bold : bool fontname : Optional [ str ] = None dict = attrs . asdict","title":"BaseStyle"},{"location":"reference/models/style/#edspdf.models.style.Style","text":"Bases: BaseStyle Model acting as an abstraction for a style. Source code in edspdf/models/style.py 28 29 30 31 class Style ( BaseStyle ): \"\"\" Model acting as an abstraction for a style. \"\"\"","title":"Style"},{"location":"reference/models/style/#edspdf.models.style.StyledText","text":"Bases: BaseModel Abstraction of a word, containing the style and the text. Source code in edspdf/models/style.py 40 41 42 43 44 45 46 47 48 class StyledText ( BaseModel ): \"\"\" Abstraction of a word, containing the style and the text. \"\"\" text : str style : Style dict = attrs . asdict","title":"StyledText"},{"location":"reference/models/text_box/","text":"edspdf.models.text_box","title":"text_box"},{"location":"reference/models/text_box/#edspdfmodelstext_box","text":"","title":"edspdf.models.text_box"},{"location":"reference/recipes/","text":"edspdf.recipes","title":"`edspdf.recipes`"},{"location":"reference/recipes/#edspdfrecipes","text":"","title":"edspdf.recipes"},{"location":"reference/recipes/train/","text":"edspdf.recipes.train","title":"train"},{"location":"reference/recipes/train/#edspdfrecipestrain","text":"","title":"edspdf.recipes.train"},{"location":"reference/utils/","text":"edspdf.utils","title":"`edspdf.utils`"},{"location":"reference/utils/#edspdfutils","text":"","title":"edspdf.utils"},{"location":"reference/utils/alignment/","text":"edspdf.utils.alignment align_box_labels ( src_boxes , dst_boxes , threshold = 0.0001 , group_by_source = False , pollution_label = None ) Align lines with possibly overlapping (and non-exhaustive) labels. Possible matches are sorted by covered area. Lines with no overlap at all PARAMETER DESCRIPTION src_boxes The labelled boxes that will be used to determine the label of the dst_boxes TYPE: Sequence [ Box ] dst_boxes The non-labelled boxes that will be assigned a label TYPE: Sequence [ Box ] group_by_source Whether to perform majority voting between different sources of annotations if any TYPE: bool DEFAULT: False threshold Threshold to use for discounting a label. Used if the labels DataFrame does not provide a threshold column, or to fill NaN values thereof. TYPE: float, default 1 DEFAULT: 0.0001 RETURNS DESCRIPTION List [ Box ] A copy of the boxes, with the labels mapped from the source boxes Source code in edspdf/utils/alignment.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def align_box_labels ( src_boxes : Sequence [ Box ], dst_boxes : Sequence [ Box ], threshold : float = 0.0001 , group_by_source : bool = False , pollution_label : Any = None , ) -> Sequence [ Box ]: \"\"\" Align lines with possibly overlapping (and non-exhaustive) labels. Possible matches are sorted by covered area. Lines with no overlap at all Parameters ---------- src_boxes: Sequence[Box] The labelled boxes that will be used to determine the label of the dst_boxes dst_boxes: Sequence[Box] The non-labelled boxes that will be assigned a label group_by_source: bool Whether to perform majority voting between different sources of annotations if any threshold : float, default 1 Threshold to use for discounting a label. Used if the `labels` DataFrame does not provide a `threshold` column, or to fill `NaN` values thereof. Returns ------- List[Box] A copy of the boxes, with the labels mapped from the source boxes \"\"\" return [ b for page in sorted ( set (( b . page for b in dst_boxes ))) for b in _align_box_labels_on_page ( src_boxes = [ b for b in src_boxes if page is None or b . page is None or b . page == page ], dst_boxes = [ b for b in dst_boxes if page is None or b . page is None or b . page == page ], threshold = threshold , group_by_source = group_by_source , pollution_label = pollution_label , ) ]","title":"alignment"},{"location":"reference/utils/alignment/#edspdfutilsalignment","text":"","title":"edspdf.utils.alignment"},{"location":"reference/utils/alignment/#edspdf.utils.alignment.align_box_labels","text":"Align lines with possibly overlapping (and non-exhaustive) labels. Possible matches are sorted by covered area. Lines with no overlap at all PARAMETER DESCRIPTION src_boxes The labelled boxes that will be used to determine the label of the dst_boxes TYPE: Sequence [ Box ] dst_boxes The non-labelled boxes that will be assigned a label TYPE: Sequence [ Box ] group_by_source Whether to perform majority voting between different sources of annotations if any TYPE: bool DEFAULT: False threshold Threshold to use for discounting a label. Used if the labels DataFrame does not provide a threshold column, or to fill NaN values thereof. TYPE: float, default 1 DEFAULT: 0.0001 RETURNS DESCRIPTION List [ Box ] A copy of the boxes, with the labels mapped from the source boxes Source code in edspdf/utils/alignment.py 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def align_box_labels ( src_boxes : Sequence [ Box ], dst_boxes : Sequence [ Box ], threshold : float = 0.0001 , group_by_source : bool = False , pollution_label : Any = None , ) -> Sequence [ Box ]: \"\"\" Align lines with possibly overlapping (and non-exhaustive) labels. Possible matches are sorted by covered area. Lines with no overlap at all Parameters ---------- src_boxes: Sequence[Box] The labelled boxes that will be used to determine the label of the dst_boxes dst_boxes: Sequence[Box] The non-labelled boxes that will be assigned a label group_by_source: bool Whether to perform majority voting between different sources of annotations if any threshold : float, default 1 Threshold to use for discounting a label. Used if the `labels` DataFrame does not provide a `threshold` column, or to fill `NaN` values thereof. Returns ------- List[Box] A copy of the boxes, with the labels mapped from the source boxes \"\"\" return [ b for page in sorted ( set (( b . page for b in dst_boxes ))) for b in _align_box_labels_on_page ( src_boxes = [ b for b in src_boxes if page is None or b . page is None or b . page == page ], dst_boxes = [ b for b in dst_boxes if page is None or b . page is None or b . page == page ], threshold = threshold , group_by_source = group_by_source , pollution_label = pollution_label , ) ]","title":"align_box_labels()"},{"location":"reference/utils/collections/","text":"edspdf.utils.collections multi_tee Makes copies of an iterable such that every iteration over it starts from 0. If the iterable is a sequence (list, tuple), just returns it since every iter() over the object restart from the beginning Source code in edspdf/utils/collections.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 class multi_tee : \"\"\" Makes copies of an iterable such that every iteration over it starts from 0. If the iterable is a sequence (list, tuple), just returns it since every iter() over the object restart from the beginning \"\"\" def __new__ ( cls , iterable ): if isinstance ( iterable , Sequence ): return iterable return super () . __new__ ( cls ) def __init__ ( self , iterable ): self . main , self . copy = itertools . tee ( iterable ) def __iter__ ( self ): if self . copy is not None : it = self . copy self . copy = None return it return copy . copy ( self . main )","title":"collections"},{"location":"reference/utils/collections/#edspdfutilscollections","text":"","title":"edspdf.utils.collections"},{"location":"reference/utils/collections/#edspdf.utils.collections.multi_tee","text":"Makes copies of an iterable such that every iteration over it starts from 0. If the iterable is a sequence (list, tuple), just returns it since every iter() over the object restart from the beginning Source code in edspdf/utils/collections.py 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 class multi_tee : \"\"\" Makes copies of an iterable such that every iteration over it starts from 0. If the iterable is a sequence (list, tuple), just returns it since every iter() over the object restart from the beginning \"\"\" def __new__ ( cls , iterable ): if isinstance ( iterable , Sequence ): return iterable return super () . __new__ ( cls ) def __init__ ( self , iterable ): self . main , self . copy = itertools . tee ( iterable ) def __iter__ ( self ): if self . copy is not None : it = self . copy self . copy = None return it return copy . copy ( self . main )","title":"multi_tee"},{"location":"reference/utils/optimization/","text":"edspdf.utils.optimization","title":"optimization"},{"location":"reference/utils/optimization/#edspdfutilsoptimization","text":"","title":"edspdf.utils.optimization"},{"location":"reference/utils/random/","text":"edspdf.utils.random set_seed Source code in edspdf/utils/random.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class set_seed : def __init__ ( self , seed , cuda = torch . cuda . is_available ()): \"\"\" Set seed values for random generators. If used as a context, restore the random state used before entering the context. Parameters ---------- seed: int Value used as a seed. cuda: bool Saves the cuda random states too \"\"\" # if seed is True: # seed = random.randint(1, 2**16) if seed is True : seed = random . randint ( 1 , 2 ** 16 ) self . state = get_random_generator_state ( cuda ) if seed is not None : random . seed ( seed ) torch . manual_seed ( seed ) np . random . seed ( seed ) if cuda : # pragma: no cover torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) def __enter__ ( self ): return self def __exit__ ( self , exc_type , exc_val , exc_tb ): set_random_generator_state ( self . state ) __init__ ( seed , cuda = torch . cuda . is_available ()) Set seed values for random generators. If used as a context, restore the random state used before entering the context. PARAMETER DESCRIPTION seed Value used as a seed. cuda Saves the cuda random states too DEFAULT: torch.cuda.is_available() Source code in edspdf/utils/random.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __init__ ( self , seed , cuda = torch . cuda . is_available ()): \"\"\" Set seed values for random generators. If used as a context, restore the random state used before entering the context. Parameters ---------- seed: int Value used as a seed. cuda: bool Saves the cuda random states too \"\"\" # if seed is True: # seed = random.randint(1, 2**16) if seed is True : seed = random . randint ( 1 , 2 ** 16 ) self . state = get_random_generator_state ( cuda ) if seed is not None : random . seed ( seed ) torch . manual_seed ( seed ) np . random . seed ( seed ) if cuda : # pragma: no cover torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) get_random_generator_state ( cuda = torch . cuda . is_available ()) Get the torch , numpy and random random generator state. PARAMETER DESCRIPTION cuda Saves the cuda random states too DEFAULT: torch.cuda.is_available() RETURNS DESCRIPTION RandomGeneratorState Source code in edspdf/utils/random.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def get_random_generator_state ( cuda = torch . cuda . is_available ()): \"\"\" Get the `torch`, `numpy` and `random` random generator state. Parameters ---------- cuda: bool Saves the cuda random states too Returns ------- RandomGeneratorState \"\"\" return RandomGeneratorState ( random . getstate (), torch . random . get_rng_state (), np . random . get_state (), torch . cuda . get_rng_state_all () if cuda else None , ) set_random_generator_state ( state ) Set the torch , numpy and random random generator state. PARAMETER DESCRIPTION state Source code in edspdf/utils/random.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def set_random_generator_state ( state ): \"\"\" Set the `torch`, `numpy` and `random` random generator state. Parameters ---------- state: RandomGeneratorState \"\"\" random . setstate ( state . random ) torch . random . set_rng_state ( state . torch ) np . random . set_state ( state . numpy ) if ( state . torch_cuda is not None and torch . cuda . is_available () and len ( state . torch_cuda ) == torch . cuda . device_count () ): # pragma: no cover torch . cuda . set_rng_state_all ( state . torch_cuda )","title":"random"},{"location":"reference/utils/random/#edspdfutilsrandom","text":"","title":"edspdf.utils.random"},{"location":"reference/utils/random/#edspdf.utils.random.set_seed","text":"Source code in edspdf/utils/random.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 class set_seed : def __init__ ( self , seed , cuda = torch . cuda . is_available ()): \"\"\" Set seed values for random generators. If used as a context, restore the random state used before entering the context. Parameters ---------- seed: int Value used as a seed. cuda: bool Saves the cuda random states too \"\"\" # if seed is True: # seed = random.randint(1, 2**16) if seed is True : seed = random . randint ( 1 , 2 ** 16 ) self . state = get_random_generator_state ( cuda ) if seed is not None : random . seed ( seed ) torch . manual_seed ( seed ) np . random . seed ( seed ) if cuda : # pragma: no cover torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed ) def __enter__ ( self ): return self def __exit__ ( self , exc_type , exc_val , exc_tb ): set_random_generator_state ( self . state )","title":"set_seed"},{"location":"reference/utils/random/#edspdf.utils.random.set_seed.__init__","text":"Set seed values for random generators. If used as a context, restore the random state used before entering the context. PARAMETER DESCRIPTION seed Value used as a seed. cuda Saves the cuda random states too DEFAULT: torch.cuda.is_available() Source code in edspdf/utils/random.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 def __init__ ( self , seed , cuda = torch . cuda . is_available ()): \"\"\" Set seed values for random generators. If used as a context, restore the random state used before entering the context. Parameters ---------- seed: int Value used as a seed. cuda: bool Saves the cuda random states too \"\"\" # if seed is True: # seed = random.randint(1, 2**16) if seed is True : seed = random . randint ( 1 , 2 ** 16 ) self . state = get_random_generator_state ( cuda ) if seed is not None : random . seed ( seed ) torch . manual_seed ( seed ) np . random . seed ( seed ) if cuda : # pragma: no cover torch . cuda . manual_seed ( seed ) torch . cuda . manual_seed_all ( seed )","title":"__init__()"},{"location":"reference/utils/random/#edspdf.utils.random.get_random_generator_state","text":"Get the torch , numpy and random random generator state. PARAMETER DESCRIPTION cuda Saves the cuda random states too DEFAULT: torch.cuda.is_available() RETURNS DESCRIPTION RandomGeneratorState Source code in edspdf/utils/random.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def get_random_generator_state ( cuda = torch . cuda . is_available ()): \"\"\" Get the `torch`, `numpy` and `random` random generator state. Parameters ---------- cuda: bool Saves the cuda random states too Returns ------- RandomGeneratorState \"\"\" return RandomGeneratorState ( random . getstate (), torch . random . get_rng_state (), np . random . get_state (), torch . cuda . get_rng_state_all () if cuda else None , )","title":"get_random_generator_state()"},{"location":"reference/utils/random/#edspdf.utils.random.set_random_generator_state","text":"Set the torch , numpy and random random generator state. PARAMETER DESCRIPTION state Source code in edspdf/utils/random.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def set_random_generator_state ( state ): \"\"\" Set the `torch`, `numpy` and `random` random generator state. Parameters ---------- state: RandomGeneratorState \"\"\" random . setstate ( state . random ) torch . random . set_rng_state ( state . torch ) np . random . set_state ( state . numpy ) if ( state . torch_cuda is not None and torch . cuda . is_available () and len ( state . torch_cuda ) == torch . cuda . device_count () ): # pragma: no cover torch . cuda . set_rng_state_all ( state . torch_cuda )","title":"set_random_generator_state()"},{"location":"reference/utils/torch/","text":"edspdf.utils.torch compute_pdf_relative_positions ( x0 , y0 , x1 , y1 , width , height , n_relative_positions ) Compute relative positions between boxes. Input boxes must be split between pages with the shape n_pages * n_boxes PARAMETER DESCRIPTION x0 y0 x1 y1 width height n_relative_positions Maximum range of embeddable relative positions between boxes (further distances will be capped to \u00b1n_relative_positions // 2) RETURNS DESCRIPTION torch . LongTensor Shape: n_pages * n_boxes * n_boxes * 2 Source code in edspdf/utils/torch.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def compute_pdf_relative_positions ( x0 , y0 , x1 , y1 , width , height , n_relative_positions ): \"\"\" Compute relative positions between boxes. Input boxes must be split between pages with the shape n_pages * n_boxes Parameters ---------- x0: torch.FloatTensor y0: torch.FloatTensor x1: torch.FloatTensor y1: torch.FloatTensor width: torch.FloatTensor height: torch.FloatTensor n_relative_positions: int Maximum range of embeddable relative positions between boxes (further distances will be capped to \u00b1n_relative_positions // 2) Returns ------- torch.LongTensor Shape: n_pages * n_boxes * n_boxes * 2 \"\"\" dx = x0 [:, None , :] - x0 [:, :, None ] # B begin -> A begin dx = ( dx * n_relative_positions ) . long () dy = y0 [:, None , :] - y0 [:, :, None ] # If query above (dy > 0) key, use query height ref_height = ( dy >= 0 ) . float () * height . float ()[:, :, None ] + ( dy < 0 ) . float () * height [:, None , :] dy0 = y1 [:, None , :] - y0 [:, :, None ] # A begin -> B end dy1 = y0 [:, None , :] - y1 [:, :, None ] # A end -> B begin offset = 0.5 dy = torch . where ( # where A fully above B (dy0 and dy1 > 0), dy is min distance (( dy0 + offset ) . sign () > 0 ) & (( dy1 + offset ) . sign () > 0 ), ( torch . minimum ( dy0 , dy1 ) / ref_height + offset ) . ceil (), # where A fully below B (dy0 and dy1 < 0), dy is -(min -distances) torch . where ( (( dy0 - offset ) . sign () < 0 ) & (( dy1 - offset ) . sign () < 0 ), ( torch . maximum ( dy0 , dy1 ) / ref_height - offset ) . floor (), 0 , ), ) dy = ( dy . abs () . ceil () * dy . sign ()) . long () relative_positions = torch . stack ([ dx , dy ], dim =- 1 ) return relative_positions log_einsum_exp ( formula , * ops ) Numerically stable log of einsum of exponents of operands Source code in edspdf/utils/torch.py 76 77 78 79 80 81 82 83 84 def log_einsum_exp ( formula , * ops ): \"\"\" Numerically stable log of einsum of exponents of operands \"\"\" maxes = [ op . max () for op in ops ] ops = [ op - op_max for op , op_max in zip ( ops , maxes )] res = torch . einsum ( formula , * ( op . exp () for op in ops )) . log () res = res + sum ( maxes ) return res","title":"torch"},{"location":"reference/utils/torch/#edspdfutilstorch","text":"","title":"edspdf.utils.torch"},{"location":"reference/utils/torch/#edspdf.utils.torch.compute_pdf_relative_positions","text":"Compute relative positions between boxes. Input boxes must be split between pages with the shape n_pages * n_boxes PARAMETER DESCRIPTION x0 y0 x1 y1 width height n_relative_positions Maximum range of embeddable relative positions between boxes (further distances will be capped to \u00b1n_relative_positions // 2) RETURNS DESCRIPTION torch . LongTensor Shape: n_pages * n_boxes * n_boxes * 2 Source code in edspdf/utils/torch.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def compute_pdf_relative_positions ( x0 , y0 , x1 , y1 , width , height , n_relative_positions ): \"\"\" Compute relative positions between boxes. Input boxes must be split between pages with the shape n_pages * n_boxes Parameters ---------- x0: torch.FloatTensor y0: torch.FloatTensor x1: torch.FloatTensor y1: torch.FloatTensor width: torch.FloatTensor height: torch.FloatTensor n_relative_positions: int Maximum range of embeddable relative positions between boxes (further distances will be capped to \u00b1n_relative_positions // 2) Returns ------- torch.LongTensor Shape: n_pages * n_boxes * n_boxes * 2 \"\"\" dx = x0 [:, None , :] - x0 [:, :, None ] # B begin -> A begin dx = ( dx * n_relative_positions ) . long () dy = y0 [:, None , :] - y0 [:, :, None ] # If query above (dy > 0) key, use query height ref_height = ( dy >= 0 ) . float () * height . float ()[:, :, None ] + ( dy < 0 ) . float () * height [:, None , :] dy0 = y1 [:, None , :] - y0 [:, :, None ] # A begin -> B end dy1 = y0 [:, None , :] - y1 [:, :, None ] # A end -> B begin offset = 0.5 dy = torch . where ( # where A fully above B (dy0 and dy1 > 0), dy is min distance (( dy0 + offset ) . sign () > 0 ) & (( dy1 + offset ) . sign () > 0 ), ( torch . minimum ( dy0 , dy1 ) / ref_height + offset ) . ceil (), # where A fully below B (dy0 and dy1 < 0), dy is -(min -distances) torch . where ( (( dy0 - offset ) . sign () < 0 ) & (( dy1 - offset ) . sign () < 0 ), ( torch . maximum ( dy0 , dy1 ) / ref_height - offset ) . floor (), 0 , ), ) dy = ( dy . abs () . ceil () * dy . sign ()) . long () relative_positions = torch . stack ([ dx , dy ], dim =- 1 ) return relative_positions","title":"compute_pdf_relative_positions()"},{"location":"reference/utils/torch/#edspdf.utils.torch.log_einsum_exp","text":"Numerically stable log of einsum of exponents of operands Source code in edspdf/utils/torch.py 76 77 78 79 80 81 82 83 84 def log_einsum_exp ( formula , * ops ): \"\"\" Numerically stable log of einsum of exponents of operands \"\"\" maxes = [ op . max () for op in ops ] ops = [ op - op_max for op , op_max in zip ( ops , maxes )] res = torch . einsum ( formula , * ( op . exp () for op in ops )) . log () res = res + sum ( maxes ) return res","title":"log_einsum_exp()"},{"location":"reference/visualization/","text":"edspdf.visualization","title":"`edspdf.visualization`"},{"location":"reference/visualization/#edspdfvisualization","text":"","title":"edspdf.visualization"},{"location":"reference/visualization/annotations/","text":"edspdf.visualization.annotations","title":"annotations"},{"location":"reference/visualization/annotations/#edspdfvisualizationannotations","text":"","title":"edspdf.visualization.annotations"},{"location":"reference/visualization/merge/","text":"edspdf.visualization.merge","title":"merge"},{"location":"reference/visualization/merge/#edspdfvisualizationmerge","text":"","title":"edspdf.visualization.merge"},{"location":"utilities/","text":"Overview EDS-PDF provides a few utilities help annotate PDF documents, and debug the output of an extraction pipeline.","title":"Overview"},{"location":"utilities/#overview","text":"EDS-PDF provides a few utilities help annotate PDF documents, and debug the output of an extraction pipeline.","title":"Overview"},{"location":"utilities/alignment/","text":"Alignment To simplify the annotation process, EDS-PDF provides a utility that aligns bounding boxes with text blocs extracted from a PDF document. This is particularly useful for annotating documents. Blocs Blocs + Annotation Aligned Merged Blocs","title":"Alignment"},{"location":"utilities/alignment/#alignment","text":"To simplify the annotation process, EDS-PDF provides a utility that aligns bounding boxes with text blocs extracted from a PDF document. This is particularly useful for annotating documents. Blocs Blocs + Annotation Aligned Merged Blocs","title":"Alignment"},{"location":"utilities/visualisation/","text":"Visualisation EDS-PDF provides utilities to help you visualise the output of the pipeline. Visualising a pipeline's output You can use EDS-PDF to overlay labelled bounding boxes on top of a PDF document. from edspdf import Config , load from pathlib import Path from edspdf.visualization.annotations import show_annotations config = \"\"\" [pipeline] components = [\"extractor\", \"classifier\", \"aggregator\"] components_config = $ {components} [components] [components.extractor] @factory = \"pdfminer-extractor\" extract_style = true [components.classifier] @factory = \"mask-classifier\" x0 = 0.25 x1 = 0.95 y0 = 0.3 y1 = 0.9 threshold = 0.1 \"\"\" model = load ( Config . from_str ( config ) . resolve ()[ \"pipeline\" ]) # Get a PDF pdf = Path ( \"letter.pdf\" ) . read_bytes () # Construct the DataFrame of blocs doc = model ( pdf ) # Compute an image representation of each page of the PDF # overlaid with the predicted bounding boxes imgs = show_annotations ( pdf = pdf , annotations = doc . lines ) imgs [ 0 ] If you run this code in a Jupyter notebook, you'll see the following: Merging blocs together To help debug a pipeline (or a labelled dataset), you might want to merge blocs together according to their labels. EDS-PDF provides a merge_lines method that does just that. # \u2191 Omitted code above \u2191 from edspdf.visualization.merge import merge_boxes merged = merge_boxes ( doc . lines ) imgs = show_annotations ( pdf = pdf , annotations = merged ) imgs [ 0 ] See the difference: Original Merged The merge_boxes method uses the notion of maximal cliques to compute merges. It forbids the combined blocs from overlapping with any bloc from another label.","title":"Visualisation"},{"location":"utilities/visualisation/#visualisation","text":"EDS-PDF provides utilities to help you visualise the output of the pipeline.","title":"Visualisation"},{"location":"utilities/visualisation/#visualising-a-pipelines-output","text":"You can use EDS-PDF to overlay labelled bounding boxes on top of a PDF document. from edspdf import Config , load from pathlib import Path from edspdf.visualization.annotations import show_annotations config = \"\"\" [pipeline] components = [\"extractor\", \"classifier\", \"aggregator\"] components_config = $ {components} [components] [components.extractor] @factory = \"pdfminer-extractor\" extract_style = true [components.classifier] @factory = \"mask-classifier\" x0 = 0.25 x1 = 0.95 y0 = 0.3 y1 = 0.9 threshold = 0.1 \"\"\" model = load ( Config . from_str ( config ) . resolve ()[ \"pipeline\" ]) # Get a PDF pdf = Path ( \"letter.pdf\" ) . read_bytes () # Construct the DataFrame of blocs doc = model ( pdf ) # Compute an image representation of each page of the PDF # overlaid with the predicted bounding boxes imgs = show_annotations ( pdf = pdf , annotations = doc . lines ) imgs [ 0 ] If you run this code in a Jupyter notebook, you'll see the following:","title":"Visualising a pipeline's output"},{"location":"utilities/visualisation/#merging-blocs-together","text":"To help debug a pipeline (or a labelled dataset), you might want to merge blocs together according to their labels. EDS-PDF provides a merge_lines method that does just that. # \u2191 Omitted code above \u2191 from edspdf.visualization.merge import merge_boxes merged = merge_boxes ( doc . lines ) imgs = show_annotations ( pdf = pdf , annotations = merged ) imgs [ 0 ] See the difference: Original Merged The merge_boxes method uses the notion of maximal cliques to compute merges. It forbids the combined blocs from overlapping with any bloc from another label.","title":"Merging blocs together"}]}